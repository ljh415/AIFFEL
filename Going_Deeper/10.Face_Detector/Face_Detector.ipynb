{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_widerface(config_path):\n",
    "    boxes_per_img = []\n",
    "    with open(config_path) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            num_of_obj = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(num_of_obj):\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                if w == 0:\n",
    "                    # remove boxes with no width\n",
    "                    continue\n",
    "                if h == 0:\n",
    "                    # remove boxes with no height\n",
    "                    continue\n",
    "                # Because our network is outputting 7x7 grid then it's not worth processing images with more than\n",
    "                # 5 faces because it's highly probable they are close to each other.\n",
    "                # You could remove this filter if you decide to switch to larger grid (like 14x14)\n",
    "                # Don't worry about number of train data because even with this filter we have around 16k samples\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if num_of_obj == 0:\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            boxes_per_img.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return boxes_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        logging.info('{}: Invalid JPEG data or crop window'.format(image_file))\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_849.jpg', 'width': 1024, 'height': 1385, 'depth': 3, 'class': [1], 'xmin': [449], 'ymin': [330], 'xmax': [571], 'ymax': [479], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_904.jpg', 'width': 1024, 'height': 1432, 'depth': 3, 'class': [1], 'xmin': [361], 'ymin': [98], 'xmax': [624], 'ymax': [437], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg', 'width': 1024, 'height': 768, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [78, 78, 113, 134, 163, 201, 182, 245, 304, 328, 389, 406, 436, 522, 643, 653, 793, 535, 29, 3, 20], 'ymin': [221, 238, 212, 260, 250, 218, 266, 279, 265, 295, 281, 293, 290, 328, 320, 224, 337, 311, 220, 232, 215], 'xmax': [85, 92, 124, 149, 177, 211, 197, 263, 320, 344, 406, 427, 458, 543, 666, 670, 816, 551, 40, 14, 32], 'ymax': [229, 255, 227, 275, 267, 230, 283, 294, 282, 315, 300, 314, 307, 346, 342, 249, 367, 328, 235, 247, 231], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_117.jpg', 'width': 1024, 'height': 682, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [69, 227, 296, 353, 885, 819, 727, 598, 740], 'ymin': [359, 382, 305, 280, 377, 391, 342, 246, 308], 'xmax': [119, 283, 340, 393, 948, 853, 764, 631, 785], 'ymax': [395, 425, 331, 316, 418, 434, 373, 275, 341], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel0042/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_778.jpg', 'width': 1024, 'height': 852, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [27, 63, 64, 88, 231, 263, 367, 198, 293, 412, 441, 475, 510, 576, 577, 595, 570, 645, 719, 791, 884, 898, 945, 922, 743, 841, 980, 1001, 488, 586, 669, 744, 803, 294, 203], 'ymin': [226, 95, 63, 13, 1, 122, 68, 98, 161, 36, 23, 40, 23, 30, 71, 94, 126, 171, 98, 154, 97, 48, 89, 38, 71, 18, 56, 107, 2, 1, 1, 2, 3, 2, 0], 'xmax': [60, 79, 81, 104, 244, 277, 382, 213, 345, 426, 458, 489, 524, 592, 593, 611, 583, 697, 730, 845, 900, 913, 960, 937, 754, 857, 993, 1015, 500, 601, 681, 762, 821, 305, 216], 'ymax': [262, 114, 81, 28, 14, 142, 91, 116, 220, 56, 36, 61, 40, 45, 92, 114, 142, 229, 113, 203, 118, 69, 109, 54, 89, 34, 76, 120, 20, 18, 16, 17, 20, 12, 14], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "dataset_path = os.getenv('HOME')+'/aiffel/face_detector/widerface'\n",
    "anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "file_path = 'WIDER_train'\n",
    "for i, info in enumerate(parse_widerface(os.path.join(dataset_path, 'wider_face_split', anno_txt))):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(dataset_path, file_path, 'images', info[0])\n",
    "    error, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(image_string, image_info_list):\n",
    "\n",
    "    for info in image_info_list:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 12852/12880 [00:41<00:00, 303.76it/s]"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "rootPath = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "dataset_path = 'widerface'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    logging.info('Please define valid dataset path.')\n",
    "else:\n",
    "    logging.info('Loading {}'.format(dataset_path))\n",
    "\n",
    "logging.info('Reading configuration...')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    output_file = rootPath + '/dataset/train_mask.tfrecord' if split == 'train' else rootPath + '/dataset/val_mask.tfrecord'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "\n",
    "        counter = 0\n",
    "        skipped = 0\n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt' if split == 'train' else 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train' if split == 'train' else 'WIDER_val'\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(rootPath, dataset_path, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(rootPath, dataset_path, file_path, 'images', info[0])\n",
    "\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                skipped += 1\n",
    "                logging.info('Skipped {:d} of {:d} images.'.format(skipped, counter))\n",
    "\n",
    "    logging.info('Wrote {} images to {}'.format(counter, output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# priors box \n",
    "---\n",
    "- SSD 모델 중에서 가장 중요한 특징 중 하나는 piror box (또는 anchor box)를 필요로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_size': (240, 320),\n",
       " 'dataset_path': 'dataset/train_mask.tfrecord',\n",
       " 'val_path': 'dataset/val_mask.tfrecord',\n",
       " 'dataset_len': 12880,\n",
       " 'val_len': 3226,\n",
       " 'using_crop': True,\n",
       " 'using_bin': True,\n",
       " 'using_flip': True,\n",
       " 'using_distort': True,\n",
       " 'using_normalizing': True,\n",
       " 'labels_list': ['background', 'face'],\n",
       " 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
       " 'steps': [8, 16, 32, 64],\n",
       " 'match_thresh': 0.45,\n",
       " 'variances': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'base_channel': 16,\n",
       " 'resume': False,\n",
       " 'epoch': 100,\n",
       " 'init_lr': 0.01,\n",
       " 'lr_decay_epoch': [50, 70],\n",
       " 'lr_rate': 0.1,\n",
       " 'warmup_epoch': 5,\n",
       " 'min_lr': 0.0001,\n",
       " 'weights_decay': 0.0005,\n",
       " 'momentum': 0.9,\n",
       " 'save_freq': 10,\n",
       " 'score_threshold': 0.5,\n",
       " 'nms_threshold': 0.4,\n",
       " 'max_number_keep': 200}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "    # general setting\n",
    "    \"batch_size\": 32,\n",
    "    \"input_size\": (240, 320),  # (h,w)\n",
    "\n",
    "    # training dataset\n",
    "    \"dataset_path\": 'dataset/train_mask.tfrecord',  # 'dataset/trainval_mask.tfrecord'\n",
    "    \"val_path\": 'dataset/val_mask.tfrecord',  #\n",
    "    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\n",
    "    \"val_len\": 3226,\n",
    "    \"using_crop\": True,\n",
    "    \"using_bin\": True,\n",
    "    \"using_flip\": True,\n",
    "    \"using_distort\": True,\n",
    "    \"using_normalizing\": True,\n",
    "    \"labels_list\": ['background', 'face'],  # xml annotation\n",
    "\n",
    "    # anchor setting\n",
    "    \"min_sizes\":[[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
    "    \"steps\": [8, 16, 32, 64],\n",
    "    \"match_thresh\": 0.45,\n",
    "    \"variances\": [0.1, 0.2],\n",
    "    \"clip\": False,\n",
    "\n",
    "    # network\n",
    "    \"base_channel\": 16,\n",
    "\n",
    "    # training setting\n",
    "    \"resume\": False,  # if False,training from scratch\n",
    "    \"epoch\": 100,\n",
    "    \"init_lr\": 1e-2,\n",
    "    \"lr_decay_epoch\": [50, 70],\n",
    "    \"lr_rate\": 0.1,\n",
    "    \"warmup_epoch\": 5,\n",
    "    \"min_lr\": 1e-4,\n",
    "\n",
    "    \"weights_decay\": 5e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"save_freq\": 10, #frequency of save model weights\n",
    "\n",
    "    # inference\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"nms_threshold\": 0.4,\n",
    "    \"max_number_keep\": 200\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320)\n"
     ]
    }
   ],
   "source": [
    "image_sizes = cfg['input_size']\n",
    "min_sizes = cfg[\"min_sizes\"]\n",
    "steps = cfg[\"steps\"]\n",
    "clip = cfg[\"clip\"]\n",
    "\n",
    "if isinstance(image_sizes, int):\n",
    "    image_sizes = (image_sizes, image_sizes)\n",
    "elif isinstance(image_sizes, tuple):\n",
    "    image_sizes = image_sizes\n",
    "else:\n",
    "    raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "print(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 40], [15, 20], [8, 10], [4, 5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "for m in range(4):\n",
    "    if (steps[m] != pow(2, (m + 3))):\n",
    "        print(\"steps must be [8,16,32,64]\")\n",
    "        sys.exit()\n",
    "\n",
    "assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps]\n",
    "\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17680"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = []\n",
    "num_box_fm_cell=[]\n",
    "for k, f in enumerate(feature_maps):\n",
    "    num_box_fm_cell.append(len(min_sizes[k]))\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            if isinstance(min_size, int):\n",
    "                min_size = (min_size, min_size)\n",
    "            elif isinstance(min_size, tuple):\n",
    "                min_size=min_size\n",
    "            else:\n",
    "                raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "            s_kx = min_size[1] / image_sizes[1]\n",
    "            s_ky = min_size[0] / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "priors = np.asarray(anchors).reshape([-1, 4])\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.01666667, 0.03125   , 0.04166667],\n",
       "       [0.0125    , 0.01666667, 0.05      , 0.06666667],\n",
       "       [0.0125    , 0.01666667, 0.075     , 0.1       ],\n",
       "       ...,\n",
       "       [0.9       , 0.93333333, 0.4       , 0.53333333],\n",
       "       [0.9       , 0.93333333, 0.6       , 0.8       ],\n",
       "       [0.9       , 0.93333333, 0.8       , 1.06666667]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 메소드 ```prior_box(cfg, image_sizes=None)``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_box(cfg,image_sizes=None):\n",
    "    \"\"\"prior box\"\"\"\n",
    "    if image_sizes is None:\n",
    "        image_sizes = cfg['input_size']\n",
    "    min_sizes=cfg[\"min_sizes\"]\n",
    "    steps=cfg[\"steps\"]\n",
    "    clip=cfg[\"clip\"]\n",
    "\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    for m in range(4):\n",
    "        if (steps[m] != pow(2, (m + 3))):\n",
    "            print(\"steps must be [8,16,32,64]\")\n",
    "            sys.exit()\n",
    "\n",
    "    assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps]\n",
    "\n",
    "    anchors = []\n",
    "    num_box_fm_cell=[]\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        num_box_fm_cell.append(len(min_sizes[k]))\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                if isinstance(min_size, int):\n",
    "                    min_size = (min_size, min_size)\n",
    "                elif isinstance(min_size, tuple):\n",
    "                    min_size=min_size\n",
    "                else:\n",
    "                    raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "                s_kx = min_size[1] / image_sizes[1]\n",
    "                s_ky = min_size[0] / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "    output = np.asarray(anchors).reshape([-1, 4])\n",
    "\n",
    "    if clip:\n",
    "        output = np.clip(output, 0, 1)\n",
    "    return output,num_box_fm_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD model 빌드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1), use_bn=True, padding=None, block_id=None):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu).\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_conv_block(inputs, pointwise_conv_filters,\n",
    "                          depth_multiplier=1, strides=(1, 1), use_bn=True, block_id=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "        # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False if use_bn else True,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False if use_bn else True,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _branch_block(input, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_head_block(inputs, filters, strides=(1, 1), block_id=None):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_heads(x, idx, num_class, num_cell):\n",
    "    \"\"\" Compute outputs of classification and regression heads\n",
    "    Args:\n",
    "        x: the input feature map\n",
    "        idx: index of the head layer\n",
    "    Returns:\n",
    "        conf: output of the idx-th classification head\n",
    "        loc: output of the idx-th regression head\n",
    "    \"\"\"\n",
    "    conf = _create_head_block(inputs=x, filters=num_cell[idx] * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs=x, filters=num_cell[idx] * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "\n",
    "    return conf, loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SsdModel(cfg, num_cell, training=False, name='ssd_model'):\n",
    "    image_sizes = cfg['input_size']   if training else None\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    elif image_sizes == None:\n",
    "        image_sizes = (None, None)\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    base_channel = cfg[\"base_channel\"]\n",
    "    num_class = len(cfg['labels_list'])\n",
    "\n",
    "    x = inputs = tf.keras.layers.Input(shape=[image_sizes[0], image_sizes[1], 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))  # 120*160*16\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))  # 60*80\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))  # 30*40\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))  # 15*20\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 8*10\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 4*5\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    head_idx = 0\n",
    "    assert len(extra_layers) == len(num_cell)\n",
    "    for layer in extra_layers:\n",
    "        conf, loc = _compute_heads(layer, head_idx, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "        head_idx += 1\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_1 (ZeroPadding2D)      (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, None, None, 1 432         conv_pad_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_1 (BatchNormalization)  (None, None, None, 1 64          conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_1 (ReLU)              (None, None, None, 1 0           conv_bn_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, None, None, 3 4608        conv_relu_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_2 (BatchNormalization)  (None, None, None, 3 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_2 (ReLU)              (None, None, None, 3 0           conv_bn_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_3 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, None, None, 3 9216        conv_pad_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_3 (BatchNormalization)  (None, None, None, 3 128         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_3 (ReLU)              (None, None, None, 3 0           conv_bn_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, None, None, 3 9216        conv_relu_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_4 (BatchNormalization)  (None, None, None, 3 128         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_4 (ReLU)              (None, None, None, 3 0           conv_bn_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_5 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, None, None, 6 18432       conv_pad_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_5 (BatchNormalization)  (None, None, None, 6 256         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_5 (ReLU)              (None, None, None, 6 0           conv_bn_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, None, None, 6 36864       conv_relu_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_6 (BatchNormalization)  (None, None, None, 6 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_6 (ReLU)              (None, None, None, 6 0           conv_bn_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, None, None, 6 36864       conv_relu_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_7 (BatchNormalization)  (None, None, None, 6 256         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_7 (ReLU)              (None, None, None, 6 0           conv_bn_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, None, None, 6 36864       conv_relu_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_8 (BatchNormalization)  (None, None, None, 6 256         conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_8 (ReLU)              (None, None, None, 6 0           conv_bn_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_9 (ZeroPadding2D)      (None, None, None, 6 0           conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, None, None, 1 73728       conv_pad_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_9 (BatchNormalization)  (None, None, None, 1 512         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_9 (ReLU)              (None, None, None, 1 0           conv_bn_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, None, None, 1 147456      conv_relu_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_10 (BatchNormalization) (None, None, None, 1 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_10 (ReLU)             (None, None, None, 1 0           conv_bn_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, None, None, 1 147456      conv_relu_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_11 (BatchNormalization) (None, None, None, 1 512         conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_11 (ReLU)             (None, None, None, 1 0           conv_bn_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)     (None, None, None, 1 0           conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D)    (None, None, None, 1 1152        conv_pad_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormalizati (None, None, None, 1 512         conv_dw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)          (None, None, None, 1 0           conv_dw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12 (Conv2D)             (None, None, None, 2 32768       conv_dw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)          (None, None, None, 2 0           conv_pw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)          (None, None, None, 2 0           conv_dw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13 (Conv2D)             (None, None, None, 2 65536       conv_dw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)          (None, None, None, 2 0           conv_pw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_14 (ZeroPadding2D)     (None, None, None, 2 0           conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pad_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_relu (ReLU)          (None, None, None, 2 0           conv_dw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14 (Conv2D)             (None, None, None, 2 65536       conv_dw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_relu (ReLU)          (None, None, None, 2 0           conv_pw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 1 9232        conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 18448       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 36880       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 36880       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, None, None, 1 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 1 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 1 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 18464       conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 3 36896       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 3 73760       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 3 73760       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 4 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 4 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 4 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, None, None, 4 0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, None, None, 4 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, None, None, 4 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, None, None, 4 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 5196        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 8 3464        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 8 3464        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 5196        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 2598        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 1732        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 4 1732        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 2598        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 4)      0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, None, 4)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, None, 4)      0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, None, 4)      0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, None, 2)      0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None, 2)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, None, 2)      0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, None, 2)      0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model = SsdModel(cfg, num_cell=[3, 2, 2, 3], training=False)\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation, Prior 적용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation\n",
    "```tf.data.TFRecordDataset.map()``` 내에서 호출할 메소드들입니다.\n",
    "\n",
    "- _crop\n",
    "- _pad_to_square\n",
    "- _resize\n",
    "- _flip\n",
    "- _distort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        \"\"\"\n",
    "        return iof of a and b, numpy version for data augenmentation\n",
    "        \"\"\"\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(img, labels, img_dim):\n",
    "    ''' # resize and boxes coordinate to percent'''\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "    if isinstance(img_dim, int):\n",
    "        img_dim = (img_dim, img_dim)\n",
    "    elif isinstance(img_dim,tuple):\n",
    "        img_dim = img_dim\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [img_dim[0], img_dim[1]], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior box 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2]:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaccard index를 계산하는 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_bbox(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth\n",
    "    boxes we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n",
    "\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tf(labels, priors, match_thresh, variances=None):\n",
    "    \"\"\"tensorflow encoding\"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    priors = tf.cast(priors, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "\n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, priors)\n",
    "    best_prior_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_prior_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.ones_like(best_prior_idx, tf.float32) * 2.)\n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n",
    "\n",
    "    # Scale Ground-Truth Boxes\n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n",
    "    loc_t = _encode_bbox(matches_bbox, priors, variances)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)  # [num_priors]\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_thresh), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset\n",
    "- ```_transform_data``` : aumemtation과 prior box label을 적용하여 기존의 dataset을 변환하는 메소드\n",
    "- ```_parse_tfrecord``` : tfrecord 에 ```_transform_data```를 적용하는 함수 클로저 생성\n",
    "- ```load_tfrecord_dataset``` : ```tf.data.TFRecordDataset.map()```에 _parse_tfrecord을 적용하는 실제 데이터셋 변환 메인 메소드\n",
    "- ```load_dataset``` : ```load_tfrecord_dataset```을 통해 train, validation 데이터셋을 생성하는 최종 메소드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data(img_dim, using_crop,using_flip, using_distort, using_encoding,using_normalizing, priors,\n",
    "                    match_thresh,  variances):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if using_crop:\n",
    "        # randomly crop\n",
    "            img, labels = _crop(img, labels)\n",
    "\n",
    "            # padding to square\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        # resize and boxes coordinate to percent\n",
    "        img, labels = _resize(img, labels, img_dim)\n",
    "\n",
    "        # randomly left-right flip\n",
    "        if using_flip:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        # distort\n",
    "        if using_distort:\n",
    "            img = _distort(img)\n",
    "\n",
    "        # encode labels to feature targets\n",
    "        if using_encoding:\n",
    "            labels = encode_tf(labels=labels, priors=priors, match_thresh=match_thresh, variances=variances)\n",
    "        if using_normalizing:\n",
    "            img=(img/255.0-0.5)/1.0\n",
    "\n",
    "        return img, labels\n",
    "    return transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfrecord(img_dim,using_crop, using_flip, using_distort,\n",
    "                    using_encoding, using_normalizing,priors, match_thresh,  variances):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(\n",
    "            img_dim, using_crop,using_flip, using_distort, using_encoding, using_normalizing,priors,\n",
    "            match_thresh,  variances)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n",
    "                          using_crop=True,using_flip=True, using_distort=True,\n",
    "                          using_encoding=True, using_normalizing=True,\n",
    "                          priors=None, match_thresh=0.45,variances=None,\n",
    "                          shuffle=True, repeat=True,buffer_size=10240):\n",
    "\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    if not using_encoding:\n",
    "        assert batch_size == 1\n",
    "    else:\n",
    "        assert priors is not None\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if repeat:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(img_dim, using_crop, using_flip, using_distort,\n",
    "                        using_encoding, using_normalizing,priors, match_thresh,  variances),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg, priors, shuffle=True, buffer_size=10240,train=True):\n",
    "    \"\"\"load dataset\"\"\"\n",
    "    global dataset\n",
    "    if train:\n",
    "        logging.info(\"load train dataset from {}\".format(cfg['dataset_path']))\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['dataset_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=cfg['using_crop'],\n",
    "            using_flip=cfg['using_flip'],\n",
    "            using_distort=cfg['using_distort'],\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=True,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['val_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=False,\n",
    "            using_flip=False,\n",
    "            using_distort=False,\n",
    "            using_encoding=True,\n",
    "            using_normalizing=True,\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=False,\n",
    "            buffer_size=buffer_size)\n",
    "        logging.info(\"load validation dataset from {}\".format(cfg['val_path']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler\n",
    "- ```PiecewiseConstantWarmUpDecay```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiecewiseConstantWarmUpDecay(\n",
    "        tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule wiht warm up schedule.\n",
    "    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n",
    "\n",
    "    def __init__(self, boundaries, values, warmup_steps, min_lr,\n",
    "                 name=None):\n",
    "        super(PiecewiseConstantWarmUpDecay, self).__init__()\n",
    "\n",
    "        if len(boundaries) != len(values) - 1:\n",
    "            raise ValueError(\n",
    "                    \"The length of boundaries should be 1 less than the\"\n",
    "                    \"length of values\")\n",
    "\n",
    "        self.boundaries = boundaries\n",
    "        self.values = values\n",
    "        self.name = name\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n",
    "            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
    "            pred_fn_pairs = []\n",
    "            warmup_steps = self.warmup_steps\n",
    "            boundaries = self.boundaries\n",
    "            values = self.values\n",
    "            min_lr = self.min_lr\n",
    "\n",
    "            pred_fn_pairs.append(\n",
    "                (step <= warmup_steps,\n",
    "                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n",
    "            pred_fn_pairs.append(\n",
    "                (tf.logical_and(step <= boundaries[0],\n",
    "                                step > warmup_steps),\n",
    "                 lambda: tf.constant(values[0])))\n",
    "            pred_fn_pairs.append(\n",
    "                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n",
    "\n",
    "            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n",
    "                                    values[1:-1]):\n",
    "                # Need to bind v here; can do this with lambda v=v: ...\n",
    "                pred = (step > low) & (step <= high)\n",
    "                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n",
    "\n",
    "            # The default isn't needed here because our conditions are mutually\n",
    "            # exclusive and exhaustive, but tf.case requires it.\n",
    "            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n",
    "                           exclusive=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"boundaries\": self.boundaries,\n",
    "                \"values\": self.values,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"min_lr\": self.min_lr,\n",
    "                \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n",
    "                      warmup_steps=0., min_lr=0.,\n",
    "                      name='MultiStepWarmUpLR'):\n",
    "    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n",
    "    assert warmup_steps <= lr_steps[0]\n",
    "    assert min_lr <= initial_learning_rate\n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "    return PiecewiseConstantWarmUpDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_negative_mining(loss, class_truth, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        class_truth: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        class_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # class_truth: B x N\n",
    "    pos_idx = class_truth > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiBoxLoss(num_class=3, neg_pos_ratio=3.0):\n",
    "    def multi_loss(y_true, y_pred):\n",
    "        \"\"\" Compute losses for SSD\n",
    "               regression loss: smooth L1\n",
    "               classification loss: cross entropy\n",
    "           Args:\n",
    "               y_true: [B,N,5]\n",
    "               y_pred: [B,N,num_class]\n",
    "               class_pred: outputs of classification heads (B,N, num_classes)\n",
    "               loc_pred: outputs of regression heads (B,N, 4)\n",
    "               class_truth: classification targets (B,N)\n",
    "               loc_truth: regression targets (B,N, 4)\n",
    "           Returns:\n",
    "               class_loss: classification loss\n",
    "               loc_loss: regression loss\n",
    "       \"\"\"\n",
    "        num_batch = tf.shape(y_true)[0]\n",
    "        num_prior = tf.shape(y_true)[1]\n",
    "        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n",
    "        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses without reduction\n",
    "        temp_loss = cross_entropy(class_truth, class_pred)\n",
    "        # 2. hard negative mining\n",
    "        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        loss_class = cross_entropy(\n",
    "            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # localization loss only consist of positive examples (smooth L1)\n",
    "        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "\n",
    "        loss_class = loss_class / num_pos\n",
    "        loss_loc = loss_loc / num_pos\n",
    "        return loss_loc, loss_class\n",
    "\n",
    "    return multi_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9da3a096cba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFATAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mweights_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HOME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/aiffel/face_detector/checkpoints'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "global load_t1\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "\n",
    "weights_dir = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "logging.info(\"Load configuration...\")\n",
    "label_classes = cfg['labels_list']\n",
    "logging.info(f\"Total image sample:{cfg['dataset_len']},Total classes number:\"\n",
    "             f\"{len(label_classes)},classes list:{label_classes}\")\n",
    "\n",
    "logging.info(\"Compute prior boxes...\")\n",
    "priors, num_cell = prior_box(cfg)\n",
    "logging.info(f\"Prior boxes number:{len(priors)},default anchor box number per feature map cell:{num_cell}\") # 4420, [3, 2, 2, 3]\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
