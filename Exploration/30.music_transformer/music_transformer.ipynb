{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 : 다양한 조건의 음악 생성하기\n",
    "---\n",
    "## 1. MAESTRO 데이터셋을 전처리하여 훈련용 데이터셋 구성하기\n",
    "## 2. Music Transformer 모델을 구현하여 학습 진행하기\n",
    "    - 단, 20Epcoh을 완전히 학습 진행하는 것은 아니다.\n",
    "    - 최소 2Epoch까지는 진행\n",
    "## 3. 제공된 체크포인트 파일을 이용하여 다양한 midi 파일 생성하기\n",
    "    - midi파일을 생성하는 단계에서 바꾸어 볼 수 있는 조건에는 무엇이 있는지 확인\n",
    "    - 조건을 변경해 가며 5개 이상의 midi 파일을 생성\n",
    "    - 가장 잘 생성된 midi파일을 첨부하여 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAESTRO 데이터셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import concurrent.futures\n",
    "\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플로 1개의 MIDI 파일을 골라봅니다.\n",
    "midi_file = os.getenv('HOME')+'/aiffel/music_transformer/data/maestro-v2.0.0/2018/MIDI-Unprocessed_Chamber1_MID--AUDIO_07_R3_2018_wav--2.midi'\n",
    "\n",
    "midi = mido.MidiFile(midi_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MIDI 파일 앞부분의 30개 정도의 이벤트 구조가 어떻게 되어 있는지 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG [0]----------------\n",
      "0\n",
      "set_tempo\n",
      "MSG [1]----------------\n",
      "0\n",
      "time_signature\n",
      "MSG [2]----------------\n",
      "0\n",
      "program_change\n",
      "MSG [3]----------------\n",
      "0\n",
      "control_change\n",
      "MSG [4]----------------\n",
      "0\n",
      "control_change\n",
      "MSG [5]----------------\n",
      "0.5143229166666666\n",
      "control_change\n",
      "MSG [6]----------------\n",
      "0.6328125\n",
      "control_change\n",
      "MSG [7]----------------\n",
      "0.7903645833333333\n",
      "control_change\n",
      "MSG [8]----------------\n",
      "0.9999999999999999\n",
      "control_change\n",
      "MSG [9]----------------\n",
      "1.0325520833333333\n",
      "note_on\n",
      "[1.0325520833333333, 1, 74, 86]\n",
      "MSG [10]----------------\n",
      "1.0442708333333333\n",
      "note_on\n",
      "[1.0442708333333333, 1, 38, 77]\n",
      "MSG [11]----------------\n",
      "1.0794270833333333\n",
      "control_change\n",
      "MSG [12]----------------\n",
      "1.1184895833333333\n",
      "control_change\n",
      "MSG [13]----------------\n",
      "1.1588541666666665\n",
      "control_change\n",
      "MSG [14]----------------\n",
      "1.2174479166666665\n",
      "control_change\n",
      "MSG [15]----------------\n",
      "1.2265624999999998\n",
      "note_on\n",
      "[1.2265624999999998, 0, 74, 0]\n",
      "MSG [16]----------------\n",
      "1.2369791666666665\n",
      "control_change\n",
      "MSG [17]----------------\n",
      "1.2395833333333333\n",
      "note_on\n",
      "[1.2395833333333333, 1, 73, 69]\n",
      "MSG [18]----------------\n",
      "1.2408854166666665\n",
      "note_on\n",
      "[1.2408854166666665, 1, 37, 64]\n",
      "MSG [19]----------------\n",
      "1.2460937499999998\n",
      "note_on\n",
      "[1.2460937499999998, 0, 38, 0]\n",
      "MSG [20]----------------\n",
      "1.2565104166666665\n",
      "control_change\n",
      "MSG [21]----------------\n",
      "1.2695312499999998\n",
      "note_on\n",
      "[1.2695312499999998, 1, 34, 64]\n",
      "MSG [22]----------------\n",
      "1.2734374999999998\n",
      "note_on\n",
      "[1.2734374999999998, 1, 71, 71]\n",
      "MSG [23]----------------\n",
      "1.2760416666666665\n",
      "control_change\n",
      "MSG [24]----------------\n",
      "1.2968749999999998\n",
      "control_change\n",
      "MSG [25]----------------\n",
      "1.309895833333333\n",
      "note_on\n",
      "[1.309895833333333, 0, 34, 0]\n",
      "MSG [26]----------------\n",
      "1.3164062499999998\n",
      "control_change\n",
      "MSG [27]----------------\n",
      "1.3164062499999998\n",
      "note_on\n",
      "[1.3164062499999998, 0, 73, 0]\n",
      "MSG [28]----------------\n",
      "1.3242187499999998\n",
      "note_on\n",
      "[1.3242187499999998, 1, 35, 64]\n",
      "MSG [29]----------------\n",
      "1.3242187499999998\n",
      "note_on\n",
      "[1.3242187499999998, 0, 37, 0]\n",
      "MSG [30]----------------\n",
      "1.3359374999999998\n",
      "control_change\n",
      "MSG [31]----------------\n",
      "1.3437499999999998\n",
      "note_on\n",
      "[1.3437499999999998, 0, 71, 0]\n"
     ]
    }
   ],
   "source": [
    "ON = 1\n",
    "OFF = 0\n",
    "CC = 2\n",
    "\n",
    "current_time = 0\n",
    "eventlist = []\n",
    "cc = False\n",
    "for idx, msg in enumerate(midi):\n",
    "    print('MSG [{}]----------------'.format(idx))\n",
    "    current_time += msg.time\n",
    "    print(current_time)\n",
    "    print(msg.type)\n",
    "    if msg.type is 'note_on' and msg.velocity > 0:\n",
    "        event = [current_time, ON, msg.note, msg.velocity]\n",
    "        print(event)\n",
    "    elif msg.type is 'note_off' or (msg.type is 'note_on' and msg.velocity == 0):\n",
    "        event = [current_time, OFF, msg.note, msg.velocity]\n",
    "        print(event)\n",
    "        \n",
    "    if msg.type is 'control_change':\n",
    "        if msg.control != 64:\n",
    "            continue\n",
    "        if cc == False and msg.value > 0:\n",
    "            cc = True\n",
    "            event = [current_time, CC, 0, 1]\n",
    "            print(event)\n",
    "        elif cc == True and msg.value == 0:\n",
    "            cc = False\n",
    "            event = [current_time, CC, 0, 0]\n",
    "            print(event)\n",
    "\n",
    "    if idx > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIDI 앞부분의 이벤트 메시지 타입은 ```control_change``` 등의 세팅 부분이고, 실제 악보 부분은 ```note_on``` 메시지를 통해 구현됩니다. 위 코드에서 이벤트 구조는 ```[음 지속시간, ON/OFF, 음고(pitch), 속도(velocity)]```에 해당합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```get_data()```함수\n",
    "- MIDI파일을 가공만 한 것이 아니라 time, note, interval 등에 대한 Augmentation까지 함께 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalDim = 100\n",
    "\n",
    "VelocityDim = 32\n",
    "VelocityOffset = IntervalDim\n",
    "\n",
    "NoteOnDim = NoteOffDim = 128  #128\n",
    "NoteOnOffset = IntervalDim + VelocityDim\n",
    "NoteOffOffset = IntervalDim + VelocityDim + NoteOnDim\n",
    "\n",
    "CCDim = 2\n",
    "CCOffset = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim + CCDim # 390\n",
    "\n",
    "def get_data(data, length):    \n",
    "    # time augmentation\n",
    "    data[:, 0] *= np.random.uniform(0.80, 1.20)\n",
    "    \n",
    "    # absolute time to relative interval\n",
    "    data[1:, 0] = data[1:, 0] - data[:-1, 0]\n",
    "    data[0, 0] = 0\n",
    "    \n",
    "    # discretize interval into IntervalDim\n",
    "    data[:, 0] = np.clip(np.round(data[:, 0] * IntervalDim), 0, IntervalDim - 1)\n",
    "    \n",
    "    # Note augmentation\n",
    "    data[:, 2] += np.random.randint(-6, 6)\n",
    "    data[:, 2] = np.clip(data[:, 2], 0, NoteOnDim - 1)\n",
    "    \n",
    "    eventlist = []\n",
    "    for d in data:\n",
    "        # append interval\n",
    "        interval = d[0]\n",
    "        eventlist.append(interval)\n",
    "    \n",
    "        # note on case\n",
    "        if d[1] == 1:\n",
    "            velocity = (d[3] / 128) * VelocityDim + VelocityOffset\n",
    "            note = d[2] + NoteOnOffset\n",
    "            eventlist.append(velocity)\n",
    "            eventlist.append(note)\n",
    "            \n",
    "        # note off case\n",
    "        elif d[1] == 0:\n",
    "            note = d[2] + NoteOffOffset\n",
    "            eventlist.append(note)\n",
    "        # CC\n",
    "        elif d[1] == 2:\n",
    "            event = CCOffset + d[3]\n",
    "            eventlist.append(event)\n",
    "            \n",
    "    eventlist = np.array(eventlist).astype(np.int)\n",
    "    \n",
    "    if len(eventlist) > (length+1):\n",
    "        start_index = np.random.randint(0, len(eventlist) - (length+1))\n",
    "        eventlist = eventlist[start_index:start_index+(length+1)]\n",
    "        \n",
    "    # pad zeros\n",
    "    if len(eventlist) < (length+1):\n",
    "        pad = (length+1) - len(eventlist)\n",
    "        eventlist = np.pad(eventlist, (pad, 0), 'constant')\n",
    "        \n",
    "    x = eventlist[:length]\n",
    "    y = eventlist[1:length+1]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1282,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.getenv('HOME')+'/aiffel/music_transformer/data/midi_test.npy'\n",
    "\n",
    "get_midi = np.load(data_path, allow_pickle=True)\n",
    "get_midi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 256\n",
    "train = []\n",
    "labels = []\n",
    "\n",
    "for midi_list in get_midi:\n",
    "    cut_list = [midi_list[i:i+length] for i in range(0, len(midi_list), length)]\n",
    "    for sublist in cut_list:\n",
    "        x, y = get_data(np.array(sublist), length)\n",
    "        train.append(x)\n",
    "        labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59268, 256) (59268, 256)\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(train.shape, labels.shape)   # 학습을 위해 MIDI list를 256 길이로 나누었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pad = pad_sequences(train,\n",
    "                               maxlen=length,\n",
    "                               padding='post',\n",
    "                               value=0)\n",
    "train_label_pad = pad_sequences(labels,\n",
    "                                maxlen=length,\n",
    "                                padding='post',\n",
    "                                value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_casting(train, label):\n",
    "    train = tf.cast(train, tf.int64)\n",
    "    label = tf.cast(label, tf.int64)\n",
    "\n",
    "    return train, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "train_dataset = train_dataset.map(tensor_casting)\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[205   1 117 ... 342   3 330]\n",
      " [  4 117 227 ...   1 116 211]\n",
      " [188  13 324 ...   5 333   0]\n",
      " ...\n",
      " [305   1 293 ... 121 176   0]\n",
      " [207   5 116 ... 179   0 120]\n",
      " [321   1 357 ... 114 208   6]], shape=(16, 256), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  1 117 207 ...   3 330   2]\n",
      " [117 227   0 ... 116 211   2]\n",
      " [ 13 324   4 ... 333   0 338]\n",
      " ...\n",
      " [  1 293  19 ... 176   0 119]\n",
      " [  5 116 186 ...   0 120 183]\n",
      " [  1 357   5 ... 208   6 114]], shape=(16, 256), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for t,l in train_dataset.take(1):\n",
    "    print(t)\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Tranformer 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 1), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeGlobalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(RelativeGlobalAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.headDim = d_model // num_heads\n",
    "        self.contextDim = int(self.headDim * self.num_heads)\n",
    "        self.eventDim = 390\n",
    "        self.E = self.add_weight('E', shape=[self.num_heads, length, self.headDim])\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wk = tf.keras.layers.Dense(self.headDim)\n",
    "        self.wv = tf.keras.layers.Dense(self.headDim)\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        q = tf.stack([self.wq(q) for _ in range(self.num_heads)])\n",
    "        k = tf.stack([self.wk(k) for _ in range(self.num_heads)])\n",
    "        v = tf.stack([self.wv(v) for _ in range(self.num_heads)])\n",
    "\n",
    "        self.batch_size = q.shape[1]\n",
    "        self.max_len = q.shape[2]\n",
    "        \n",
    "        #skewing\n",
    "        # E = Heads, Time, HeadDim\n",
    "        # [Heads, Batch * Time, HeadDim]\n",
    "        Q_ = tf.reshape(q, [self.num_heads, self.batch_size * self.max_len, self.headDim])\n",
    "        # [Heads, Batch * Time, Time]\n",
    "        S = tf.matmul(Q_, self.E, transpose_b=True)\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len, self.max_len])\n",
    "        # [Heads, Batch, Time, Time+1]\n",
    "        S = tf.pad(S, ((0, 0), (0, 0), (0, 0), (1, 0)))\n",
    "        # [Heads, Batch, Time+1, Time]\n",
    "        S = tf.reshape(S, [self.num_heads, self.batch_size, self.max_len + 1, self.max_len])   \n",
    "        # [Heads, Batch, Time, Time]\n",
    "        S = S[:, :, 1:]\n",
    "        # [Heads, Batch, Time, Time]\n",
    "        attention = (tf.matmul(q, k, transpose_b=True) + S) / np.sqrt(self.headDim)\n",
    "        # mask tf 2.0 == tf.linalg.band_part\n",
    "        get_mask = tf.linalg.band_part(tf.ones([self.max_len, self.max_len]), -1, 0)\n",
    "        attention = attention * get_mask - tf.cast(1e10, attention.dtype) * (1-get_mask)\n",
    "        score = tf.nn.softmax(attention, axis=3)\n",
    "\n",
    "        # [Heads, Batch, Time, HeadDim]\n",
    "        context = tf.matmul(score, v)\n",
    "        # [Batch, Time, Heads, HeadDim]\n",
    "        context = tf.transpose(context, [1, 2, 0, 3])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        context = tf.reshape(context, [self.batch_size, self.max_len, self.d_model])\n",
    "        # [Batch, Time, ContextDim]\n",
    "        logits = tf.keras.layers.Dense(self.d_model)(context)\n",
    "\n",
    "        return logits, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.rga = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.rga(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.rga1 = RelativeGlobalAttention(d_model, num_heads)\n",
    "        self.rga2 = RelativeGlobalAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.rga1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.rga2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attention_weights = {}\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(input_vocab_size)\n",
    "\n",
    "    def call(self, inp, training, enc_padding_mask, \n",
    "             look_ahead_mask, dec_padding_mask):\n",
    "        embed = self.embedding(inp)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        enc_output = self.encoder(embed, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            embed, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 390   # MIDI가 낼 수 있는 소리의 종류\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "music_transformer = MusicTransformer(num_layers, d_model, num_heads, dff,\n",
    "                                     input_vocab_size, rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/aiffel/music_transformer/models/ckpt-11\n",
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.getenv('HOME')+'/aiffel/music_transformer/models/'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(music_transformer=music_transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(ckpt_manager.latest_checkpoint)\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.1934\n",
      "Epoch 1 Batch 50 Loss 3.3179\n",
      "Epoch 1 Batch 100 Loss 3.3125\n",
      "Epoch 1 Batch 150 Loss 3.3160\n",
      "Epoch 1 Batch 200 Loss 3.3140\n",
      "Epoch 1 Batch 250 Loss 3.3117\n",
      "Epoch 1 Batch 300 Loss 3.3122\n",
      "Epoch 1 Batch 350 Loss 3.3105\n",
      "Epoch 1 Batch 400 Loss 3.3115\n",
      "Epoch 1 Batch 450 Loss 3.3115\n",
      "Epoch 1 Batch 500 Loss 3.3120\n",
      "Epoch 1 Batch 550 Loss 3.3126\n",
      "Epoch 1 Batch 600 Loss 3.3119\n",
      "Epoch 1 Batch 650 Loss 3.3139\n",
      "Epoch 1 Batch 700 Loss 3.3141\n",
      "Epoch 1 Batch 750 Loss 3.3136\n",
      "Epoch 1 Batch 800 Loss 3.3141\n",
      "Epoch 1 Batch 850 Loss 3.3134\n",
      "Epoch 1 Batch 900 Loss 3.3131\n",
      "Epoch 1 Batch 950 Loss 3.3128\n",
      "Epoch 1 Batch 1000 Loss 3.3127\n",
      "Epoch 1 Batch 1050 Loss 3.3128\n",
      "Epoch 1 Batch 1100 Loss 3.3136\n",
      "Epoch 1 Batch 1150 Loss 3.3136\n",
      "Epoch 1 Batch 1200 Loss 3.3137\n",
      "Epoch 1 Batch 1250 Loss 3.3136\n",
      "Epoch 1 Batch 1300 Loss 3.3137\n",
      "Epoch 1 Batch 1350 Loss 3.3141\n",
      "Epoch 1 Batch 1400 Loss 3.3145\n",
      "Epoch 1 Batch 1450 Loss 3.3147\n",
      "Epoch 1 Batch 1500 Loss 3.3143\n",
      "Epoch 1 Batch 1550 Loss 3.3148\n",
      "Epoch 1 Batch 1600 Loss 3.3149\n",
      "Epoch 1 Batch 1650 Loss 3.3147\n",
      "Epoch 1 Batch 1700 Loss 3.3150\n",
      "Epoch 1 Batch 1750 Loss 3.3150\n",
      "Epoch 1 Batch 1800 Loss 3.3149\n",
      "Epoch 1 Batch 1850 Loss 3.3149\n",
      "Epoch 1 Batch 1900 Loss 3.3154\n",
      "Epoch 1 Batch 1950 Loss 3.3157\n",
      "Epoch 1 Batch 2000 Loss 3.3160\n",
      "Epoch 1 Batch 2050 Loss 3.3163\n",
      "Epoch 1 Batch 2100 Loss 3.3159\n",
      "Epoch 1 Batch 2150 Loss 3.3161\n",
      "Epoch 1 Batch 2200 Loss 3.3163\n",
      "Epoch 1 Batch 2250 Loss 3.3160\n",
      "Epoch 1 Batch 2300 Loss 3.3161\n",
      "Epoch 1 Batch 2350 Loss 3.3162\n",
      "Epoch 1 Batch 2400 Loss 3.3161\n",
      "Epoch 1 Batch 2450 Loss 3.3160\n",
      "Epoch 1 Batch 2500 Loss 3.3158\n",
      "Epoch 1 Batch 2550 Loss 3.3158\n",
      "Epoch 1 Batch 2600 Loss 3.3159\n",
      "Epoch 1 Batch 2650 Loss 3.3159\n",
      "Epoch 1 Batch 2700 Loss 3.3161\n",
      "Epoch 1 Batch 2750 Loss 3.3164\n",
      "Epoch 1 Batch 2800 Loss 3.3166\n",
      "Epoch 1 Batch 2850 Loss 3.3168\n",
      "Epoch 1 Batch 2900 Loss 3.3168\n",
      "Epoch 1 Batch 2950 Loss 3.3166\n",
      "Epoch 1 Batch 3000 Loss 3.3168\n",
      "Epoch 1 Batch 3050 Loss 3.3169\n",
      "Epoch 1 Batch 3100 Loss 3.3171\n",
      "Epoch 1 Batch 3150 Loss 3.3172\n",
      "Epoch 1 Batch 3200 Loss 3.3169\n",
      "Epoch 1 Batch 3250 Loss 3.3168\n",
      "Epoch 1 Batch 3300 Loss 3.3169\n",
      "Epoch 1 Batch 3350 Loss 3.3169\n",
      "Epoch 1 Batch 3400 Loss 3.3167\n",
      "Epoch 1 Batch 3450 Loss 3.3169\n",
      "Epoch 1 Batch 3500 Loss 3.3167\n",
      "Epoch 1 Batch 3550 Loss 3.3166\n",
      "Epoch 1 Batch 3600 Loss 3.3166\n",
      "Epoch 1 Batch 3650 Loss 3.3166\n",
      "Epoch 1 Batch 3700 Loss 3.3166\n",
      "Epoch 1 Loss 3.3166\n",
      "Time taken for 1 epoch: 1350.3032042980194 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.2105\n",
      "Epoch 2 Batch 50 Loss 3.2914\n",
      "Epoch 2 Batch 100 Loss 3.2957\n",
      "Epoch 2 Batch 150 Loss 3.2930\n",
      "Epoch 2 Batch 200 Loss 3.2976\n",
      "Epoch 2 Batch 250 Loss 3.2988\n",
      "Epoch 2 Batch 300 Loss 3.2997\n",
      "Epoch 2 Batch 350 Loss 3.2989\n",
      "Epoch 2 Batch 400 Loss 3.2995\n",
      "Epoch 2 Batch 450 Loss 3.2999\n",
      "Epoch 2 Batch 500 Loss 3.3013\n",
      "Epoch 2 Batch 550 Loss 3.3028\n",
      "Epoch 2 Batch 600 Loss 3.3025\n",
      "Epoch 2 Batch 650 Loss 3.3034\n",
      "Epoch 2 Batch 700 Loss 3.3034\n",
      "Epoch 2 Batch 750 Loss 3.3037\n",
      "Epoch 2 Batch 800 Loss 3.3040\n",
      "Epoch 2 Batch 850 Loss 3.3046\n",
      "Epoch 2 Batch 900 Loss 3.3045\n",
      "Epoch 2 Batch 950 Loss 3.3046\n",
      "Epoch 2 Batch 1000 Loss 3.3046\n",
      "Epoch 2 Batch 1050 Loss 3.3055\n",
      "Epoch 2 Batch 1100 Loss 3.3060\n",
      "Epoch 2 Batch 1150 Loss 3.3060\n",
      "Epoch 2 Batch 1200 Loss 3.3053\n",
      "Epoch 2 Batch 1250 Loss 3.3055\n",
      "Epoch 2 Batch 1300 Loss 3.3061\n",
      "Epoch 2 Batch 1350 Loss 3.3061\n",
      "Epoch 2 Batch 1400 Loss 3.3066\n",
      "Epoch 2 Batch 1450 Loss 3.3071\n",
      "Epoch 2 Batch 1500 Loss 3.3071\n",
      "Epoch 2 Batch 1550 Loss 3.3074\n",
      "Epoch 2 Batch 1600 Loss 3.3070\n",
      "Epoch 2 Batch 1650 Loss 3.3077\n",
      "Epoch 2 Batch 1700 Loss 3.3075\n",
      "Epoch 2 Batch 1750 Loss 3.3077\n",
      "Epoch 2 Batch 1800 Loss 3.3083\n",
      "Epoch 2 Batch 1850 Loss 3.3082\n",
      "Epoch 2 Batch 1900 Loss 3.3084\n",
      "Epoch 2 Batch 1950 Loss 3.3086\n",
      "Epoch 2 Batch 2000 Loss 3.3087\n",
      "Epoch 2 Batch 2050 Loss 3.3086\n",
      "Epoch 2 Batch 2100 Loss 3.3090\n",
      "Epoch 2 Batch 2150 Loss 3.3088\n",
      "Epoch 2 Batch 2200 Loss 3.3089\n",
      "Epoch 2 Batch 2250 Loss 3.3088\n",
      "Epoch 2 Batch 2300 Loss 3.3087\n",
      "Epoch 2 Batch 2350 Loss 3.3091\n",
      "Epoch 2 Batch 2400 Loss 3.3092\n",
      "Epoch 2 Batch 2450 Loss 3.3094\n",
      "Epoch 2 Batch 2500 Loss 3.3093\n",
      "Epoch 2 Batch 2550 Loss 3.3092\n",
      "Epoch 2 Batch 2600 Loss 3.3091\n",
      "Epoch 2 Batch 2650 Loss 3.3092\n",
      "Epoch 2 Batch 2700 Loss 3.3098\n",
      "Epoch 2 Batch 2750 Loss 3.3099\n",
      "Epoch 2 Batch 2800 Loss 3.3100\n",
      "Epoch 2 Batch 2850 Loss 3.3101\n",
      "Epoch 2 Batch 2900 Loss 3.3100\n",
      "Epoch 2 Batch 2950 Loss 3.3102\n",
      "Epoch 2 Batch 3000 Loss 3.3103\n",
      "Epoch 2 Batch 3050 Loss 3.3103\n",
      "Epoch 2 Batch 3100 Loss 3.3103\n",
      "Epoch 2 Batch 3150 Loss 3.3102\n",
      "Epoch 2 Batch 3200 Loss 3.3102\n",
      "Epoch 2 Batch 3250 Loss 3.3101\n",
      "Epoch 2 Batch 3300 Loss 3.3101\n",
      "Epoch 2 Batch 3350 Loss 3.3101\n",
      "Epoch 2 Batch 3400 Loss 3.3103\n",
      "Epoch 2 Batch 3450 Loss 3.3101\n",
      "Epoch 2 Batch 3500 Loss 3.3101\n",
      "Epoch 2 Batch 3550 Loss 3.3102\n",
      "Epoch 2 Batch 3600 Loss 3.3101\n",
      "Epoch 2 Batch 3650 Loss 3.3101\n",
      "Epoch 2 Batch 3700 Loss 3.3100\n",
      "Saving checkpoint for epoch 2 at /home/aiffel0042/aiffel/music_transformer/models/ckpt-12\n",
      "Epoch 2 Loss 3.3100\n",
      "Time taken for 1 epoch: 1362.8976438045502 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EPOCHS = 20  \n",
    "EPOCHS = 2  # 1epoch가 매우 오래 걸립니다. \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _ = music_transformer(inp, True, None, None, None)\n",
    "            loss = loss_function(tar, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, music_transformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, music_transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result()))\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generation 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.getenv('HOME')+'/aiffel/music_transformer/models/10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aiffel0042/aiffel/music_transformer/models/10/ckpt-5'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/aiffel/music_transformer/models/10/ckpt-9\n",
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.getenv('HOME')+'/aiffel/music_transformer/models/10'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(music_transformer=music_transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    print(ckpt_manager.latest_checkpoint)\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad, train_label_pad))\n",
    "test_dataset = test_dataset.map(tensor_casting)\n",
    "test_dataset = test_dataset.shuffle(10000).batch(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "185\n",
      "3\n",
      "320\n",
      "1\n",
      "119\n",
      "202\n",
      "1\n",
      "388\n",
      "4\n",
      "108\n",
      "202\n",
      "1\n",
      "110\n",
      "185\n",
      "8\n",
      "328\n",
      "4\n",
      "115\n",
      "180\n",
      "5\n",
      "113\n",
      "191\n",
      "2\n",
      "309\n",
      "7\n",
      "116\n",
      "194\n",
      "5\n",
      "316\n",
      "7\n",
      "302\n",
      "7\n",
      "117\n",
      "186\n",
      "2\n",
      "105\n",
      "176\n",
      "3\n",
      "324\n",
      "6\n",
      "332\n",
      "4\n",
      "119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 300\n",
    "_inputs = np.zeros([1, N], dtype=np.int32)\n",
    "\n",
    "for x, y in test_dataset.take(1):\n",
    "    _inputs[:, :length] = x[None, :]\n",
    "    \n",
    "for i in range(N - length):\n",
    "    predictions, _ = music_transformer(_inputs[:, i:i+length], False, None, None, None)\n",
    "    predictions = tf.squeeze(predictions, 0)    \n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    print(predicted_id)\n",
    "    \n",
    "    # 예측된 단어를 다음 입력으로 모델에 전달\n",
    "    # 이전 은닉 상태와 함께\n",
    "    _inputs[:, i+length] = predicted_id\n",
    "\n",
    "_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Event():\n",
    "    def __init__(self, time, note, cc, on, velocity):\n",
    "        self.time = time\n",
    "        self.note = note\n",
    "        self.on = on\n",
    "        self.cc = cc\n",
    "        self.velocity = velocity\n",
    "\n",
    "    def get_event_sequence(self):\n",
    "        return [self.time, self.note, int(self.on)]\n",
    "\n",
    "class Note():\n",
    "    def __init__(self):\n",
    "        self.pitch = 0\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_list = []\n",
    "time = 0\n",
    "event = None\n",
    "\n",
    "EventDim = IntervalDim + VelocityDim + NoteOnDim + NoteOffDim # 388\n",
    "\n",
    "for _input in _inputs[0]:\n",
    "    # interval\n",
    "    if _input < IntervalDim: \n",
    "        time += _input\n",
    "        event = Event(time, 0, False, 0, 0)\n",
    "\n",
    "    # velocity\n",
    "    elif _input < NoteOnOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.velocity = (_input - VelocityOffset) / VelocityDim * 128\n",
    "\n",
    "    # note on\n",
    "    elif _input < NoteOffOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "\n",
    "        event.note = _input - NoteOnOffset\n",
    "        event.on = True\n",
    "        event_list.append(event)\n",
    "\n",
    "        event = None\n",
    "\n",
    "    # note off\n",
    "    elif _input < CCOffset:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.note = _input - NoteOffOffset\n",
    "        event.on = False\n",
    "        event_list.append(event)\n",
    "        event = None\n",
    "\n",
    "    ## CC\n",
    "    else:\n",
    "        if event is None:\n",
    "            continue\n",
    "        event.cc = True\n",
    "        on = _input - CCOffset == 1\n",
    "        event.on = on\n",
    "        event_list.append(event)\n",
    "        event = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track 0: \n",
      "<meta message set_tempo tempo=666667 time=0>\n",
      "note_on channel=0 note=59 velocity=56 time=1\n",
      "note_off channel=0 note=59 velocity=0 time=1\n",
      "note_on channel=0 note=62 velocity=60 time=1\n",
      "note_off channel=0 note=62 velocity=0 time=1\n",
      "note_off channel=0 note=47 velocity=0 time=0\n",
      "note_on channel=0 note=68 velocity=60 time=1\n",
      "note_off channel=0 note=68 velocity=0 time=1\n",
      "note_on channel=0 note=47 velocity=60 time=3\n",
      "note_on channel=0 note=42 velocity=52 time=0\n",
      "note_off channel=0 note=41 velocity=0 time=0\n",
      "note_on channel=0 note=48 velocity=52 time=3\n",
      "note_off channel=0 note=48 velocity=0 time=1\n",
      "note_on channel=0 note=49 velocity=48 time=2\n",
      "note_off channel=0 note=49 velocity=0 time=1\n",
      "note_on channel=0 note=51 velocity=56 time=2\n",
      "note_off channel=0 note=51 velocity=0 time=1\n",
      "note_off channel=0 note=42 velocity=0 time=2\n",
      "note_on channel=0 note=46 velocity=60 time=0\n",
      "note_on channel=0 note=52 velocity=56 time=0\n",
      "note_off channel=0 note=47 velocity=0 time=1\n",
      "note_off channel=0 note=52 velocity=0 time=0\n",
      "note_on channel=0 note=56 velocity=64 time=2\n",
      "note_off channel=0 note=56 velocity=0 time=1\n",
      "note_on channel=0 note=54 velocity=60 time=2\n",
      "note_off channel=0 note=54 velocity=0 time=2\n",
      "note_on channel=0 note=52 velocity=52 time=1\n",
      "note_off channel=0 note=52 velocity=0 time=1\n",
      "note_on channel=0 note=47 velocity=56 time=3\n",
      "note_on channel=0 note=35 velocity=40 time=0\n",
      "note_on channel=0 note=51 velocity=56 time=0\n",
      "note_off channel=0 note=51 velocity=0 time=1\n",
      "note_on channel=0 note=54 velocity=56 time=2\n",
      "note_off channel=0 note=46 velocity=0 time=0\n",
      "note_off channel=0 note=54 velocity=0 time=2\n",
      "note_on channel=0 note=59 velocity=24 time=2\n",
      "note_off channel=0 note=35 velocity=0 time=0\n",
      "note_off channel=0 note=47 velocity=0 time=0\n",
      "note_off channel=0 note=59 velocity=0 time=0\n",
      "note_on channel=0 note=62 velocity=72 time=1\n",
      "note_off channel=0 note=62 velocity=0 time=1\n",
      "note_on channel=0 note=63 velocity=64 time=3\n",
      "note_off channel=0 note=63 velocity=0 time=1\n",
      "note_on channel=0 note=66 velocity=68 time=2\n",
      "note_off channel=0 note=66 velocity=0 time=1\n",
      "note_on channel=0 note=71 velocity=76 time=1\n",
      "note_off channel=0 note=71 velocity=0 time=1\n",
      "note_on channel=0 note=75 velocity=68 time=3\n",
      "note_off channel=0 note=75 velocity=0 time=1\n",
      "note_on channel=0 note=54 velocity=80 time=0\n",
      "note_on channel=0 note=46 velocity=64 time=0\n",
      "note_on channel=0 note=73 velocity=76 time=1\n",
      "note_off channel=0 note=46 velocity=0 time=1\n",
      "note_off channel=0 note=73 velocity=0 time=0\n",
      "note_on channel=0 note=70 velocity=56 time=1\n",
      "note_off channel=0 note=70 velocity=0 time=1\n",
      "note_on channel=0 note=66 velocity=56 time=2\n",
      "note_off channel=0 note=66 velocity=0 time=1\n",
      "note_on channel=0 note=64 velocity=64 time=2\n",
      "note_off channel=0 note=64 velocity=0 time=1\n",
      "note_on channel=0 note=42 velocity=56 time=1\n",
      "note_on channel=0 note=61 velocity=64 time=0\n",
      "note_off channel=0 note=61 velocity=0 time=1\n",
      "note_off channel=0 note=42 velocity=0 time=1\n",
      "note_on channel=0 note=58 velocity=64 time=1\n",
      "control_change channel=0 control=64 value=30 time=0\n",
      "note_off channel=0 note=58 velocity=0 time=1\n",
      "note_on channel=0 note=61 velocity=56 time=2\n",
      "control_change channel=0 control=64 value=127 time=0\n",
      "note_off channel=0 note=61 velocity=0 time=2\n",
      "note_on channel=0 note=64 velocity=56 time=0\n",
      "note_off channel=0 note=64 velocity=0 time=1\n",
      "note_on channel=0 note=47 velocity=52 time=3\n",
      "note_off channel=0 note=54 velocity=0 time=2\n",
      "note_off channel=0 note=47 velocity=0 time=1\n",
      "note_on channel=0 note=63 velocity=60 time=1\n",
      "note_on channel=0 note=61 velocity=64 time=3\n",
      "note_off channel=0 note=63 velocity=0 time=0\n",
      "note_on channel=0 note=59 velocity=48 time=3\n",
      "note_off channel=0 note=61 velocity=0 time=0\n",
      "note_off channel=0 note=59 velocity=0 time=3\n",
      "note_on channel=0 note=37 velocity=56 time=1\n",
      "note_on channel=0 note=53 velocity=64 time=0\n",
      "note_on channel=0 note=25 velocity=48 time=0\n",
      "note_on channel=0 note=56 velocity=60 time=4\n",
      "note_on channel=0 note=62 velocity=72 time=3\n",
      "note_off channel=0 note=25 velocity=0 time=1\n",
      "note_off channel=0 note=37 velocity=0 time=0\n",
      "note_off channel=0 note=53 velocity=0 time=1\n",
      "note_on channel=0 note=59 velocity=68 time=0\n",
      "note_off channel=0 note=56 velocity=0 time=1\n",
      "note_off channel=0 note=62 velocity=0 time=0\n",
      "note_on channel=0 note=37 velocity=76 time=2\n",
      "note_on channel=0 note=25 velocity=64 time=0\n",
      "note_on channel=0 note=53 velocity=84 time=0\n",
      "note_off channel=0 note=59 velocity=0 time=2\n",
      "note_on channel=0 note=56 velocity=68 time=0\n",
      "note_on channel=0 note=62 velocity=80 time=4\n",
      "note_off channel=0 note=37 velocity=0 time=1\n",
      "note_off channel=0 note=25 velocity=0 time=1\n",
      "note_on channel=0 note=59 velocity=76 time=0\n",
      "note_off channel=0 note=53 velocity=0 time=0\n",
      "note_off channel=0 note=56 velocity=0 time=0\n",
      "note_on channel=0 note=53 velocity=84 time=0\n",
      "note_off channel=0 note=60 velocity=0 time=1\n",
      "note_on channel=0 note=70 velocity=76 time=0\n",
      "control_change channel=0 control=64 value=30 time=0\n",
      "note_off channel=0 note=70 velocity=0 time=0\n",
      "note_on channel=0 note=70 velocity=32 time=1\n",
      "note_off channel=0 note=53 velocity=0 time=0\n",
      "note_on channel=0 note=53 velocity=40 time=0\n",
      "note_off channel=0 note=68 velocity=0 time=2\n",
      "note_on channel=0 note=48 velocity=60 time=1\n",
      "note_off channel=0 note=59 velocity=0 time=0\n",
      "note_on channel=0 note=59 velocity=52 time=1\n",
      "note_off channel=0 note=49 velocity=0 time=0\n",
      "note_off channel=0 note=62 velocity=0 time=0\n",
      "note_on channel=0 note=62 velocity=64 time=2\n",
      "note_off channel=0 note=56 velocity=0 time=1\n",
      "note_off channel=0 note=42 velocity=0 time=2\n",
      "note_on channel=0 note=54 velocity=68 time=2\n",
      "note_on channel=0 note=44 velocity=20 time=0\n",
      "note_off channel=0 note=64 velocity=0 time=1\n",
      "note_off channel=0 note=72 velocity=0 time=2\n",
      "<meta message end_of_track time=0>\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from mido import Message, MidiFile, MidiTrack, MetaMessage, bpm2tempo\n",
    "\n",
    "midi = MidiFile()\n",
    "output_midi_path = os.getenv('HOME')+'/aiffel/music_transformer/data/output_file11_.mid'\n",
    "\n",
    "# Instantiate a MIDI Track (contains a list of MIDI events)\n",
    "track = MidiTrack()\n",
    "track.append(MetaMessage(\"set_tempo\", tempo=bpm2tempo(90)))\n",
    "# Append the track to the pattern\n",
    "midi.tracks.append(track)\n",
    "\n",
    "prev_time = 0\n",
    "pitches = [None for _ in range(128)]\n",
    "for event in event_list:\n",
    "    tick = (event.time - prev_time) // 3\n",
    "    midi.ticks_per_beat = 6\n",
    "    prev_time = event.time\n",
    "\n",
    "    # case NOTE:\n",
    "    if not event.cc:\n",
    "        if event.on:\n",
    "            if pitches[event.note] is not None:\n",
    "                # Instantiate a MIDI note off event, append it to the track\n",
    "                off = Message('note_off', note=event.note, velocity=0, time=0)\n",
    "                track.append(off)\n",
    "                pitches[event.note] = None\n",
    "\n",
    "            # Instantiate a MIDI note on event, append it to the track\n",
    "            on = Message('note_on', note=event.note, velocity=int(event.velocity), time=tick)\n",
    "            track.append(on)\n",
    "            pitches[event.note] = prev_time\n",
    "        else:\n",
    "            # Instantiate a MIDI note off event, append it to the track\n",
    "            off = Message('note_off', note=event.note, velocity=0, time=tick)\n",
    "            track.append(off)\n",
    "            pitches[event.note] = None\n",
    "\n",
    "#     case CC:\n",
    "    elif event.cc:\n",
    "        if event.on:\n",
    "            cc = Message('control_change', control=64, time=tick, value=127)\n",
    "        else:\n",
    "            cc = Message('control_change', control=64, time=tick, value=30)\n",
    "\n",
    "        track.append(cc)\n",
    "\n",
    "    for pitch in range(128):\n",
    "        if pitches[pitch] is not None and pitches[pitch] + 100 < prev_time:\n",
    "            off = Message('note_off', note=pitch, velocity=0, time=0)\n",
    "            track.append(off)\n",
    "            pitches[pitch] = None\n",
    "\n",
    "\n",
    "# Add the end of track event, append it to the track\n",
    "track.append(MetaMessage(\"end_of_track\"))\n",
    "\n",
    "# Save the pattern to disk\n",
    "midi.save(output_midi_path)\n",
    "\n",
    "for i, track in enumerate(midi.tracks):\n",
    "    print('Track {}: {}'.format(i, track.name))\n",
    "    for msg in track:\n",
    "        print(msg)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
