{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow((train_data[i]+1)/2, cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 0:\n",
    "            new_t_labels.append([0])\n",
    "        else:\n",
    "            new_t_labels.append([1])\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 50,000(5,000 x 10 classes)   train data   ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  # Generator 32X32X3    . \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/proj_ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer generator_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer discriminator_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Steps : 100, \t Total Gen Loss : 13.688617706298828, \t Total Dis Loss : 0.6913903951644897\n",
      "Steps : 200, \t Total Gen Loss : 15.849128723144531, \t Total Dis Loss : 0.6769456267356873\n",
      "Steps : 300, \t Total Gen Loss : 14.304644584655762, \t Total Dis Loss : 0.6766873598098755\n",
      "Steps : 400, \t Total Gen Loss : 13.693676948547363, \t Total Dis Loss : 0.5119608640670776\n",
      "Steps : 500, \t Total Gen Loss : 16.066085815429688, \t Total Dis Loss : 0.23394039273262024\n",
      "Steps : 600, \t Total Gen Loss : 14.38297176361084, \t Total Dis Loss : 0.43790239095687866\n",
      "Steps : 700, \t Total Gen Loss : 16.141950607299805, \t Total Dis Loss : 0.09018167853355408\n",
      "Steps : 800, \t Total Gen Loss : 14.927329063415527, \t Total Dis Loss : 0.25361019372940063\n",
      "Steps : 900, \t Total Gen Loss : 18.212583541870117, \t Total Dis Loss : 0.119361013174057\n",
      "Steps : 1000, \t Total Gen Loss : 19.347759246826172, \t Total Dis Loss : 0.03460397943854332\n",
      "Steps : 1100, \t Total Gen Loss : 20.598772048950195, \t Total Dis Loss : 0.02976756915450096\n",
      "Steps : 1200, \t Total Gen Loss : 17.439401626586914, \t Total Dis Loss : 0.04735117405653\n",
      "Steps : 1300, \t Total Gen Loss : 16.860458374023438, \t Total Dis Loss : 0.19637061655521393\n",
      "Steps : 1400, \t Total Gen Loss : 19.74872589111328, \t Total Dis Loss : 0.019751161336898804\n",
      "Steps : 1500, \t Total Gen Loss : 20.68160629272461, \t Total Dis Loss : 0.01912730559706688\n",
      "Steps : 1600, \t Total Gen Loss : 18.533212661743164, \t Total Dis Loss : 0.0177315566688776\n",
      "Steps : 1700, \t Total Gen Loss : 19.790037155151367, \t Total Dis Loss : 0.02656525932252407\n",
      "Steps : 1800, \t Total Gen Loss : 21.911300659179688, \t Total Dis Loss : 0.01760590448975563\n",
      "Steps : 1900, \t Total Gen Loss : 20.573816299438477, \t Total Dis Loss : 0.21941784024238586\n",
      "Steps : 2000, \t Total Gen Loss : 20.010547637939453, \t Total Dis Loss : 0.020983565598726273\n",
      "Steps : 2100, \t Total Gen Loss : 21.50273323059082, \t Total Dis Loss : 0.02158818393945694\n",
      "Steps : 2200, \t Total Gen Loss : 19.851774215698242, \t Total Dis Loss : 0.0632406547665596\n",
      "Steps : 2300, \t Total Gen Loss : 22.428220748901367, \t Total Dis Loss : 0.011350485496222973\n",
      "Steps : 2400, \t Total Gen Loss : 19.469451904296875, \t Total Dis Loss : 0.34065449237823486\n",
      "Steps : 2500, \t Total Gen Loss : 17.759292602539062, \t Total Dis Loss : 0.12828293442726135\n",
      "Steps : 2600, \t Total Gen Loss : 20.222150802612305, \t Total Dis Loss : 0.020073864609003067\n",
      "Steps : 2700, \t Total Gen Loss : 18.78284454345703, \t Total Dis Loss : 0.01885104365646839\n",
      "Steps : 2800, \t Total Gen Loss : 20.434337615966797, \t Total Dis Loss : 0.2532658576965332\n",
      "Steps : 2900, \t Total Gen Loss : 19.836715698242188, \t Total Dis Loss : 0.0683637484908104\n",
      "Steps : 3000, \t Total Gen Loss : 23.288148880004883, \t Total Dis Loss : 0.0042679524049162865\n",
      "Steps : 3100, \t Total Gen Loss : 19.545129776000977, \t Total Dis Loss : 0.007440537679940462\n",
      "Steps : 3200, \t Total Gen Loss : 19.299623489379883, \t Total Dis Loss : 0.010434871539473534\n",
      "Steps : 3300, \t Total Gen Loss : 21.626440048217773, \t Total Dis Loss : 0.011069676838815212\n",
      "Steps : 3400, \t Total Gen Loss : 20.983661651611328, \t Total Dis Loss : 0.007687815465033054\n",
      "Steps : 3500, \t Total Gen Loss : 20.94451141357422, \t Total Dis Loss : 0.008922346867620945\n",
      "Steps : 3600, \t Total Gen Loss : 20.409713745117188, \t Total Dis Loss : 0.007101569324731827\n",
      "Steps : 3700, \t Total Gen Loss : 21.43992805480957, \t Total Dis Loss : 0.009521545842289925\n",
      "Steps : 3800, \t Total Gen Loss : 21.145000457763672, \t Total Dis Loss : 0.04824022948741913\n",
      "Steps : 3900, \t Total Gen Loss : 21.584157943725586, \t Total Dis Loss : 0.00849185511469841\n",
      "Steps : 4000, \t Total Gen Loss : 20.771686553955078, \t Total Dis Loss : 0.01596330665051937\n",
      "Steps : 4100, \t Total Gen Loss : 19.936845779418945, \t Total Dis Loss : 0.004959309473633766\n",
      "Steps : 4200, \t Total Gen Loss : 20.925556182861328, \t Total Dis Loss : 0.0073249731212854385\n",
      "Steps : 4300, \t Total Gen Loss : 22.072946548461914, \t Total Dis Loss : 0.0019807033240795135\n",
      "Steps : 4400, \t Total Gen Loss : 19.94485092163086, \t Total Dis Loss : 0.002333658514544368\n",
      "Steps : 4500, \t Total Gen Loss : 20.613567352294922, \t Total Dis Loss : 0.03471112623810768\n",
      "Steps : 4600, \t Total Gen Loss : 21.30052947998047, \t Total Dis Loss : 0.0035091531462967396\n",
      "Steps : 4700, \t Total Gen Loss : 21.394622802734375, \t Total Dis Loss : 0.04018286243081093\n",
      "Steps : 4800, \t Total Gen Loss : 23.026683807373047, \t Total Dis Loss : 0.0036492757499217987\n",
      "Steps : 4900, \t Total Gen Loss : 21.798667907714844, \t Total Dis Loss : 0.005989965051412582\n",
      "Steps : 5000, \t Total Gen Loss : 20.92107391357422, \t Total Dis Loss : 0.020670831203460693\n",
      "Steps : 5100, \t Total Gen Loss : 22.131696701049805, \t Total Dis Loss : 0.01802191324532032\n",
      "Steps : 5200, \t Total Gen Loss : 22.107810974121094, \t Total Dis Loss : 0.0022456240840256214\n",
      "Steps : 5300, \t Total Gen Loss : 24.410884857177734, \t Total Dis Loss : 0.004666492342948914\n",
      "Steps : 5400, \t Total Gen Loss : 23.069351196289062, \t Total Dis Loss : 0.004509192891418934\n",
      "Steps : 5500, \t Total Gen Loss : 22.349964141845703, \t Total Dis Loss : 0.049092892557382584\n",
      "Steps : 5600, \t Total Gen Loss : 19.564380645751953, \t Total Dis Loss : 0.00306139187887311\n",
      "Time for epoch 1 is 286.419145822525 sec\n",
      "Steps : 5700, \t Total Gen Loss : 24.158004760742188, \t Total Dis Loss : 0.006084563210606575\n",
      "Steps : 5800, \t Total Gen Loss : 25.49693489074707, \t Total Dis Loss : 0.0034541694913059473\n",
      "Steps : 5900, \t Total Gen Loss : 23.339200973510742, \t Total Dis Loss : 0.0037008095532655716\n",
      "Steps : 6000, \t Total Gen Loss : 21.613868713378906, \t Total Dis Loss : 0.001971929334104061\n",
      "Steps : 6100, \t Total Gen Loss : 25.511966705322266, \t Total Dis Loss : 0.017552470788359642\n",
      "Steps : 6200, \t Total Gen Loss : 24.424434661865234, \t Total Dis Loss : 0.0027407112065702677\n",
      "Steps : 6300, \t Total Gen Loss : 20.886411666870117, \t Total Dis Loss : 0.005665245000272989\n",
      "Steps : 6400, \t Total Gen Loss : 23.477081298828125, \t Total Dis Loss : 0.10944550484418869\n",
      "Steps : 6500, \t Total Gen Loss : 24.478235244750977, \t Total Dis Loss : 0.005264444276690483\n",
      "Steps : 6600, \t Total Gen Loss : 22.430583953857422, \t Total Dis Loss : 0.006431449204683304\n",
      "Steps : 6700, \t Total Gen Loss : 20.22974395751953, \t Total Dis Loss : 0.005260216072201729\n",
      "Steps : 6800, \t Total Gen Loss : 23.979122161865234, \t Total Dis Loss : 0.005644990596920252\n",
      "Steps : 6900, \t Total Gen Loss : 22.7058162689209, \t Total Dis Loss : 0.019940489903092384\n",
      "Steps : 7000, \t Total Gen Loss : 21.921466827392578, \t Total Dis Loss : 0.005697421729564667\n",
      "Steps : 7100, \t Total Gen Loss : 23.733366012573242, \t Total Dis Loss : 0.008258819580078125\n",
      "Steps : 7200, \t Total Gen Loss : 21.962940216064453, \t Total Dis Loss : 0.006312821991741657\n",
      "Steps : 7300, \t Total Gen Loss : 23.409133911132812, \t Total Dis Loss : 0.003658501897007227\n",
      "Steps : 7400, \t Total Gen Loss : 21.539472579956055, \t Total Dis Loss : 0.008394644595682621\n",
      "Steps : 7500, \t Total Gen Loss : 20.836366653442383, \t Total Dis Loss : 0.00342703890055418\n",
      "Steps : 7600, \t Total Gen Loss : 21.075029373168945, \t Total Dis Loss : 0.005170788615942001\n",
      "Steps : 7700, \t Total Gen Loss : 21.974018096923828, \t Total Dis Loss : 0.0037676645442843437\n",
      "Steps : 7800, \t Total Gen Loss : 23.4094181060791, \t Total Dis Loss : 0.008087516762316227\n",
      "Steps : 7900, \t Total Gen Loss : 20.86464500427246, \t Total Dis Loss : 0.001928874058648944\n",
      "Steps : 8000, \t Total Gen Loss : 25.929492950439453, \t Total Dis Loss : 0.001550844986923039\n",
      "Steps : 8100, \t Total Gen Loss : 24.507549285888672, \t Total Dis Loss : 0.0027782993856817484\n",
      "Steps : 8200, \t Total Gen Loss : 22.864965438842773, \t Total Dis Loss : 0.0019274060614407063\n",
      "Steps : 8300, \t Total Gen Loss : 25.373764038085938, \t Total Dis Loss : 0.005941011011600494\n",
      "Steps : 8400, \t Total Gen Loss : 23.37628173828125, \t Total Dis Loss : 0.001206893241032958\n",
      "Steps : 8500, \t Total Gen Loss : 22.303438186645508, \t Total Dis Loss : 0.004862606525421143\n",
      "Steps : 8600, \t Total Gen Loss : 21.689762115478516, \t Total Dis Loss : 0.9275810122489929\n",
      "Steps : 8700, \t Total Gen Loss : 22.80723762512207, \t Total Dis Loss : 0.011288060806691647\n",
      "Steps : 8800, \t Total Gen Loss : 24.281814575195312, \t Total Dis Loss : 0.002088000299409032\n",
      "Steps : 8900, \t Total Gen Loss : 25.8720703125, \t Total Dis Loss : 0.006202802062034607\n",
      "Steps : 9000, \t Total Gen Loss : 22.49700927734375, \t Total Dis Loss : 0.03751201182603836\n",
      "Steps : 9100, \t Total Gen Loss : 23.506624221801758, \t Total Dis Loss : 0.004525660537183285\n",
      "Steps : 9200, \t Total Gen Loss : 22.065494537353516, \t Total Dis Loss : 0.012677846476435661\n",
      "Steps : 9300, \t Total Gen Loss : 21.98094367980957, \t Total Dis Loss : 0.007161696441471577\n",
      "Steps : 9400, \t Total Gen Loss : 23.59343147277832, \t Total Dis Loss : 0.0020931593608111143\n",
      "Steps : 9500, \t Total Gen Loss : 24.901561737060547, \t Total Dis Loss : 0.0008318654145114124\n",
      "Steps : 9600, \t Total Gen Loss : 25.383859634399414, \t Total Dis Loss : 0.0009693739702925086\n",
      "Steps : 9700, \t Total Gen Loss : 23.453760147094727, \t Total Dis Loss : 0.004460139200091362\n",
      "Steps : 9800, \t Total Gen Loss : 22.506683349609375, \t Total Dis Loss : 0.008453899063169956\n",
      "Steps : 9900, \t Total Gen Loss : 25.020780563354492, \t Total Dis Loss : 0.003621787764132023\n",
      "Steps : 10000, \t Total Gen Loss : 23.193130493164062, \t Total Dis Loss : 0.005611644592136145\n",
      "Steps : 10100, \t Total Gen Loss : 20.802644729614258, \t Total Dis Loss : 0.10302244871854782\n",
      "Steps : 10200, \t Total Gen Loss : 20.21857452392578, \t Total Dis Loss : 0.01944870315492153\n",
      "Steps : 10300, \t Total Gen Loss : 21.1658935546875, \t Total Dis Loss : 0.0009325575665570796\n",
      "Steps : 10400, \t Total Gen Loss : 19.499155044555664, \t Total Dis Loss : 0.0014691570540890098\n",
      "Steps : 10500, \t Total Gen Loss : 24.324142456054688, \t Total Dis Loss : 0.006000840105116367\n",
      "Steps : 10600, \t Total Gen Loss : 22.887054443359375, \t Total Dis Loss : 0.0011498548556119204\n",
      "Steps : 10700, \t Total Gen Loss : 22.112756729125977, \t Total Dis Loss : 0.0031308128964155912\n",
      "Steps : 10800, \t Total Gen Loss : 21.30381965637207, \t Total Dis Loss : 0.0028933179564774036\n",
      "Steps : 10900, \t Total Gen Loss : 24.527807235717773, \t Total Dis Loss : 0.0007519952487200499\n",
      "Steps : 11000, \t Total Gen Loss : 23.600614547729492, \t Total Dis Loss : 0.0029304465278983116\n",
      "Steps : 11100, \t Total Gen Loss : 25.00953483581543, \t Total Dis Loss : 0.005974192172288895\n",
      "Steps : 11200, \t Total Gen Loss : 20.107093811035156, \t Total Dis Loss : 0.029516318812966347\n",
      "Time for epoch 2 is 277.58388662338257 sec\n",
      "Steps : 11300, \t Total Gen Loss : 24.5150203704834, \t Total Dis Loss : 0.00534058827906847\n",
      "Steps : 11400, \t Total Gen Loss : 23.912029266357422, \t Total Dis Loss : 0.0020156698301434517\n",
      "Steps : 11500, \t Total Gen Loss : 25.114561080932617, \t Total Dis Loss : 0.0031585749238729477\n",
      "Steps : 11600, \t Total Gen Loss : 19.098844528198242, \t Total Dis Loss : 0.6052582859992981\n",
      "Steps : 11700, \t Total Gen Loss : 22.300979614257812, \t Total Dis Loss : 0.02726435661315918\n",
      "Steps : 11800, \t Total Gen Loss : 21.299606323242188, \t Total Dis Loss : 0.0015728676225990057\n",
      "Steps : 11900, \t Total Gen Loss : 22.124570846557617, \t Total Dis Loss : 0.0025128978304564953\n",
      "Steps : 12000, \t Total Gen Loss : 24.968059539794922, \t Total Dis Loss : 0.006431525107473135\n",
      "Steps : 12100, \t Total Gen Loss : 23.026493072509766, \t Total Dis Loss : 0.004477785434573889\n",
      "Steps : 12200, \t Total Gen Loss : 23.647146224975586, \t Total Dis Loss : 0.0023057018406689167\n",
      "Steps : 12300, \t Total Gen Loss : 23.033309936523438, \t Total Dis Loss : 0.005253732670098543\n",
      "Steps : 12400, \t Total Gen Loss : 23.32197380065918, \t Total Dis Loss : 0.000891258823685348\n",
      "Steps : 12500, \t Total Gen Loss : 23.823368072509766, \t Total Dis Loss : 0.0009067851351574063\n",
      "Steps : 12600, \t Total Gen Loss : 27.423744201660156, \t Total Dis Loss : 0.0006403325824066997\n",
      "Steps : 12700, \t Total Gen Loss : 24.040271759033203, \t Total Dis Loss : 0.000834539532661438\n",
      "Steps : 12800, \t Total Gen Loss : 24.407817840576172, \t Total Dis Loss : 0.0018774840282276273\n",
      "Steps : 12900, \t Total Gen Loss : 22.242725372314453, \t Total Dis Loss : 0.0015485791955143213\n",
      "Steps : 13000, \t Total Gen Loss : 26.960756301879883, \t Total Dis Loss : 0.0007124110125005245\n",
      "Steps : 13100, \t Total Gen Loss : 24.478368759155273, \t Total Dis Loss : 0.0014312968123704195\n",
      "Steps : 13200, \t Total Gen Loss : 21.226972579956055, \t Total Dis Loss : 0.2789381742477417\n",
      "Steps : 13300, \t Total Gen Loss : 20.563871383666992, \t Total Dis Loss : 0.01701783761382103\n",
      "Steps : 13400, \t Total Gen Loss : 23.776371002197266, \t Total Dis Loss : 0.0037089185789227486\n",
      "Steps : 13500, \t Total Gen Loss : 23.64144515991211, \t Total Dis Loss : 0.011339597404003143\n",
      "Steps : 13600, \t Total Gen Loss : 22.38582992553711, \t Total Dis Loss : 0.005167076364159584\n",
      "Steps : 13700, \t Total Gen Loss : 25.39599609375, \t Total Dis Loss : 0.0016124239191412926\n",
      "Steps : 13800, \t Total Gen Loss : 23.035858154296875, \t Total Dis Loss : 0.0010137525387108326\n",
      "Steps : 13900, \t Total Gen Loss : 23.297826766967773, \t Total Dis Loss : 0.0048431814648211\n",
      "Steps : 14000, \t Total Gen Loss : 23.30445671081543, \t Total Dis Loss : 0.0012481837766245008\n",
      "Steps : 14100, \t Total Gen Loss : 27.223892211914062, \t Total Dis Loss : 0.0021427820902317762\n",
      "Steps : 14200, \t Total Gen Loss : 21.76980209350586, \t Total Dis Loss : 0.003970370627939701\n",
      "Steps : 14300, \t Total Gen Loss : 25.61212730407715, \t Total Dis Loss : 0.0008412542520090938\n",
      "Steps : 14400, \t Total Gen Loss : 31.172740936279297, \t Total Dis Loss : 0.0010880731279030442\n",
      "Steps : 14500, \t Total Gen Loss : 27.485273361206055, \t Total Dis Loss : 0.0006095393327996135\n",
      "Steps : 14600, \t Total Gen Loss : 25.193843841552734, \t Total Dis Loss : 0.07276292145252228\n",
      "Steps : 14700, \t Total Gen Loss : 24.650203704833984, \t Total Dis Loss : 0.0009153740247711539\n",
      "Steps : 14800, \t Total Gen Loss : 24.505720138549805, \t Total Dis Loss : 0.0026204646565020084\n",
      "Steps : 14900, \t Total Gen Loss : 24.834152221679688, \t Total Dis Loss : 0.004946855828166008\n",
      "Steps : 15000, \t Total Gen Loss : 26.729703903198242, \t Total Dis Loss : 0.0022811617236584425\n",
      "Steps : 15100, \t Total Gen Loss : 22.715354919433594, \t Total Dis Loss : 0.002537535270676017\n",
      "Steps : 15200, \t Total Gen Loss : 28.337324142456055, \t Total Dis Loss : 0.0016438489546999335\n",
      "Steps : 15300, \t Total Gen Loss : 25.597139358520508, \t Total Dis Loss : 0.0011433970648795366\n",
      "Steps : 15400, \t Total Gen Loss : 23.039506912231445, \t Total Dis Loss : 0.0005963903968222439\n",
      "Steps : 15500, \t Total Gen Loss : 24.487478256225586, \t Total Dis Loss : 0.000624984095338732\n",
      "Steps : 15600, \t Total Gen Loss : 20.711042404174805, \t Total Dis Loss : 0.029655948281288147\n",
      "Steps : 15700, \t Total Gen Loss : 24.040679931640625, \t Total Dis Loss : 0.014453444629907608\n",
      "Steps : 15800, \t Total Gen Loss : 27.454479217529297, \t Total Dis Loss : 0.003403065726161003\n",
      "Steps : 15900, \t Total Gen Loss : 22.59482192993164, \t Total Dis Loss : 0.002594889374449849\n",
      "Steps : 16000, \t Total Gen Loss : 26.67559814453125, \t Total Dis Loss : 0.0006196965696290135\n",
      "Steps : 16100, \t Total Gen Loss : 21.097637176513672, \t Total Dis Loss : 0.013674035668373108\n",
      "Steps : 16200, \t Total Gen Loss : 26.424480438232422, \t Total Dis Loss : 0.0023140734992921352\n",
      "Steps : 16300, \t Total Gen Loss : 25.83897590637207, \t Total Dis Loss : 0.006373592186719179\n",
      "Steps : 16400, \t Total Gen Loss : 21.826202392578125, \t Total Dis Loss : 0.005302881356328726\n",
      "Steps : 16500, \t Total Gen Loss : 25.381999969482422, \t Total Dis Loss : 0.0012036802945658565\n",
      "Steps : 16600, \t Total Gen Loss : 25.130126953125, \t Total Dis Loss : 0.004331471864134073\n",
      "Steps : 16700, \t Total Gen Loss : 21.653818130493164, \t Total Dis Loss : 0.0016189125599339604\n",
      "Steps : 16800, \t Total Gen Loss : 23.067707061767578, \t Total Dis Loss : 0.000680450233630836\n",
      "Time for epoch 3 is 277.6894087791443 sec\n",
      "Steps : 16900, \t Total Gen Loss : 24.55544662475586, \t Total Dis Loss : 0.00021498155547305942\n",
      "Steps : 17000, \t Total Gen Loss : 22.779911041259766, \t Total Dis Loss : 0.0021451343782246113\n",
      "Steps : 17100, \t Total Gen Loss : 26.60812759399414, \t Total Dis Loss : 0.001541165285743773\n",
      "Steps : 17200, \t Total Gen Loss : 22.74797821044922, \t Total Dis Loss : 0.000364878389518708\n",
      "Steps : 17300, \t Total Gen Loss : 19.756975173950195, \t Total Dis Loss : 0.0031351123470813036\n",
      "Steps : 17400, \t Total Gen Loss : 24.18996810913086, \t Total Dis Loss : 0.0012661985820159316\n",
      "Steps : 17500, \t Total Gen Loss : 24.178207397460938, \t Total Dis Loss : 0.0007067996775731444\n",
      "Steps : 17600, \t Total Gen Loss : 25.34351921081543, \t Total Dis Loss : 0.0023683649487793446\n",
      "Steps : 17700, \t Total Gen Loss : 24.167625427246094, \t Total Dis Loss : 0.0027607774827629328\n",
      "Steps : 17800, \t Total Gen Loss : 25.1976375579834, \t Total Dis Loss : 0.0016085560200735927\n",
      "Steps : 17900, \t Total Gen Loss : 25.22455596923828, \t Total Dis Loss : 0.0008491777698509395\n",
      "Steps : 18000, \t Total Gen Loss : 27.262596130371094, \t Total Dis Loss : 0.0013281387509778142\n",
      "Steps : 18100, \t Total Gen Loss : 27.14554214477539, \t Total Dis Loss : 0.0005411180318333209\n",
      "Steps : 18200, \t Total Gen Loss : 25.12593650817871, \t Total Dis Loss : 0.0008617342682555318\n",
      "Steps : 18300, \t Total Gen Loss : 27.726768493652344, \t Total Dis Loss : 0.00044380768667906523\n",
      "Steps : 18400, \t Total Gen Loss : 24.723304748535156, \t Total Dis Loss : 0.0015419446863234043\n",
      "Steps : 18500, \t Total Gen Loss : 24.609750747680664, \t Total Dis Loss : 0.006067697424441576\n",
      "Steps : 18600, \t Total Gen Loss : 22.175479888916016, \t Total Dis Loss : 0.0035745403729379177\n",
      "Steps : 18700, \t Total Gen Loss : 25.351490020751953, \t Total Dis Loss : 0.0011708951788023114\n",
      "Steps : 18800, \t Total Gen Loss : 25.689937591552734, \t Total Dis Loss : 0.0013466186355799437\n",
      "Steps : 18900, \t Total Gen Loss : 24.038156509399414, \t Total Dis Loss : 0.0003389807534404099\n",
      "Steps : 19000, \t Total Gen Loss : 25.95684051513672, \t Total Dis Loss : 0.0006890881340950727\n",
      "Steps : 19100, \t Total Gen Loss : 28.32029914855957, \t Total Dis Loss : 0.004082718398422003\n",
      "Steps : 19200, \t Total Gen Loss : 26.826160430908203, \t Total Dis Loss : 0.000292871700366959\n",
      "Steps : 19300, \t Total Gen Loss : 25.951295852661133, \t Total Dis Loss : 0.0006088131340220571\n",
      "Steps : 19400, \t Total Gen Loss : 29.02495574951172, \t Total Dis Loss : 0.0002808048448059708\n",
      "Steps : 19500, \t Total Gen Loss : 24.224302291870117, \t Total Dis Loss : 0.012116660363972187\n",
      "Steps : 19600, \t Total Gen Loss : 29.47705841064453, \t Total Dis Loss : 0.004434112925082445\n",
      "Steps : 19700, \t Total Gen Loss : 24.917463302612305, \t Total Dis Loss : 0.00200842646881938\n",
      "Steps : 19800, \t Total Gen Loss : 24.818788528442383, \t Total Dis Loss : 0.002938519697636366\n",
      "Steps : 19900, \t Total Gen Loss : 23.662607192993164, \t Total Dis Loss : 0.0004114310722798109\n",
      "Steps : 20000, \t Total Gen Loss : 26.974897384643555, \t Total Dis Loss : 0.006520780734717846\n",
      "Steps : 20100, \t Total Gen Loss : 26.396728515625, \t Total Dis Loss : 0.005977037362754345\n",
      "Steps : 20200, \t Total Gen Loss : 23.409826278686523, \t Total Dis Loss : 0.005143166985362768\n",
      "Steps : 20300, \t Total Gen Loss : 24.66217613220215, \t Total Dis Loss : 0.0018219018820673227\n",
      "Steps : 20400, \t Total Gen Loss : 24.618560791015625, \t Total Dis Loss : 0.0007911981083452702\n",
      "Steps : 20500, \t Total Gen Loss : 23.617155075073242, \t Total Dis Loss : 0.0017114344518631697\n",
      "Steps : 20600, \t Total Gen Loss : 25.664161682128906, \t Total Dis Loss : 0.0013118395581841469\n",
      "Steps : 20700, \t Total Gen Loss : 28.416227340698242, \t Total Dis Loss : 0.0009604437509551644\n",
      "Steps : 20800, \t Total Gen Loss : 25.252613067626953, \t Total Dis Loss : 0.00025966737302951515\n",
      "Steps : 20900, \t Total Gen Loss : 26.963172912597656, \t Total Dis Loss : 0.0003951365943066776\n",
      "Steps : 21000, \t Total Gen Loss : 27.002552032470703, \t Total Dis Loss : 0.0025862883776426315\n",
      "Steps : 21100, \t Total Gen Loss : 28.098257064819336, \t Total Dis Loss : 0.0007445409428328276\n",
      "Steps : 21200, \t Total Gen Loss : 24.677978515625, \t Total Dis Loss : 0.002389858476817608\n",
      "Steps : 21300, \t Total Gen Loss : 26.560375213623047, \t Total Dis Loss : 0.0015317789511755109\n",
      "Steps : 21400, \t Total Gen Loss : 25.68137550354004, \t Total Dis Loss : 0.0004620324762072414\n",
      "Steps : 21500, \t Total Gen Loss : 25.207271575927734, \t Total Dis Loss : 0.0003249343135394156\n",
      "Steps : 21600, \t Total Gen Loss : 27.536544799804688, \t Total Dis Loss : 0.0010850741527974606\n",
      "Steps : 21700, \t Total Gen Loss : 23.453519821166992, \t Total Dis Loss : 0.0012896147090941668\n",
      "Steps : 21800, \t Total Gen Loss : 27.117563247680664, \t Total Dis Loss : 0.05845256894826889\n",
      "Steps : 21900, \t Total Gen Loss : 24.520565032958984, \t Total Dis Loss : 0.000383920269086957\n",
      "Steps : 22000, \t Total Gen Loss : 23.876813888549805, \t Total Dis Loss : 0.0029366575181484222\n",
      "Steps : 22100, \t Total Gen Loss : 24.9190673828125, \t Total Dis Loss : 0.0006705732084810734\n",
      "Steps : 22200, \t Total Gen Loss : 27.64204978942871, \t Total Dis Loss : 0.00018388210446573794\n",
      "Steps : 22300, \t Total Gen Loss : 25.9395751953125, \t Total Dis Loss : 0.0005884621641598642\n",
      "Steps : 22400, \t Total Gen Loss : 24.491661071777344, \t Total Dis Loss : 0.00038963748374953866\n",
      "Steps : 22500, \t Total Gen Loss : 22.826984405517578, \t Total Dis Loss : 0.011602190323174\n",
      "Time for epoch 4 is 278.0804898738861 sec\n",
      "Steps : 22600, \t Total Gen Loss : 22.64996910095215, \t Total Dis Loss : 0.004023169167339802\n",
      "Steps : 22700, \t Total Gen Loss : 23.58264923095703, \t Total Dis Loss : 0.0010142384562641382\n",
      "Steps : 22800, \t Total Gen Loss : 25.123783111572266, \t Total Dis Loss : 0.0007704943418502808\n",
      "Steps : 22900, \t Total Gen Loss : 25.702289581298828, \t Total Dis Loss : 0.0012380750849843025\n",
      "Steps : 23000, \t Total Gen Loss : 25.252988815307617, \t Total Dis Loss : 0.0022686924785375595\n",
      "Steps : 23100, \t Total Gen Loss : 22.06644630432129, \t Total Dis Loss : 0.0018315782072022557\n",
      "Steps : 23200, \t Total Gen Loss : 24.371862411499023, \t Total Dis Loss : 0.0007044426165521145\n",
      "Steps : 23300, \t Total Gen Loss : 25.79178810119629, \t Total Dis Loss : 0.0018209436675533652\n",
      "Steps : 23400, \t Total Gen Loss : 23.617158889770508, \t Total Dis Loss : 0.0007791650714352727\n",
      "Steps : 23500, \t Total Gen Loss : 25.578563690185547, \t Total Dis Loss : 0.0013245742302387953\n",
      "Steps : 23600, \t Total Gen Loss : 28.66427230834961, \t Total Dis Loss : 0.0010010127443820238\n",
      "Steps : 23700, \t Total Gen Loss : 25.4005069732666, \t Total Dis Loss : 0.0002234713756479323\n",
      "Steps : 23800, \t Total Gen Loss : 25.686904907226562, \t Total Dis Loss : 0.005037042312324047\n",
      "Steps : 23900, \t Total Gen Loss : 26.587770462036133, \t Total Dis Loss : 0.002419872209429741\n",
      "Steps : 24000, \t Total Gen Loss : 28.12433624267578, \t Total Dis Loss : 0.0003715349012054503\n",
      "Steps : 24100, \t Total Gen Loss : 25.71927833557129, \t Total Dis Loss : 0.0003225406107958406\n",
      "Steps : 24200, \t Total Gen Loss : 28.727500915527344, \t Total Dis Loss : 0.003338073380291462\n",
      "Steps : 24300, \t Total Gen Loss : 27.392288208007812, \t Total Dis Loss : 0.0019469235558062792\n",
      "Steps : 24400, \t Total Gen Loss : 25.04863929748535, \t Total Dis Loss : 0.0021268194541335106\n",
      "Steps : 24500, \t Total Gen Loss : 26.484983444213867, \t Total Dis Loss : 0.00045132727245800197\n",
      "Steps : 24600, \t Total Gen Loss : 29.715173721313477, \t Total Dis Loss : 0.0007239772821776569\n",
      "Steps : 24700, \t Total Gen Loss : 26.18671226501465, \t Total Dis Loss : 0.004244555253535509\n",
      "Steps : 24800, \t Total Gen Loss : 26.557024002075195, \t Total Dis Loss : 0.0018682280788198113\n",
      "Steps : 24900, \t Total Gen Loss : 26.70196533203125, \t Total Dis Loss : 0.0004159950476605445\n",
      "Steps : 25000, \t Total Gen Loss : 26.804977416992188, \t Total Dis Loss : 0.0007752274977974594\n",
      "Steps : 25100, \t Total Gen Loss : 25.366798400878906, \t Total Dis Loss : 0.0004536316846497357\n",
      "Steps : 25200, \t Total Gen Loss : 27.364078521728516, \t Total Dis Loss : 0.0004246622556820512\n",
      "Steps : 25300, \t Total Gen Loss : 24.451622009277344, \t Total Dis Loss : 0.0007023095386102796\n",
      "Steps : 25400, \t Total Gen Loss : 25.94225311279297, \t Total Dis Loss : 0.00018708506831899285\n",
      "Steps : 25500, \t Total Gen Loss : 26.787281036376953, \t Total Dis Loss : 0.000684400147292763\n",
      "Steps : 25600, \t Total Gen Loss : 27.086074829101562, \t Total Dis Loss : 0.0008202108438126743\n",
      "Steps : 25700, \t Total Gen Loss : 28.39334487915039, \t Total Dis Loss : 0.000907778216060251\n",
      "Steps : 25800, \t Total Gen Loss : 26.54184341430664, \t Total Dis Loss : 0.0007622726261615753\n",
      "Steps : 25900, \t Total Gen Loss : 24.085695266723633, \t Total Dis Loss : 0.07855603098869324\n",
      "Steps : 26000, \t Total Gen Loss : 26.38421058654785, \t Total Dis Loss : 0.0012006285833194852\n",
      "Steps : 26100, \t Total Gen Loss : 29.20869255065918, \t Total Dis Loss : 0.030773429200053215\n",
      "Steps : 26200, \t Total Gen Loss : 31.977222442626953, \t Total Dis Loss : 0.0005258477758616209\n",
      "Steps : 26300, \t Total Gen Loss : 29.54945182800293, \t Total Dis Loss : 0.001182381995022297\n",
      "Steps : 26400, \t Total Gen Loss : 23.176916122436523, \t Total Dis Loss : 0.0020749615505337715\n",
      "Steps : 26500, \t Total Gen Loss : 27.146629333496094, \t Total Dis Loss : 0.010421575978398323\n",
      "Steps : 26600, \t Total Gen Loss : 24.517972946166992, \t Total Dis Loss : 0.0003740779065992683\n",
      "Steps : 26700, \t Total Gen Loss : 27.55642318725586, \t Total Dis Loss : 0.0005041738040745258\n",
      "Steps : 26800, \t Total Gen Loss : 24.679964065551758, \t Total Dis Loss : 0.0023636717814952135\n",
      "Steps : 26900, \t Total Gen Loss : 22.492958068847656, \t Total Dis Loss : 0.001349742989987135\n",
      "Steps : 27000, \t Total Gen Loss : 24.463764190673828, \t Total Dis Loss : 0.0005330322310328484\n",
      "Steps : 27100, \t Total Gen Loss : 21.35814094543457, \t Total Dis Loss : 0.0002756889443844557\n",
      "Steps : 27200, \t Total Gen Loss : 25.236221313476562, \t Total Dis Loss : 0.0005601970478892326\n",
      "Steps : 27300, \t Total Gen Loss : 21.525463104248047, \t Total Dis Loss : 0.01279356051236391\n",
      "Steps : 27400, \t Total Gen Loss : 24.2584171295166, \t Total Dis Loss : 0.0005979371489956975\n",
      "Steps : 27500, \t Total Gen Loss : 23.161495208740234, \t Total Dis Loss : 0.00033524486934766173\n",
      "Steps : 27600, \t Total Gen Loss : 23.963218688964844, \t Total Dis Loss : 0.001661557937040925\n",
      "Steps : 27700, \t Total Gen Loss : 28.057586669921875, \t Total Dis Loss : 0.0005610018852166831\n",
      "Steps : 27800, \t Total Gen Loss : 23.46731948852539, \t Total Dis Loss : 0.0015345304273068905\n",
      "Steps : 27900, \t Total Gen Loss : 23.277801513671875, \t Total Dis Loss : 0.0018962421454489231\n",
      "Steps : 28000, \t Total Gen Loss : 25.360958099365234, \t Total Dis Loss : 0.001284617348574102\n",
      "Steps : 28100, \t Total Gen Loss : 26.40424346923828, \t Total Dis Loss : 0.0007219259277917445\n",
      "Time for epoch 5 is 277.64253067970276 sec\n",
      "Steps : 28200, \t Total Gen Loss : 24.198766708374023, \t Total Dis Loss : 0.0009958125883713365\n",
      "Steps : 28300, \t Total Gen Loss : 25.881397247314453, \t Total Dis Loss : 0.0005544748273678124\n",
      "Steps : 28400, \t Total Gen Loss : 25.91292381286621, \t Total Dis Loss : 0.0008461902034468949\n",
      "Steps : 28500, \t Total Gen Loss : 22.92837905883789, \t Total Dis Loss : 0.0039326888509094715\n",
      "Steps : 28600, \t Total Gen Loss : 24.079872131347656, \t Total Dis Loss : 0.0005156950792297721\n",
      "Steps : 28700, \t Total Gen Loss : 23.896095275878906, \t Total Dis Loss : 0.00015403956058435142\n",
      "Steps : 28800, \t Total Gen Loss : 25.153926849365234, \t Total Dis Loss : 0.0026581515558063984\n",
      "Steps : 28900, \t Total Gen Loss : 24.33822250366211, \t Total Dis Loss : 0.0007647858583368361\n",
      "Steps : 29000, \t Total Gen Loss : 24.509130477905273, \t Total Dis Loss : 0.0006665292894467711\n",
      "Steps : 29100, \t Total Gen Loss : 25.83346176147461, \t Total Dis Loss : 0.00042401329847052693\n",
      "Steps : 29200, \t Total Gen Loss : 24.910064697265625, \t Total Dis Loss : 0.0005628441576845944\n",
      "Steps : 29300, \t Total Gen Loss : 27.94209861755371, \t Total Dis Loss : 0.00018379322136752307\n",
      "Steps : 29400, \t Total Gen Loss : 22.526872634887695, \t Total Dis Loss : 0.0008105445886030793\n",
      "Steps : 29500, \t Total Gen Loss : 24.78799057006836, \t Total Dis Loss : 0.0003239462384954095\n",
      "Steps : 29600, \t Total Gen Loss : 24.60272216796875, \t Total Dis Loss : 0.0003132606507278979\n",
      "Steps : 29700, \t Total Gen Loss : 22.657989501953125, \t Total Dis Loss : 0.0008477743249386549\n",
      "Steps : 29800, \t Total Gen Loss : 26.25141143798828, \t Total Dis Loss : 0.0004223444848321378\n",
      "Steps : 29900, \t Total Gen Loss : 23.929786682128906, \t Total Dis Loss : 0.0011559502454474568\n",
      "Steps : 30000, \t Total Gen Loss : 25.997678756713867, \t Total Dis Loss : 0.0005588043131865561\n",
      "Steps : 30100, \t Total Gen Loss : 24.49030113220215, \t Total Dis Loss : 0.0006809543701820076\n",
      "Steps : 30200, \t Total Gen Loss : 21.863048553466797, \t Total Dis Loss : 0.001405705465003848\n",
      "Steps : 30300, \t Total Gen Loss : 22.877777099609375, \t Total Dis Loss : 0.0010533357271924615\n",
      "Steps : 30400, \t Total Gen Loss : 23.08682632446289, \t Total Dis Loss : 0.002155340975150466\n",
      "Steps : 30500, \t Total Gen Loss : 25.30905532836914, \t Total Dis Loss : 0.00021048991766292602\n",
      "Steps : 30600, \t Total Gen Loss : 26.456953048706055, \t Total Dis Loss : 0.0008998402627184987\n",
      "Steps : 30700, \t Total Gen Loss : 24.95655059814453, \t Total Dis Loss : 0.000644528481643647\n",
      "Steps : 30800, \t Total Gen Loss : 24.71630859375, \t Total Dis Loss : 0.00038340804167091846\n",
      "Steps : 30900, \t Total Gen Loss : 24.724151611328125, \t Total Dis Loss : 0.00038893776945769787\n",
      "Steps : 31000, \t Total Gen Loss : 23.743797302246094, \t Total Dis Loss : 0.002948048058897257\n",
      "Steps : 31100, \t Total Gen Loss : 22.284849166870117, \t Total Dis Loss : 0.0003608383995015174\n",
      "Steps : 31200, \t Total Gen Loss : 25.95934295654297, \t Total Dis Loss : 0.00040827467455528677\n",
      "Steps : 31300, \t Total Gen Loss : 24.311155319213867, \t Total Dis Loss : 0.0003143381036352366\n",
      "Steps : 31400, \t Total Gen Loss : 24.090255737304688, \t Total Dis Loss : 0.0001600325631443411\n",
      "Steps : 31500, \t Total Gen Loss : 24.623353958129883, \t Total Dis Loss : 0.00018789619207382202\n",
      "Steps : 31600, \t Total Gen Loss : 25.639144897460938, \t Total Dis Loss : 0.00032656185794621706\n",
      "Steps : 31700, \t Total Gen Loss : 24.366291046142578, \t Total Dis Loss : 0.0016780649311840534\n",
      "Steps : 31800, \t Total Gen Loss : 27.633914947509766, \t Total Dis Loss : 0.00543286744505167\n",
      "Steps : 31900, \t Total Gen Loss : 27.47309684753418, \t Total Dis Loss : 0.0005909926258027554\n",
      "Steps : 32000, \t Total Gen Loss : 30.188758850097656, \t Total Dis Loss : 0.000786605232860893\n",
      "Steps : 32100, \t Total Gen Loss : 27.157167434692383, \t Total Dis Loss : 0.0005242089973762631\n",
      "Steps : 32200, \t Total Gen Loss : 28.016387939453125, \t Total Dis Loss : 0.0008508538594469428\n",
      "Steps : 32300, \t Total Gen Loss : 23.010448455810547, \t Total Dis Loss : 0.0011244784109294415\n",
      "Steps : 32400, \t Total Gen Loss : 32.01060104370117, \t Total Dis Loss : 0.00044744909973815084\n",
      "Steps : 32500, \t Total Gen Loss : 28.09453773498535, \t Total Dis Loss : 0.001287452527321875\n",
      "Steps : 32600, \t Total Gen Loss : 25.345455169677734, \t Total Dis Loss : 0.0012444054009392858\n",
      "Steps : 32700, \t Total Gen Loss : 22.01772689819336, \t Total Dis Loss : 0.059392739087343216\n",
      "Steps : 32800, \t Total Gen Loss : 25.102916717529297, \t Total Dis Loss : 0.001407736330293119\n",
      "Steps : 32900, \t Total Gen Loss : 24.357519149780273, \t Total Dis Loss : 0.0004688852932304144\n",
      "Steps : 33000, \t Total Gen Loss : 25.225250244140625, \t Total Dis Loss : 0.0006891252123750746\n",
      "Steps : 33100, \t Total Gen Loss : 24.02906608581543, \t Total Dis Loss : 0.0009818266844376922\n",
      "Steps : 33200, \t Total Gen Loss : 25.074352264404297, \t Total Dis Loss : 0.0005191119271330535\n",
      "Steps : 33300, \t Total Gen Loss : 23.982995986938477, \t Total Dis Loss : 0.0002685442450456321\n",
      "Steps : 33400, \t Total Gen Loss : 22.445852279663086, \t Total Dis Loss : 0.0007389012607745826\n",
      "Steps : 33500, \t Total Gen Loss : 22.316390991210938, \t Total Dis Loss : 0.0005137220723554492\n",
      "Steps : 33600, \t Total Gen Loss : 22.795167922973633, \t Total Dis Loss : 0.0008404708933085203\n",
      "Steps : 33700, \t Total Gen Loss : 22.802289962768555, \t Total Dis Loss : 0.0012964109191671014\n",
      "Time for epoch 6 is 277.15892028808594 sec\n",
      "Steps : 33800, \t Total Gen Loss : 24.524066925048828, \t Total Dis Loss : 0.0006787450984120369\n",
      "Steps : 33900, \t Total Gen Loss : 21.959978103637695, \t Total Dis Loss : 0.0004833102866541594\n",
      "Steps : 34000, \t Total Gen Loss : 24.194786071777344, \t Total Dis Loss : 0.0003851866349577904\n",
      "Steps : 34100, \t Total Gen Loss : 22.595294952392578, \t Total Dis Loss : 0.0005088730249553919\n",
      "Steps : 34200, \t Total Gen Loss : 25.22506332397461, \t Total Dis Loss : 0.0014648649375885725\n",
      "Steps : 34300, \t Total Gen Loss : 24.257183074951172, \t Total Dis Loss : 0.0004217967507429421\n",
      "Steps : 34400, \t Total Gen Loss : 23.117584228515625, \t Total Dis Loss : 0.000811116595286876\n",
      "Steps : 34500, \t Total Gen Loss : 24.57274055480957, \t Total Dis Loss : 0.0006865428294986486\n",
      "Steps : 34600, \t Total Gen Loss : 28.426557540893555, \t Total Dis Loss : 0.00029613508377224207\n",
      "Steps : 34700, \t Total Gen Loss : 24.604948043823242, \t Total Dis Loss : 0.00031028917874209583\n",
      "Steps : 34800, \t Total Gen Loss : 24.91853141784668, \t Total Dis Loss : 0.0006277318461798131\n",
      "Steps : 34900, \t Total Gen Loss : 28.11151695251465, \t Total Dis Loss : 0.0001960327208507806\n",
      "Steps : 35000, \t Total Gen Loss : 25.381059646606445, \t Total Dis Loss : 0.001477497979067266\n",
      "Steps : 35100, \t Total Gen Loss : 24.235397338867188, \t Total Dis Loss : 0.00011819947394542396\n",
      "Steps : 35200, \t Total Gen Loss : 26.518932342529297, \t Total Dis Loss : 0.004473403096199036\n",
      "Steps : 35300, \t Total Gen Loss : 27.931730270385742, \t Total Dis Loss : 0.0002036311780102551\n",
      "Steps : 35400, \t Total Gen Loss : 27.392473220825195, \t Total Dis Loss : 0.0020447075366973877\n",
      "Steps : 35500, \t Total Gen Loss : 27.04037857055664, \t Total Dis Loss : 0.0002581109874881804\n",
      "Steps : 35600, \t Total Gen Loss : 27.600839614868164, \t Total Dis Loss : 0.00019661057740449905\n",
      "Steps : 35700, \t Total Gen Loss : 26.4504451751709, \t Total Dis Loss : 0.0017549187177792192\n",
      "Steps : 35800, \t Total Gen Loss : 23.031272888183594, \t Total Dis Loss : 0.07731558382511139\n",
      "Steps : 35900, \t Total Gen Loss : 23.838462829589844, \t Total Dis Loss : 0.0009865069296211004\n",
      "Steps : 36000, \t Total Gen Loss : 25.907127380371094, \t Total Dis Loss : 0.00077642104588449\n",
      "Steps : 36100, \t Total Gen Loss : 25.36756134033203, \t Total Dis Loss : 0.0005057527450844646\n",
      "Steps : 36200, \t Total Gen Loss : 27.08365821838379, \t Total Dis Loss : 0.0008047361043281853\n",
      "Steps : 36300, \t Total Gen Loss : 29.924087524414062, \t Total Dis Loss : 0.00029332799022085965\n",
      "Steps : 36400, \t Total Gen Loss : 29.30143928527832, \t Total Dis Loss : 0.0003079833695665002\n",
      "Steps : 36500, \t Total Gen Loss : 28.54689598083496, \t Total Dis Loss : 0.0007258283440023661\n",
      "Steps : 36600, \t Total Gen Loss : 29.705060958862305, \t Total Dis Loss : 0.0007219772669486701\n",
      "Steps : 36700, \t Total Gen Loss : 26.63245964050293, \t Total Dis Loss : 0.0005435840575955808\n",
      "Steps : 36800, \t Total Gen Loss : 27.10146713256836, \t Total Dis Loss : 0.00042364036198705435\n",
      "Steps : 36900, \t Total Gen Loss : 28.374940872192383, \t Total Dis Loss : 0.0004517151974141598\n",
      "Steps : 37000, \t Total Gen Loss : 27.499971389770508, \t Total Dis Loss : 0.00030574382981285453\n",
      "Steps : 37100, \t Total Gen Loss : 30.129650115966797, \t Total Dis Loss : 0.0006189103587530553\n",
      "Steps : 37200, \t Total Gen Loss : 26.371278762817383, \t Total Dis Loss : 0.0005173305398784578\n",
      "Steps : 37300, \t Total Gen Loss : 25.82616424560547, \t Total Dis Loss : 0.0001747875357978046\n",
      "Steps : 37400, \t Total Gen Loss : 25.497074127197266, \t Total Dis Loss : 0.003521244041621685\n",
      "Steps : 37500, \t Total Gen Loss : 29.357099533081055, \t Total Dis Loss : 0.010785218328237534\n",
      "Steps : 37600, \t Total Gen Loss : 28.130939483642578, \t Total Dis Loss : 0.0010029100812971592\n",
      "Steps : 37700, \t Total Gen Loss : 26.352094650268555, \t Total Dis Loss : 0.00016292273357976228\n",
      "Steps : 37800, \t Total Gen Loss : 26.932947158813477, \t Total Dis Loss : 0.003823918057605624\n",
      "Steps : 37900, \t Total Gen Loss : 26.65677261352539, \t Total Dis Loss : 0.00710695143789053\n",
      "Steps : 38000, \t Total Gen Loss : 26.898393630981445, \t Total Dis Loss : 0.0022458480671048164\n",
      "Steps : 38100, \t Total Gen Loss : 28.138225555419922, \t Total Dis Loss : 0.020111238583922386\n",
      "Steps : 38200, \t Total Gen Loss : 26.90797996520996, \t Total Dis Loss : 0.012925097718834877\n",
      "Steps : 38300, \t Total Gen Loss : 29.633623123168945, \t Total Dis Loss : 0.0003573698049876839\n",
      "Steps : 38400, \t Total Gen Loss : 30.542858123779297, \t Total Dis Loss : 0.00036032628850080073\n",
      "Steps : 38500, \t Total Gen Loss : 30.339731216430664, \t Total Dis Loss : 0.0016803753096610308\n",
      "Steps : 38600, \t Total Gen Loss : 27.676271438598633, \t Total Dis Loss : 0.00043939039460383356\n",
      "Steps : 38700, \t Total Gen Loss : 26.04052734375, \t Total Dis Loss : 0.0029167919419705868\n",
      "Steps : 38800, \t Total Gen Loss : 25.368135452270508, \t Total Dis Loss : 0.0025020097382366657\n",
      "Steps : 38900, \t Total Gen Loss : 28.418861389160156, \t Total Dis Loss : 0.0006023194873705506\n",
      "Steps : 39000, \t Total Gen Loss : 30.6301326751709, \t Total Dis Loss : 0.0030744243413209915\n",
      "Steps : 39100, \t Total Gen Loss : 28.289226531982422, \t Total Dis Loss : 0.0008038890082389116\n",
      "Steps : 39200, \t Total Gen Loss : 24.900466918945312, \t Total Dis Loss : 0.002274520229548216\n",
      "Steps : 39300, \t Total Gen Loss : 30.36374282836914, \t Total Dis Loss : 0.0023387002293020487\n",
      "Time for epoch 7 is 278.7707943916321 sec\n",
      "Steps : 39400, \t Total Gen Loss : 28.72115707397461, \t Total Dis Loss : 0.0009152812417596579\n",
      "Steps : 39500, \t Total Gen Loss : 27.404415130615234, \t Total Dis Loss : 0.00015711394371464849\n",
      "Steps : 39600, \t Total Gen Loss : 29.18280792236328, \t Total Dis Loss : 0.0008804794633761048\n",
      "Steps : 39700, \t Total Gen Loss : 26.299299240112305, \t Total Dis Loss : 0.0019103112863376737\n",
      "Steps : 39800, \t Total Gen Loss : 27.846940994262695, \t Total Dis Loss : 0.0013854834251105785\n",
      "Steps : 39900, \t Total Gen Loss : 29.334369659423828, \t Total Dis Loss : 0.0007264594314619899\n",
      "Steps : 40000, \t Total Gen Loss : 23.767061233520508, \t Total Dis Loss : 0.00021385319996625185\n",
      "Steps : 40100, \t Total Gen Loss : 25.44870948791504, \t Total Dis Loss : 0.0007277517579495907\n",
      "Steps : 40200, \t Total Gen Loss : 29.670652389526367, \t Total Dis Loss : 0.00021571411343757063\n",
      "Steps : 40300, \t Total Gen Loss : 26.62569236755371, \t Total Dis Loss : 0.004059031140059233\n",
      "Steps : 40400, \t Total Gen Loss : 26.915958404541016, \t Total Dis Loss : 0.012699990533292294\n",
      "Steps : 40500, \t Total Gen Loss : 26.938283920288086, \t Total Dis Loss : 0.0010136570781469345\n",
      "Steps : 40600, \t Total Gen Loss : 27.41505241394043, \t Total Dis Loss : 0.00033030641498044133\n",
      "Steps : 40700, \t Total Gen Loss : 23.712623596191406, \t Total Dis Loss : 0.008313695900142193\n",
      "Steps : 40800, \t Total Gen Loss : 27.191444396972656, \t Total Dis Loss : 0.003031963249668479\n",
      "Steps : 40900, \t Total Gen Loss : 23.292917251586914, \t Total Dis Loss : 0.0012717334320768714\n",
      "Steps : 41000, \t Total Gen Loss : 26.449411392211914, \t Total Dis Loss : 0.0009472536621615291\n",
      "Steps : 41100, \t Total Gen Loss : 24.592260360717773, \t Total Dis Loss : 0.0013251580530777574\n",
      "Steps : 41200, \t Total Gen Loss : 23.985952377319336, \t Total Dis Loss : 0.002469531260430813\n",
      "Steps : 41300, \t Total Gen Loss : 26.868083953857422, \t Total Dis Loss : 0.3095138370990753\n",
      "Steps : 41400, \t Total Gen Loss : 23.51067352294922, \t Total Dis Loss : 0.004014973528683186\n",
      "Steps : 41500, \t Total Gen Loss : 25.097143173217773, \t Total Dis Loss : 0.0002061745326500386\n",
      "Steps : 41600, \t Total Gen Loss : 25.401262283325195, \t Total Dis Loss : 0.0010636760853230953\n",
      "Steps : 41700, \t Total Gen Loss : 26.408679962158203, \t Total Dis Loss : 0.000644469924736768\n",
      "Steps : 41800, \t Total Gen Loss : 26.66143798828125, \t Total Dis Loss : 0.032325077801942825\n",
      "Steps : 41900, \t Total Gen Loss : 26.35188102722168, \t Total Dis Loss : 0.0006193473236635327\n",
      "Steps : 42000, \t Total Gen Loss : 31.93169593811035, \t Total Dis Loss : 0.0012868908233940601\n",
      "Steps : 42100, \t Total Gen Loss : 25.322153091430664, \t Total Dis Loss : 0.0031531837303191423\n",
      "Steps : 42200, \t Total Gen Loss : 28.666872024536133, \t Total Dis Loss : 0.0006289639277383685\n",
      "Steps : 42300, \t Total Gen Loss : 30.708147048950195, \t Total Dis Loss : 0.0003059657756239176\n",
      "Steps : 42400, \t Total Gen Loss : 30.589996337890625, \t Total Dis Loss : 0.0022446264047175646\n",
      "Steps : 42500, \t Total Gen Loss : 23.6369686126709, \t Total Dis Loss : 0.010633593425154686\n",
      "Steps : 42600, \t Total Gen Loss : 28.224605560302734, \t Total Dis Loss : 0.0007984930416569114\n",
      "Steps : 42700, \t Total Gen Loss : 27.508817672729492, \t Total Dis Loss : 0.001970971003174782\n",
      "Steps : 42800, \t Total Gen Loss : 31.799610137939453, \t Total Dis Loss : 0.013105806894600391\n",
      "Steps : 42900, \t Total Gen Loss : 26.815982818603516, \t Total Dis Loss : 0.5604938268661499\n",
      "Steps : 43000, \t Total Gen Loss : 26.26629638671875, \t Total Dis Loss : 0.0024804624263197184\n",
      "Steps : 43100, \t Total Gen Loss : 30.522741317749023, \t Total Dis Loss : 0.0023441901430487633\n",
      "Steps : 43200, \t Total Gen Loss : 30.876422882080078, \t Total Dis Loss : 0.0006339300889521837\n",
      "Steps : 43300, \t Total Gen Loss : 26.982677459716797, \t Total Dis Loss : 0.0012821998680010438\n",
      "Steps : 43400, \t Total Gen Loss : 27.220993041992188, \t Total Dis Loss : 0.0032573912758380175\n",
      "Steps : 43500, \t Total Gen Loss : 25.237613677978516, \t Total Dis Loss : 0.004007928539067507\n",
      "Steps : 43600, \t Total Gen Loss : 25.092514038085938, \t Total Dis Loss : 0.002329230308532715\n",
      "Steps : 43700, \t Total Gen Loss : 24.919414520263672, \t Total Dis Loss : 0.0022042698692530394\n",
      "Steps : 43800, \t Total Gen Loss : 27.207172393798828, \t Total Dis Loss : 0.0018672812730073929\n",
      "Steps : 43900, \t Total Gen Loss : 27.494606018066406, \t Total Dis Loss : 0.001056930050253868\n",
      "Steps : 44000, \t Total Gen Loss : 27.43002700805664, \t Total Dis Loss : 0.000740068731829524\n",
      "Steps : 44100, \t Total Gen Loss : 26.490203857421875, \t Total Dis Loss : 0.0031127817928791046\n",
      "Steps : 44200, \t Total Gen Loss : 26.330900192260742, \t Total Dis Loss : 0.011309606954455376\n",
      "Steps : 44300, \t Total Gen Loss : 28.165096282958984, \t Total Dis Loss : 0.0011866041459143162\n",
      "Steps : 44400, \t Total Gen Loss : 25.09140968322754, \t Total Dis Loss : 0.00894673727452755\n",
      "Steps : 44500, \t Total Gen Loss : 25.9893856048584, \t Total Dis Loss : 0.0008644749177619815\n",
      "Steps : 44600, \t Total Gen Loss : 28.3670597076416, \t Total Dis Loss : 0.0018920436268672347\n",
      "Steps : 44700, \t Total Gen Loss : 25.691333770751953, \t Total Dis Loss : 0.001065857708454132\n",
      "Steps : 44800, \t Total Gen Loss : 30.628915786743164, \t Total Dis Loss : 0.00026512460317462683\n",
      "Steps : 44900, \t Total Gen Loss : 24.89520263671875, \t Total Dis Loss : 0.02703700214624405\n",
      "Steps : 45000, \t Total Gen Loss : 26.322786331176758, \t Total Dis Loss : 0.002315836725756526\n",
      "Time for epoch 8 is 277.50569772720337 sec\n",
      "Steps : 45100, \t Total Gen Loss : 28.141902923583984, \t Total Dis Loss : 0.001945385942235589\n",
      "Steps : 45200, \t Total Gen Loss : 27.068405151367188, \t Total Dis Loss : 1.6502742767333984\n",
      "Steps : 45300, \t Total Gen Loss : 23.562223434448242, \t Total Dis Loss : 0.0035612271167337894\n",
      "Steps : 45400, \t Total Gen Loss : 25.24148941040039, \t Total Dis Loss : 0.0030804488342255354\n",
      "Steps : 45500, \t Total Gen Loss : 31.412179946899414, \t Total Dis Loss : 0.008247841149568558\n",
      "Steps : 45600, \t Total Gen Loss : 28.801925659179688, \t Total Dis Loss : 0.0011536756064742804\n",
      "Steps : 45700, \t Total Gen Loss : 27.99472427368164, \t Total Dis Loss : 0.0005208500078879297\n",
      "Steps : 45800, \t Total Gen Loss : 22.721065521240234, \t Total Dis Loss : 0.0018493124516680837\n",
      "Steps : 45900, \t Total Gen Loss : 26.04197883605957, \t Total Dis Loss : 0.0022771526128053665\n",
      "Steps : 46000, \t Total Gen Loss : 26.721818923950195, \t Total Dis Loss : 0.0009649732382968068\n",
      "Steps : 46100, \t Total Gen Loss : 26.356962203979492, \t Total Dis Loss : 0.0015538663137704134\n",
      "Steps : 46200, \t Total Gen Loss : 25.790634155273438, \t Total Dis Loss : 0.0013057475443929434\n",
      "Steps : 46300, \t Total Gen Loss : 23.50992774963379, \t Total Dis Loss : 0.0009071021340787411\n",
      "Steps : 46400, \t Total Gen Loss : 25.606414794921875, \t Total Dis Loss : 0.0017981616547331214\n",
      "Steps : 46500, \t Total Gen Loss : 33.92178726196289, \t Total Dis Loss : 0.00032586255110800266\n",
      "Steps : 46600, \t Total Gen Loss : 27.05135154724121, \t Total Dis Loss : 0.0013562556123360991\n",
      "Steps : 46700, \t Total Gen Loss : 26.773059844970703, \t Total Dis Loss : 0.02322014793753624\n",
      "Steps : 46800, \t Total Gen Loss : 26.19637107849121, \t Total Dis Loss : 0.002773724729195237\n",
      "Steps : 46900, \t Total Gen Loss : 24.5251522064209, \t Total Dis Loss : 0.004397857002913952\n",
      "Steps : 47000, \t Total Gen Loss : 30.09770965576172, \t Total Dis Loss : 0.0005234276177361608\n",
      "Steps : 47100, \t Total Gen Loss : 30.05990219116211, \t Total Dis Loss : 0.0023757112212479115\n",
      "Steps : 47200, \t Total Gen Loss : 28.285045623779297, \t Total Dis Loss : 0.0019812611863017082\n",
      "Steps : 47300, \t Total Gen Loss : 26.735946655273438, \t Total Dis Loss : 0.007986797019839287\n",
      "Steps : 47400, \t Total Gen Loss : 26.01761245727539, \t Total Dis Loss : 0.01052666176110506\n",
      "Steps : 47500, \t Total Gen Loss : 24.782154083251953, \t Total Dis Loss : 0.001540409866720438\n",
      "Steps : 47600, \t Total Gen Loss : 28.556804656982422, \t Total Dis Loss : 0.004267376847565174\n",
      "Steps : 47700, \t Total Gen Loss : 26.94182777404785, \t Total Dis Loss : 0.0031129918061196804\n",
      "Steps : 47800, \t Total Gen Loss : 28.49449348449707, \t Total Dis Loss : 0.0019499219488352537\n",
      "Steps : 47900, \t Total Gen Loss : 27.373998641967773, \t Total Dis Loss : 0.004517963156104088\n",
      "Steps : 48000, \t Total Gen Loss : 26.77633285522461, \t Total Dis Loss : 0.00102053745649755\n",
      "Steps : 48100, \t Total Gen Loss : 24.172300338745117, \t Total Dis Loss : 0.0008120338316075504\n",
      "Steps : 48200, \t Total Gen Loss : 25.078969955444336, \t Total Dis Loss : 0.0007213248172774911\n",
      "Steps : 48300, \t Total Gen Loss : 32.48951721191406, \t Total Dis Loss : 0.0014598307898268104\n",
      "Steps : 48400, \t Total Gen Loss : 24.86772918701172, \t Total Dis Loss : 0.0005307610263116658\n",
      "Steps : 48500, \t Total Gen Loss : 24.639842987060547, \t Total Dis Loss : 0.03543649986386299\n",
      "Steps : 48600, \t Total Gen Loss : 25.75116729736328, \t Total Dis Loss : 0.0022126948460936546\n",
      "Steps : 48700, \t Total Gen Loss : 25.024574279785156, \t Total Dis Loss : 0.0012718781363219023\n",
      "Steps : 48800, \t Total Gen Loss : 24.647798538208008, \t Total Dis Loss : 0.000486810109578073\n",
      "Steps : 48900, \t Total Gen Loss : 28.988693237304688, \t Total Dis Loss : 0.001250763889402151\n",
      "Steps : 49000, \t Total Gen Loss : 26.115589141845703, \t Total Dis Loss : 0.000835087732411921\n",
      "Steps : 49100, \t Total Gen Loss : 28.196044921875, \t Total Dis Loss : 0.001222653198055923\n",
      "Steps : 49200, \t Total Gen Loss : 27.374679565429688, \t Total Dis Loss : 0.024814600124955177\n",
      "Steps : 49300, \t Total Gen Loss : 24.799205780029297, \t Total Dis Loss : 0.0017791676800698042\n",
      "Steps : 49400, \t Total Gen Loss : 25.799795150756836, \t Total Dis Loss : 0.0010231230407953262\n",
      "Steps : 49500, \t Total Gen Loss : 24.53427505493164, \t Total Dis Loss : 0.0007232074276544154\n",
      "Steps : 49600, \t Total Gen Loss : 27.786502838134766, \t Total Dis Loss : 0.0020128581672906876\n",
      "Steps : 49700, \t Total Gen Loss : 25.014326095581055, \t Total Dis Loss : 0.004093722440302372\n",
      "Steps : 49800, \t Total Gen Loss : 28.501502990722656, \t Total Dis Loss : 0.0017322490457445383\n",
      "Steps : 49900, \t Total Gen Loss : 28.55246353149414, \t Total Dis Loss : 0.0022696699015796185\n",
      "Steps : 50000, \t Total Gen Loss : 31.819425582885742, \t Total Dis Loss : 0.007936128415167332\n",
      "Steps : 50100, \t Total Gen Loss : 29.445314407348633, \t Total Dis Loss : 0.09395607560873032\n",
      "Steps : 50200, \t Total Gen Loss : 25.801071166992188, \t Total Dis Loss : 0.002001018263399601\n",
      "Steps : 50300, \t Total Gen Loss : 32.45303726196289, \t Total Dis Loss : 0.0023596349637955427\n",
      "Steps : 50400, \t Total Gen Loss : 28.85809898376465, \t Total Dis Loss : 0.01233060471713543\n",
      "Steps : 50500, \t Total Gen Loss : 30.605953216552734, \t Total Dis Loss : 0.0019076119642704725\n",
      "Steps : 50600, \t Total Gen Loss : 28.749290466308594, \t Total Dis Loss : 0.0008168431231752038\n",
      "Time for epoch 9 is 278.4926881790161 sec\n",
      "Steps : 50700, \t Total Gen Loss : 27.528339385986328, \t Total Dis Loss : 0.001993607496842742\n",
      "Steps : 50800, \t Total Gen Loss : 22.800748825073242, \t Total Dis Loss : 0.001324926852248609\n",
      "Steps : 50900, \t Total Gen Loss : 25.55215072631836, \t Total Dis Loss : 0.0014640669105574489\n",
      "Steps : 51000, \t Total Gen Loss : 24.218639373779297, \t Total Dis Loss : 0.0019271981436759233\n",
      "Steps : 51100, \t Total Gen Loss : 28.096023559570312, \t Total Dis Loss : 0.0019709374755620956\n",
      "Steps : 51200, \t Total Gen Loss : 26.755828857421875, \t Total Dis Loss : 0.0008645956986583769\n",
      "Steps : 51300, \t Total Gen Loss : 26.82171630859375, \t Total Dis Loss : 0.0060884724371135235\n",
      "Steps : 51400, \t Total Gen Loss : 26.802648544311523, \t Total Dis Loss : 0.00306962663307786\n",
      "Steps : 51500, \t Total Gen Loss : 30.18650245666504, \t Total Dis Loss : 0.00396754452958703\n",
      "Steps : 51600, \t Total Gen Loss : 27.817222595214844, \t Total Dis Loss : 0.0015659270575270057\n",
      "Steps : 51700, \t Total Gen Loss : 29.06771469116211, \t Total Dis Loss : 0.0007636971422471106\n",
      "Steps : 51800, \t Total Gen Loss : 26.93282699584961, \t Total Dis Loss : 0.0045318882912397385\n",
      "Steps : 51900, \t Total Gen Loss : 23.395605087280273, \t Total Dis Loss : 0.0032866739202290773\n",
      "Steps : 52000, \t Total Gen Loss : 27.149572372436523, \t Total Dis Loss : 0.0017775387968868017\n",
      "Steps : 52100, \t Total Gen Loss : 25.963224411010742, \t Total Dis Loss : 0.014638356864452362\n",
      "Steps : 52200, \t Total Gen Loss : 27.79300880432129, \t Total Dis Loss : 0.004635384771972895\n",
      "Steps : 52300, \t Total Gen Loss : 31.854673385620117, \t Total Dis Loss : 0.006442509591579437\n",
      "Steps : 52400, \t Total Gen Loss : 28.584972381591797, \t Total Dis Loss : 0.001053682412020862\n",
      "Steps : 52500, \t Total Gen Loss : 25.896038055419922, \t Total Dis Loss : 0.0013749965000897646\n",
      "Steps : 52600, \t Total Gen Loss : 29.628585815429688, \t Total Dis Loss : 0.0007809968665242195\n",
      "Steps : 52700, \t Total Gen Loss : 30.499670028686523, \t Total Dis Loss : 0.0018976369174197316\n",
      "Steps : 52800, \t Total Gen Loss : 28.994712829589844, \t Total Dis Loss : 0.007447556126862764\n",
      "Steps : 52900, \t Total Gen Loss : 28.23095703125, \t Total Dis Loss : 0.005913381464779377\n",
      "Steps : 53000, \t Total Gen Loss : 28.735370635986328, \t Total Dis Loss : 0.0008898710366338491\n",
      "Steps : 53100, \t Total Gen Loss : 26.801450729370117, \t Total Dis Loss : 0.0003660299116745591\n",
      "Steps : 53200, \t Total Gen Loss : 28.108015060424805, \t Total Dis Loss : 0.001712986035272479\n",
      "Steps : 53300, \t Total Gen Loss : 26.9697265625, \t Total Dis Loss : 0.0044184192083776\n",
      "Steps : 53400, \t Total Gen Loss : 26.474079132080078, \t Total Dis Loss : 0.0020374958403408527\n",
      "Steps : 53500, \t Total Gen Loss : 23.431171417236328, \t Total Dis Loss : 0.0011295368894934654\n",
      "Steps : 53600, \t Total Gen Loss : 26.751842498779297, \t Total Dis Loss : 0.002010768512263894\n",
      "Steps : 53700, \t Total Gen Loss : 29.20113754272461, \t Total Dis Loss : 0.003292238572612405\n",
      "Steps : 53800, \t Total Gen Loss : 27.20720863342285, \t Total Dis Loss : 0.0012422287836670876\n",
      "Steps : 53900, \t Total Gen Loss : 25.02168083190918, \t Total Dis Loss : 0.40216976404190063\n",
      "Steps : 54000, \t Total Gen Loss : 28.075231552124023, \t Total Dis Loss : 0.0030357937794178724\n",
      "Steps : 54100, \t Total Gen Loss : 30.0507755279541, \t Total Dis Loss : 0.0010594751220196486\n",
      "Steps : 54200, \t Total Gen Loss : 29.708940505981445, \t Total Dis Loss : 0.00033925703610293567\n",
      "Steps : 54300, \t Total Gen Loss : 32.8955078125, \t Total Dis Loss : 0.0006703608087264001\n",
      "Steps : 54400, \t Total Gen Loss : 31.196849822998047, \t Total Dis Loss : 0.00450142053887248\n",
      "Steps : 54500, \t Total Gen Loss : 26.19074249267578, \t Total Dis Loss : 0.000746225006878376\n",
      "Steps : 54600, \t Total Gen Loss : 30.694360733032227, \t Total Dis Loss : 0.005053914152085781\n",
      "Steps : 54700, \t Total Gen Loss : 28.10224723815918, \t Total Dis Loss : 0.00041576247895136476\n",
      "Steps : 54800, \t Total Gen Loss : 30.44194221496582, \t Total Dis Loss : 0.0006932519609108567\n",
      "Steps : 54900, \t Total Gen Loss : 26.216686248779297, \t Total Dis Loss : 0.0016569688450545073\n",
      "Steps : 55000, \t Total Gen Loss : 27.902156829833984, \t Total Dis Loss : 0.002059606835246086\n",
      "Steps : 55100, \t Total Gen Loss : 27.191144943237305, \t Total Dis Loss : 0.00106359226629138\n",
      "Steps : 55200, \t Total Gen Loss : 30.301258087158203, \t Total Dis Loss : 0.001156831393018365\n",
      "Steps : 55300, \t Total Gen Loss : 22.825958251953125, \t Total Dis Loss : 0.001993708312511444\n",
      "Steps : 55400, \t Total Gen Loss : 33.2002067565918, \t Total Dis Loss : 0.0023891038727015257\n",
      "Steps : 55500, \t Total Gen Loss : 27.84549331665039, \t Total Dis Loss : 0.00114555680193007\n",
      "Steps : 55600, \t Total Gen Loss : 26.65587615966797, \t Total Dis Loss : 0.0005788831040263176\n",
      "Steps : 55700, \t Total Gen Loss : 28.85833740234375, \t Total Dis Loss : 0.0006642398075200617\n",
      "Steps : 55800, \t Total Gen Loss : 28.61562728881836, \t Total Dis Loss : 0.000711705069988966\n",
      "Steps : 55900, \t Total Gen Loss : 26.299501419067383, \t Total Dis Loss : 0.0005291919223964214\n",
      "Steps : 56000, \t Total Gen Loss : 27.334447860717773, \t Total Dis Loss : 0.0011709264945238829\n",
      "Steps : 56100, \t Total Gen Loss : 27.94890594482422, \t Total Dis Loss : 0.00028184865368530154\n",
      "Steps : 56200, \t Total Gen Loss : 30.012779235839844, \t Total Dis Loss : 0.002356874058023095\n",
      "Time for epoch 10 is 277.982216835022 sec\n",
      "Steps : 56300, \t Total Gen Loss : 30.905729293823242, \t Total Dis Loss : 0.00048188320943154395\n",
      "Steps : 56400, \t Total Gen Loss : 28.77466583251953, \t Total Dis Loss : 0.0006090311799198389\n",
      "Steps : 56500, \t Total Gen Loss : 30.04320526123047, \t Total Dis Loss : 0.0006947355577722192\n",
      "Steps : 56600, \t Total Gen Loss : 27.839872360229492, \t Total Dis Loss : 0.0030898870900273323\n",
      "Steps : 56700, \t Total Gen Loss : 28.432641983032227, \t Total Dis Loss : 0.0010082239750772715\n",
      "Steps : 56800, \t Total Gen Loss : 26.413970947265625, \t Total Dis Loss : 0.0007972816820256412\n",
      "Steps : 56900, \t Total Gen Loss : 28.818201065063477, \t Total Dis Loss : 0.0004951892187818885\n",
      "Steps : 57000, \t Total Gen Loss : 22.769115447998047, \t Total Dis Loss : 0.0010425519431009889\n",
      "Steps : 57100, \t Total Gen Loss : 30.247413635253906, \t Total Dis Loss : 0.0009905449114739895\n",
      "Steps : 57200, \t Total Gen Loss : 29.19105339050293, \t Total Dis Loss : 0.00041770728421397507\n",
      "Steps : 57300, \t Total Gen Loss : 30.117795944213867, \t Total Dis Loss : 0.007021352648735046\n",
      "Steps : 57400, \t Total Gen Loss : 29.33388328552246, \t Total Dis Loss : 0.0005190281663089991\n",
      "Steps : 57500, \t Total Gen Loss : 29.853479385375977, \t Total Dis Loss : 0.0012488705106079578\n",
      "Steps : 57600, \t Total Gen Loss : 31.00652503967285, \t Total Dis Loss : 0.00036565912887454033\n",
      "Steps : 57700, \t Total Gen Loss : 33.19732666015625, \t Total Dis Loss : 0.0002655381103977561\n",
      "Steps : 57800, \t Total Gen Loss : 27.82183837890625, \t Total Dis Loss : 0.001247884938493371\n",
      "Steps : 57900, \t Total Gen Loss : 27.666793823242188, \t Total Dis Loss : 0.0004715836839750409\n",
      "Steps : 58000, \t Total Gen Loss : 27.4483642578125, \t Total Dis Loss : 0.0006556055159308016\n",
      "Steps : 58100, \t Total Gen Loss : 31.11498260498047, \t Total Dis Loss : 0.0003210142021998763\n",
      "Steps : 58200, \t Total Gen Loss : 29.754682540893555, \t Total Dis Loss : 0.0007022556965239346\n",
      "Steps : 58300, \t Total Gen Loss : 30.291776657104492, \t Total Dis Loss : 0.0004972981405444443\n",
      "Steps : 58400, \t Total Gen Loss : 27.557912826538086, \t Total Dis Loss : 0.002387086395174265\n",
      "Steps : 58500, \t Total Gen Loss : 31.70094871520996, \t Total Dis Loss : 0.0005815245676785707\n",
      "Steps : 58600, \t Total Gen Loss : 26.93768310546875, \t Total Dis Loss : 0.0006058510625734925\n",
      "Steps : 58700, \t Total Gen Loss : 26.73563003540039, \t Total Dis Loss : 0.0017926340224221349\n",
      "Steps : 58800, \t Total Gen Loss : 26.989181518554688, \t Total Dis Loss : 0.0010151619790121913\n",
      "Steps : 58900, \t Total Gen Loss : 27.881450653076172, \t Total Dis Loss : 0.0008567149052396417\n",
      "Steps : 59000, \t Total Gen Loss : 32.3505744934082, \t Total Dis Loss : 0.001123066176660359\n",
      "Steps : 59100, \t Total Gen Loss : 32.815155029296875, \t Total Dis Loss : 0.0009436694672331214\n",
      "Steps : 59200, \t Total Gen Loss : 30.003755569458008, \t Total Dis Loss : 0.0005882937693968415\n",
      "Steps : 59300, \t Total Gen Loss : 27.00222396850586, \t Total Dis Loss : 0.00048397856880910695\n",
      "Steps : 59400, \t Total Gen Loss : 28.56838607788086, \t Total Dis Loss : 0.000940325204282999\n",
      "Steps : 59500, \t Total Gen Loss : 29.9626407623291, \t Total Dis Loss : 0.0003074440755881369\n",
      "Steps : 59600, \t Total Gen Loss : 29.219810485839844, \t Total Dis Loss : 0.0006485242629423738\n",
      "Steps : 59700, \t Total Gen Loss : 27.46756935119629, \t Total Dis Loss : 0.00016338977729901671\n",
      "Steps : 59800, \t Total Gen Loss : 31.83295440673828, \t Total Dis Loss : 0.00023919623345136642\n",
      "Steps : 59900, \t Total Gen Loss : 29.57510757446289, \t Total Dis Loss : 0.0004538075882010162\n",
      "Steps : 60000, \t Total Gen Loss : 27.42971420288086, \t Total Dis Loss : 0.0005360582144930959\n",
      "Steps : 60100, \t Total Gen Loss : 28.807931900024414, \t Total Dis Loss : 0.0005344902747310698\n",
      "Steps : 60200, \t Total Gen Loss : 28.311668395996094, \t Total Dis Loss : 0.00032361570629291236\n",
      "Steps : 60300, \t Total Gen Loss : 28.740314483642578, \t Total Dis Loss : 0.0039697736501693726\n",
      "Steps : 60400, \t Total Gen Loss : 30.351335525512695, \t Total Dis Loss : 0.0012223826488479972\n",
      "Steps : 60500, \t Total Gen Loss : 29.175081253051758, \t Total Dis Loss : 0.0004385168431326747\n",
      "Steps : 60600, \t Total Gen Loss : 26.258129119873047, \t Total Dis Loss : 0.0014764296356588602\n",
      "Steps : 60700, \t Total Gen Loss : 27.43336296081543, \t Total Dis Loss : 0.001162281259894371\n",
      "Steps : 60800, \t Total Gen Loss : 32.99394607543945, \t Total Dis Loss : 0.0019422718323767185\n",
      "Steps : 60900, \t Total Gen Loss : 23.459434509277344, \t Total Dis Loss : 0.0495944507420063\n",
      "Steps : 61000, \t Total Gen Loss : 23.938936233520508, \t Total Dis Loss : 0.02217070758342743\n",
      "Steps : 61100, \t Total Gen Loss : 20.947071075439453, \t Total Dis Loss : 1.8627116680145264\n",
      "Steps : 61200, \t Total Gen Loss : 18.697532653808594, \t Total Dis Loss : 1.585646152496338\n",
      "Steps : 61300, \t Total Gen Loss : 17.500089645385742, \t Total Dis Loss : 1.2312960624694824\n",
      "Steps : 61400, \t Total Gen Loss : 21.876771926879883, \t Total Dis Loss : 0.9930991530418396\n",
      "Steps : 61500, \t Total Gen Loss : 21.391361236572266, \t Total Dis Loss : 0.7238632440567017\n",
      "Steps : 61600, \t Total Gen Loss : 20.188493728637695, \t Total Dis Loss : 0.6021299958229065\n",
      "Steps : 61700, \t Total Gen Loss : 29.349979400634766, \t Total Dis Loss : 0.0009565076325088739\n",
      "Steps : 61800, \t Total Gen Loss : 26.275375366210938, \t Total Dis Loss : 0.0016646669246256351\n",
      "Time for epoch 11 is 278.36149501800537 sec\n",
      "Steps : 61900, \t Total Gen Loss : 27.351186752319336, \t Total Dis Loss : 0.008331263437867165\n",
      "Steps : 62000, \t Total Gen Loss : 28.32007598876953, \t Total Dis Loss : 0.0038790458347648382\n",
      "Steps : 62100, \t Total Gen Loss : 33.32722473144531, \t Total Dis Loss : 0.0008254368440248072\n",
      "Steps : 62200, \t Total Gen Loss : 30.08721923828125, \t Total Dis Loss : 0.0005295008304528892\n",
      "Steps : 62300, \t Total Gen Loss : 27.183813095092773, \t Total Dis Loss : 0.0017942889826372266\n",
      "Steps : 62400, \t Total Gen Loss : 33.25189971923828, \t Total Dis Loss : 0.0005247092340141535\n",
      "Steps : 62500, \t Total Gen Loss : 29.456104278564453, \t Total Dis Loss : 0.002040459308773279\n",
      "Steps : 62600, \t Total Gen Loss : 30.847606658935547, \t Total Dis Loss : 0.0006015166291035712\n",
      "Steps : 62700, \t Total Gen Loss : 26.452085494995117, \t Total Dis Loss : 0.01627815142273903\n",
      "Steps : 62800, \t Total Gen Loss : 27.349964141845703, \t Total Dis Loss : 0.0013705729506909847\n",
      "Steps : 62900, \t Total Gen Loss : 28.085712432861328, \t Total Dis Loss : 0.0005318806506693363\n",
      "Steps : 63000, \t Total Gen Loss : 25.319137573242188, \t Total Dis Loss : 0.0026229533832520247\n",
      "Steps : 63100, \t Total Gen Loss : 31.852188110351562, \t Total Dis Loss : 0.0002536142710596323\n",
      "Steps : 63200, \t Total Gen Loss : 28.607364654541016, \t Total Dis Loss : 0.0014599839923903346\n",
      "Steps : 63300, \t Total Gen Loss : 25.523916244506836, \t Total Dis Loss : 0.001841012155637145\n",
      "Steps : 63400, \t Total Gen Loss : 30.083087921142578, \t Total Dis Loss : 0.0028650625608861446\n",
      "Steps : 63500, \t Total Gen Loss : 28.528440475463867, \t Total Dis Loss : 0.001643644762225449\n",
      "Steps : 63600, \t Total Gen Loss : 29.270936965942383, \t Total Dis Loss : 0.0008080631378106773\n",
      "Steps : 63700, \t Total Gen Loss : 29.384992599487305, \t Total Dis Loss : 0.0019521983340382576\n",
      "Steps : 63800, \t Total Gen Loss : 28.360355377197266, \t Total Dis Loss : 0.001137045561335981\n",
      "Steps : 63900, \t Total Gen Loss : 28.677000045776367, \t Total Dis Loss : 0.0005093250656500459\n",
      "Steps : 64000, \t Total Gen Loss : 28.436059951782227, \t Total Dis Loss : 0.00018643433577381074\n",
      "Steps : 64100, \t Total Gen Loss : 31.109840393066406, \t Total Dis Loss : 0.0010219082469120622\n",
      "Steps : 64200, \t Total Gen Loss : 32.08366012573242, \t Total Dis Loss : 0.0010332170641049743\n",
      "Steps : 64300, \t Total Gen Loss : 29.48653221130371, \t Total Dis Loss : 0.001375232357531786\n",
      "Steps : 64400, \t Total Gen Loss : 29.80818748474121, \t Total Dis Loss : 0.0015522113535553217\n",
      "Steps : 64500, \t Total Gen Loss : 29.580299377441406, \t Total Dis Loss : 0.002728748368099332\n",
      "Steps : 64600, \t Total Gen Loss : 28.862319946289062, \t Total Dis Loss : 0.0013177419314160943\n",
      "Steps : 64700, \t Total Gen Loss : 29.00803565979004, \t Total Dis Loss : 0.0003449197392910719\n",
      "Steps : 64800, \t Total Gen Loss : 28.70637321472168, \t Total Dis Loss : 0.00033391150645911694\n",
      "Steps : 64900, \t Total Gen Loss : 29.431621551513672, \t Total Dis Loss : 0.0006308579468168318\n",
      "Steps : 65000, \t Total Gen Loss : 27.25804901123047, \t Total Dis Loss : 0.00046139711048454046\n",
      "Steps : 65100, \t Total Gen Loss : 28.537445068359375, \t Total Dis Loss : 0.0010621909750625491\n",
      "Steps : 65200, \t Total Gen Loss : 28.675600051879883, \t Total Dis Loss : 0.0008193948888219893\n",
      "Steps : 65300, \t Total Gen Loss : 31.21993637084961, \t Total Dis Loss : 0.00090552750043571\n",
      "Steps : 65400, \t Total Gen Loss : 28.316200256347656, \t Total Dis Loss : 0.003136351006105542\n",
      "Steps : 65500, \t Total Gen Loss : 30.933908462524414, \t Total Dis Loss : 0.0005089138285256922\n",
      "Steps : 65600, \t Total Gen Loss : 26.197885513305664, \t Total Dis Loss : 0.0012600121553987265\n",
      "Steps : 65700, \t Total Gen Loss : 27.652936935424805, \t Total Dis Loss : 0.00024209433468058705\n",
      "Steps : 65800, \t Total Gen Loss : 30.046178817749023, \t Total Dis Loss : 0.000587363145314157\n",
      "Steps : 65900, \t Total Gen Loss : 31.24481201171875, \t Total Dis Loss : 0.0005665414500981569\n",
      "Steps : 66000, \t Total Gen Loss : 26.207008361816406, \t Total Dis Loss : 0.000981774996034801\n",
      "Steps : 66100, \t Total Gen Loss : 25.421409606933594, \t Total Dis Loss : 0.0012324077542871237\n",
      "Steps : 66200, \t Total Gen Loss : 27.268999099731445, \t Total Dis Loss : 0.0018748881993815303\n",
      "Steps : 66300, \t Total Gen Loss : 25.501047134399414, \t Total Dis Loss : 0.0005258489400148392\n",
      "Steps : 66400, \t Total Gen Loss : 29.987382888793945, \t Total Dis Loss : 0.002734437817707658\n",
      "Steps : 66500, \t Total Gen Loss : 29.67641830444336, \t Total Dis Loss : 0.0005162414163351059\n",
      "Steps : 66600, \t Total Gen Loss : 27.02399444580078, \t Total Dis Loss : 0.001020737923681736\n",
      "Steps : 66700, \t Total Gen Loss : 27.600971221923828, \t Total Dis Loss : 0.0019716965034604073\n",
      "Steps : 66800, \t Total Gen Loss : 30.521549224853516, \t Total Dis Loss : 0.0003328400489408523\n",
      "Steps : 66900, \t Total Gen Loss : 26.633026123046875, \t Total Dis Loss : 0.00047045486280694604\n",
      "Steps : 67000, \t Total Gen Loss : 26.50432586669922, \t Total Dis Loss : 0.0003993967839051038\n",
      "Steps : 67100, \t Total Gen Loss : 26.43492889404297, \t Total Dis Loss : 0.001501851831562817\n",
      "Steps : 67200, \t Total Gen Loss : 29.564847946166992, \t Total Dis Loss : 0.0010109217837452888\n",
      "Steps : 67300, \t Total Gen Loss : 28.037633895874023, \t Total Dis Loss : 0.0005959240370430052\n",
      "Steps : 67400, \t Total Gen Loss : 29.292552947998047, \t Total Dis Loss : 0.0002968807821162045\n",
      "Steps : 67500, \t Total Gen Loss : 30.21133041381836, \t Total Dis Loss : 0.0009394650114700198\n",
      "Time for epoch 12 is 278.35354828834534 sec\n",
      "Steps : 67600, \t Total Gen Loss : 28.78766632080078, \t Total Dis Loss : 0.0010644819121807814\n",
      "Steps : 67700, \t Total Gen Loss : 30.08428192138672, \t Total Dis Loss : 0.0009756551589816809\n",
      "Steps : 67800, \t Total Gen Loss : 27.392375946044922, \t Total Dis Loss : 0.000285001820884645\n",
      "Steps : 67900, \t Total Gen Loss : 26.981525421142578, \t Total Dis Loss : 0.00036666603409685194\n",
      "Steps : 68000, \t Total Gen Loss : 27.603282928466797, \t Total Dis Loss : 0.000696733535733074\n",
      "Steps : 68100, \t Total Gen Loss : 28.81989097595215, \t Total Dis Loss : 0.0004296052211429924\n",
      "Steps : 68200, \t Total Gen Loss : 30.55204963684082, \t Total Dis Loss : 0.0007153662154451013\n",
      "Steps : 68300, \t Total Gen Loss : 31.466495513916016, \t Total Dis Loss : 0.1031712144613266\n",
      "Steps : 68400, \t Total Gen Loss : 28.90629768371582, \t Total Dis Loss : 0.0005301011260598898\n",
      "Steps : 68500, \t Total Gen Loss : 28.250782012939453, \t Total Dis Loss : 0.0005206075147725642\n",
      "Steps : 68600, \t Total Gen Loss : 29.53835105895996, \t Total Dis Loss : 0.0005519069381989539\n",
      "Steps : 68700, \t Total Gen Loss : 30.63433074951172, \t Total Dis Loss : 0.0004455150046851486\n",
      "Steps : 68800, \t Total Gen Loss : 31.167882919311523, \t Total Dis Loss : 0.005587052553892136\n",
      "Steps : 68900, \t Total Gen Loss : 28.11627197265625, \t Total Dis Loss : 0.001115574617870152\n",
      "Steps : 69000, \t Total Gen Loss : 24.483552932739258, \t Total Dis Loss : 0.0008127779583446681\n",
      "Steps : 69100, \t Total Gen Loss : 30.215456008911133, \t Total Dis Loss : 0.0002767310943454504\n",
      "Steps : 69200, \t Total Gen Loss : 27.140220642089844, \t Total Dis Loss : 0.0005165229667909443\n",
      "Steps : 69300, \t Total Gen Loss : 27.41523551940918, \t Total Dis Loss : 0.0006970406393520534\n",
      "Steps : 69400, \t Total Gen Loss : 27.699798583984375, \t Total Dis Loss : 0.0004891753778792918\n",
      "Steps : 69500, \t Total Gen Loss : 28.12462043762207, \t Total Dis Loss : 0.0002986284380313009\n",
      "Steps : 69600, \t Total Gen Loss : 28.68194007873535, \t Total Dis Loss : 0.0008369130082428455\n",
      "Steps : 69700, \t Total Gen Loss : 24.180561065673828, \t Total Dis Loss : 0.0012054883409291506\n",
      "Steps : 69800, \t Total Gen Loss : 31.00711441040039, \t Total Dis Loss : 0.00040629657451063395\n",
      "Steps : 69900, \t Total Gen Loss : 28.28942108154297, \t Total Dis Loss : 0.0001576581853441894\n",
      "Steps : 70000, \t Total Gen Loss : 26.785274505615234, \t Total Dis Loss : 0.016992080956697464\n",
      "Steps : 70100, \t Total Gen Loss : 33.054847717285156, \t Total Dis Loss : 0.004513546824455261\n",
      "Steps : 70200, \t Total Gen Loss : 27.52199935913086, \t Total Dis Loss : 0.0007221197010949254\n",
      "Steps : 70300, \t Total Gen Loss : 27.989147186279297, \t Total Dis Loss : 0.001088314806111157\n",
      "Steps : 70400, \t Total Gen Loss : 31.5635929107666, \t Total Dis Loss : 0.0013646893203258514\n",
      "Steps : 70500, \t Total Gen Loss : 29.821420669555664, \t Total Dis Loss : 0.0002455255889799446\n",
      "Steps : 70600, \t Total Gen Loss : 28.585186004638672, \t Total Dis Loss : 0.0003956518485210836\n",
      "Steps : 70700, \t Total Gen Loss : 31.667705535888672, \t Total Dis Loss : 0.00041516043711453676\n",
      "Steps : 70800, \t Total Gen Loss : 31.217079162597656, \t Total Dis Loss : 0.05852228403091431\n",
      "Steps : 70900, \t Total Gen Loss : 25.89691162109375, \t Total Dis Loss : 0.012234006077051163\n",
      "Steps : 71000, \t Total Gen Loss : 27.02751922607422, \t Total Dis Loss : 0.0023210037034004927\n",
      "Steps : 71100, \t Total Gen Loss : 26.117218017578125, \t Total Dis Loss : 0.0011743652867153287\n",
      "Steps : 71200, \t Total Gen Loss : 31.595312118530273, \t Total Dis Loss : 0.000433827139204368\n",
      "Steps : 71300, \t Total Gen Loss : 27.51160430908203, \t Total Dis Loss : 0.0006170934066176414\n",
      "Steps : 71400, \t Total Gen Loss : 29.412912368774414, \t Total Dis Loss : 0.00035714474506676197\n",
      "Steps : 71500, \t Total Gen Loss : 28.912569046020508, \t Total Dis Loss : 0.000603642372880131\n",
      "Steps : 71600, \t Total Gen Loss : 29.738327026367188, \t Total Dis Loss : 0.0004391439142636955\n",
      "Steps : 71700, \t Total Gen Loss : 30.328643798828125, \t Total Dis Loss : 0.001615284476429224\n",
      "Steps : 71800, \t Total Gen Loss : 31.372480392456055, \t Total Dis Loss : 0.001034488552249968\n",
      "Steps : 71900, \t Total Gen Loss : 28.304960250854492, \t Total Dis Loss : 0.00039200237370096147\n",
      "Steps : 72000, \t Total Gen Loss : 28.965120315551758, \t Total Dis Loss : 0.00036646792432293296\n",
      "Steps : 72100, \t Total Gen Loss : 26.400039672851562, \t Total Dis Loss : 0.0005001215031370521\n",
      "Steps : 72200, \t Total Gen Loss : 27.024795532226562, \t Total Dis Loss : 0.001303026918321848\n",
      "Steps : 72300, \t Total Gen Loss : 31.258094787597656, \t Total Dis Loss : 0.0007754725520499051\n",
      "Steps : 72400, \t Total Gen Loss : 31.106334686279297, \t Total Dis Loss : 0.001033189008012414\n",
      "Steps : 72500, \t Total Gen Loss : 30.827051162719727, \t Total Dis Loss : 0.0004991958267055452\n",
      "Steps : 72600, \t Total Gen Loss : 29.94231414794922, \t Total Dis Loss : 0.0006784743163734674\n",
      "Steps : 72700, \t Total Gen Loss : 28.776901245117188, \t Total Dis Loss : 0.0006671597948297858\n",
      "Steps : 72800, \t Total Gen Loss : 30.54903793334961, \t Total Dis Loss : 0.0018992162076756358\n",
      "Steps : 72900, \t Total Gen Loss : 27.398983001708984, \t Total Dis Loss : 0.0005293090362101793\n",
      "Steps : 73000, \t Total Gen Loss : 28.877222061157227, \t Total Dis Loss : 0.0007446170784533024\n",
      "Steps : 73100, \t Total Gen Loss : 27.740272521972656, \t Total Dis Loss : 0.0006018278072588146\n",
      "Time for epoch 13 is 277.0183172225952 sec\n",
      "Steps : 73200, \t Total Gen Loss : 26.41084861755371, \t Total Dis Loss : 0.004325329326093197\n",
      "Steps : 73300, \t Total Gen Loss : 28.228527069091797, \t Total Dis Loss : 0.0010196110233664513\n",
      "Steps : 73400, \t Total Gen Loss : 30.788358688354492, \t Total Dis Loss : 0.0004347054928075522\n",
      "Steps : 73500, \t Total Gen Loss : 28.574399948120117, \t Total Dis Loss : 7.607449515489861e-05\n",
      "Steps : 73600, \t Total Gen Loss : 28.795316696166992, \t Total Dis Loss : 0.000955514726229012\n",
      "Steps : 73700, \t Total Gen Loss : 30.297529220581055, \t Total Dis Loss : 0.00027599488385021687\n",
      "Steps : 73800, \t Total Gen Loss : 30.296281814575195, \t Total Dis Loss : 0.00023958987731020898\n",
      "Steps : 73900, \t Total Gen Loss : 29.34333038330078, \t Total Dis Loss : 0.0037225449923425913\n",
      "Steps : 74000, \t Total Gen Loss : 30.51607894897461, \t Total Dis Loss : 0.0007879052427597344\n",
      "Steps : 74100, \t Total Gen Loss : 30.361770629882812, \t Total Dis Loss : 0.0004162888799328357\n",
      "Steps : 74200, \t Total Gen Loss : 29.173736572265625, \t Total Dis Loss : 0.0004855215665884316\n",
      "Steps : 74300, \t Total Gen Loss : 31.65326499938965, \t Total Dis Loss : 0.0016641425900161266\n",
      "Steps : 74400, \t Total Gen Loss : 29.05335807800293, \t Total Dis Loss : 0.0004284499736968428\n",
      "Steps : 74500, \t Total Gen Loss : 31.134540557861328, \t Total Dis Loss : 0.0009471976663917303\n",
      "Steps : 74600, \t Total Gen Loss : 29.549898147583008, \t Total Dis Loss : 0.0002595930709503591\n",
      "Steps : 74700, \t Total Gen Loss : 29.589466094970703, \t Total Dis Loss : 0.00033920095302164555\n",
      "Steps : 74800, \t Total Gen Loss : 30.711360931396484, \t Total Dis Loss : 0.02869357541203499\n",
      "Steps : 74900, \t Total Gen Loss : 29.772090911865234, \t Total Dis Loss : 0.000435262598330155\n",
      "Steps : 75000, \t Total Gen Loss : 28.120441436767578, \t Total Dis Loss : 0.00086357252439484\n",
      "Steps : 75100, \t Total Gen Loss : 32.09709548950195, \t Total Dis Loss : 0.00015139079187065363\n",
      "Steps : 75200, \t Total Gen Loss : 30.62715721130371, \t Total Dis Loss : 0.001244267332367599\n",
      "Steps : 75300, \t Total Gen Loss : 27.173322677612305, \t Total Dis Loss : 0.0006333083147183061\n",
      "Steps : 75400, \t Total Gen Loss : 26.43790054321289, \t Total Dis Loss : 0.000755980727262795\n",
      "Steps : 75500, \t Total Gen Loss : 26.643699645996094, \t Total Dis Loss : 0.0006524758646264672\n",
      "Steps : 75600, \t Total Gen Loss : 28.52508544921875, \t Total Dis Loss : 0.0002351591974729672\n",
      "Steps : 75700, \t Total Gen Loss : 31.176002502441406, \t Total Dis Loss : 0.000807395379524678\n",
      "Steps : 75800, \t Total Gen Loss : 31.505340576171875, \t Total Dis Loss : 0.00246644439175725\n",
      "Steps : 75900, \t Total Gen Loss : 29.724029541015625, \t Total Dis Loss : 6.115766882430762e-05\n",
      "Steps : 76000, \t Total Gen Loss : 25.551788330078125, \t Total Dis Loss : 0.29397326707839966\n",
      "Steps : 76100, \t Total Gen Loss : 30.87942886352539, \t Total Dis Loss : 0.0007372569525614381\n",
      "Steps : 76200, \t Total Gen Loss : 32.57789611816406, \t Total Dis Loss : 0.00032529898453503847\n",
      "Steps : 76300, \t Total Gen Loss : 29.114505767822266, \t Total Dis Loss : 0.000238877342781052\n",
      "Steps : 76400, \t Total Gen Loss : 27.820432662963867, \t Total Dis Loss : 0.0002653492847457528\n",
      "Steps : 76500, \t Total Gen Loss : 27.393178939819336, \t Total Dis Loss : 0.0003368674369994551\n",
      "Steps : 76600, \t Total Gen Loss : 29.34139633178711, \t Total Dis Loss : 0.0009131127735599875\n",
      "Steps : 76700, \t Total Gen Loss : 29.819849014282227, \t Total Dis Loss : 0.00042323864181526005\n",
      "Steps : 76800, \t Total Gen Loss : 32.23411178588867, \t Total Dis Loss : 0.0004490372375585139\n",
      "Steps : 76900, \t Total Gen Loss : 28.026926040649414, \t Total Dis Loss : 0.0004754014662466943\n",
      "Steps : 77000, \t Total Gen Loss : 24.173133850097656, \t Total Dis Loss : 0.0009409563499502838\n",
      "Steps : 77100, \t Total Gen Loss : 28.415699005126953, \t Total Dis Loss : 0.0003799007390625775\n",
      "Steps : 77200, \t Total Gen Loss : 27.96514320373535, \t Total Dis Loss : 0.00037421321030706167\n",
      "Steps : 77300, \t Total Gen Loss : 28.625185012817383, \t Total Dis Loss : 0.002001292770728469\n",
      "Steps : 77400, \t Total Gen Loss : 27.871355056762695, \t Total Dis Loss : 0.0002852107281796634\n",
      "Steps : 77500, \t Total Gen Loss : 27.843191146850586, \t Total Dis Loss : 0.0009185551316477358\n",
      "Steps : 77600, \t Total Gen Loss : 26.19070053100586, \t Total Dis Loss : 0.0003878279821947217\n",
      "Steps : 77700, \t Total Gen Loss : 30.057941436767578, \t Total Dis Loss : 0.00036445428850129247\n",
      "Steps : 77800, \t Total Gen Loss : 29.30800437927246, \t Total Dis Loss : 0.0002902213600464165\n",
      "Steps : 77900, \t Total Gen Loss : 28.502017974853516, \t Total Dis Loss : 0.0016595639754086733\n",
      "Steps : 78000, \t Total Gen Loss : 27.492145538330078, \t Total Dis Loss : 0.0005947515019215643\n",
      "Steps : 78100, \t Total Gen Loss : 23.14887046813965, \t Total Dis Loss : 0.21607552468776703\n",
      "Steps : 78200, \t Total Gen Loss : 24.856563568115234, \t Total Dis Loss : 0.002177719259634614\n",
      "Steps : 78300, \t Total Gen Loss : 29.41718101501465, \t Total Dis Loss : 0.0005054778885096312\n",
      "Steps : 78400, \t Total Gen Loss : 31.80126953125, \t Total Dis Loss : 0.0015581401530653238\n",
      "Steps : 78500, \t Total Gen Loss : 30.163864135742188, \t Total Dis Loss : 0.0001806609216146171\n",
      "Steps : 78600, \t Total Gen Loss : 26.743621826171875, \t Total Dis Loss : 0.03375336155295372\n",
      "Steps : 78700, \t Total Gen Loss : 26.253522872924805, \t Total Dis Loss : 0.0006569086690433323\n",
      "Time for epoch 14 is 276.4544608592987 sec\n",
      "Steps : 78800, \t Total Gen Loss : 30.32845687866211, \t Total Dis Loss : 0.0007133096805773675\n",
      "Steps : 78900, \t Total Gen Loss : 26.066240310668945, \t Total Dis Loss : 0.00015635053568985313\n",
      "Steps : 79000, \t Total Gen Loss : 24.779464721679688, \t Total Dis Loss : 0.00044297301792539656\n",
      "Steps : 79100, \t Total Gen Loss : 29.276350021362305, \t Total Dis Loss : 0.0008529684273526073\n",
      "Steps : 79200, \t Total Gen Loss : 27.227245330810547, \t Total Dis Loss : 0.00026608031475916505\n",
      "Steps : 79300, \t Total Gen Loss : 28.174236297607422, \t Total Dis Loss : 0.0005293782451190054\n",
      "Steps : 79400, \t Total Gen Loss : 28.777660369873047, \t Total Dis Loss : 0.0006303605623543262\n",
      "Steps : 79500, \t Total Gen Loss : 27.340784072875977, \t Total Dis Loss : 0.000567263865377754\n",
      "Steps : 79600, \t Total Gen Loss : 27.691631317138672, \t Total Dis Loss : 0.0013690204359591007\n",
      "Steps : 79700, \t Total Gen Loss : 27.446361541748047, \t Total Dis Loss : 0.0005786805413663387\n",
      "Steps : 79800, \t Total Gen Loss : 28.053001403808594, \t Total Dis Loss : 0.00030637148302048445\n",
      "Steps : 79900, \t Total Gen Loss : 28.983448028564453, \t Total Dis Loss : 0.0009626158280298114\n",
      "Steps : 80000, \t Total Gen Loss : 30.692026138305664, \t Total Dis Loss : 0.0003380808047950268\n",
      "Steps : 80100, \t Total Gen Loss : 24.901174545288086, \t Total Dis Loss : 0.0010670378105714917\n",
      "Steps : 80200, \t Total Gen Loss : 30.216331481933594, \t Total Dis Loss : 0.0003203821543138474\n",
      "Steps : 80300, \t Total Gen Loss : 27.165454864501953, \t Total Dis Loss : 0.0004504814278334379\n",
      "Steps : 80400, \t Total Gen Loss : 32.31001663208008, \t Total Dis Loss : 0.0004852899583056569\n",
      "Steps : 80500, \t Total Gen Loss : 30.7355899810791, \t Total Dis Loss : 0.0002748716506175697\n",
      "Steps : 80600, \t Total Gen Loss : 28.564260482788086, \t Total Dis Loss : 0.0012145357904955745\n",
      "Steps : 80700, \t Total Gen Loss : 30.155376434326172, \t Total Dis Loss : 0.0027337356004863977\n",
      "Steps : 80800, \t Total Gen Loss : 31.085508346557617, \t Total Dis Loss : 0.0003433542442508042\n",
      "Steps : 80900, \t Total Gen Loss : 29.62193489074707, \t Total Dis Loss : 0.003392450977116823\n",
      "Steps : 81000, \t Total Gen Loss : 32.49110794067383, \t Total Dis Loss : 0.00021702342201024294\n",
      "Steps : 81100, \t Total Gen Loss : 29.508995056152344, \t Total Dis Loss : 0.00044950301526114345\n",
      "Steps : 81200, \t Total Gen Loss : 30.891191482543945, \t Total Dis Loss : 0.0027954294346272945\n",
      "Steps : 81300, \t Total Gen Loss : 31.086271286010742, \t Total Dis Loss : 0.00036229987745173275\n",
      "Steps : 81400, \t Total Gen Loss : 27.416736602783203, \t Total Dis Loss : 0.00013081984070595354\n",
      "Steps : 81500, \t Total Gen Loss : 29.812301635742188, \t Total Dis Loss : 0.0011128959013149142\n",
      "Steps : 81600, \t Total Gen Loss : 26.268808364868164, \t Total Dis Loss : 0.0011739933397620916\n",
      "Steps : 81700, \t Total Gen Loss : 28.532020568847656, \t Total Dis Loss : 0.0003327727026771754\n",
      "Steps : 81800, \t Total Gen Loss : 32.069305419921875, \t Total Dis Loss : 0.0003161842469125986\n",
      "Steps : 81900, \t Total Gen Loss : 24.14900016784668, \t Total Dis Loss : 0.00021992971596773714\n",
      "Steps : 82000, \t Total Gen Loss : 24.635330200195312, \t Total Dis Loss : 0.0017731465632095933\n",
      "Steps : 82100, \t Total Gen Loss : 29.10889434814453, \t Total Dis Loss : 0.000401407916797325\n",
      "Steps : 82200, \t Total Gen Loss : 27.29036521911621, \t Total Dis Loss : 0.0009867947082966566\n",
      "Steps : 82300, \t Total Gen Loss : 27.408493041992188, \t Total Dis Loss : 0.002926058601588011\n",
      "Steps : 82400, \t Total Gen Loss : 24.392803192138672, \t Total Dis Loss : 0.0017074926290661097\n",
      "Steps : 82500, \t Total Gen Loss : 26.324512481689453, \t Total Dis Loss : 0.0038122315891087055\n",
      "Steps : 82600, \t Total Gen Loss : 27.81775665283203, \t Total Dis Loss : 0.0005344434757716954\n",
      "Steps : 82700, \t Total Gen Loss : 26.764089584350586, \t Total Dis Loss : 0.0004967282293364406\n",
      "Steps : 82800, \t Total Gen Loss : 27.28345489501953, \t Total Dis Loss : 0.0004081723454874009\n",
      "Steps : 82900, \t Total Gen Loss : 25.88855743408203, \t Total Dis Loss : 0.0003697145148180425\n",
      "Steps : 83000, \t Total Gen Loss : 29.782129287719727, \t Total Dis Loss : 0.0013477124739438295\n",
      "Steps : 83100, \t Total Gen Loss : 26.05601692199707, \t Total Dis Loss : 0.00021904983441345394\n",
      "Steps : 83200, \t Total Gen Loss : 30.51943016052246, \t Total Dis Loss : 0.0007766218041069806\n",
      "Steps : 83300, \t Total Gen Loss : 26.734996795654297, \t Total Dis Loss : 0.001024299650453031\n",
      "Steps : 83400, \t Total Gen Loss : 29.886268615722656, \t Total Dis Loss : 0.00021853340149391443\n",
      "Steps : 83500, \t Total Gen Loss : 28.31572151184082, \t Total Dis Loss : 0.0003639264905359596\n",
      "Steps : 83600, \t Total Gen Loss : 28.33415985107422, \t Total Dis Loss : 0.0005116009851917624\n",
      "Steps : 83700, \t Total Gen Loss : 25.40070152282715, \t Total Dis Loss : 0.0009421801078133285\n",
      "Steps : 83800, \t Total Gen Loss : 27.163352966308594, \t Total Dis Loss : 0.0003233018214814365\n",
      "Steps : 83900, \t Total Gen Loss : 33.67670440673828, \t Total Dis Loss : 0.0004564778646454215\n",
      "Steps : 84000, \t Total Gen Loss : 26.78892707824707, \t Total Dis Loss : 0.0013444467913359404\n",
      "Steps : 84100, \t Total Gen Loss : 30.76337242126465, \t Total Dis Loss : 0.0023177845869213343\n",
      "Steps : 84200, \t Total Gen Loss : 29.717483520507812, \t Total Dis Loss : 0.0013479226035997272\n",
      "Steps : 84300, \t Total Gen Loss : 29.179298400878906, \t Total Dis Loss : 0.0011058810632675886\n",
      "Time for epoch 15 is 276.5972044467926 sec\n",
      "Steps : 84400, \t Total Gen Loss : 27.595121383666992, \t Total Dis Loss : 0.0004662552382797003\n",
      "Steps : 84500, \t Total Gen Loss : 27.137052536010742, \t Total Dis Loss : 0.0007387425284832716\n",
      "Steps : 84600, \t Total Gen Loss : 28.176593780517578, \t Total Dis Loss : 0.0002181116142310202\n",
      "Steps : 84700, \t Total Gen Loss : 30.49298858642578, \t Total Dis Loss : 0.0004306563059799373\n",
      "Steps : 84800, \t Total Gen Loss : 27.619001388549805, \t Total Dis Loss : 0.0010104087414219975\n",
      "Steps : 84900, \t Total Gen Loss : 32.29576873779297, \t Total Dis Loss : 0.000417499803006649\n",
      "Steps : 85000, \t Total Gen Loss : 27.937517166137695, \t Total Dis Loss : 0.0011721765622496605\n",
      "Steps : 85100, \t Total Gen Loss : 28.712488174438477, \t Total Dis Loss : 0.0003338069946039468\n",
      "Steps : 85200, \t Total Gen Loss : 32.25493240356445, \t Total Dis Loss : 0.00011664064368233085\n",
      "Steps : 85300, \t Total Gen Loss : 24.791038513183594, \t Total Dis Loss : 0.5078816413879395\n",
      "Steps : 85400, \t Total Gen Loss : 28.27107048034668, \t Total Dis Loss : 0.0003160728083457798\n",
      "Steps : 85500, \t Total Gen Loss : 27.774993896484375, \t Total Dis Loss : 0.0006693564355373383\n",
      "Steps : 85600, \t Total Gen Loss : 28.696819305419922, \t Total Dis Loss : 0.0005990705685690045\n",
      "Steps : 85700, \t Total Gen Loss : 26.825069427490234, \t Total Dis Loss : 0.00246452703140676\n",
      "Steps : 85800, \t Total Gen Loss : 25.429176330566406, \t Total Dis Loss : 0.000732964079361409\n",
      "Steps : 85900, \t Total Gen Loss : 25.12189483642578, \t Total Dis Loss : 0.0002711039560381323\n",
      "Steps : 86000, \t Total Gen Loss : 28.661640167236328, \t Total Dis Loss : 0.0003189106355421245\n",
      "Steps : 86100, \t Total Gen Loss : 28.040390014648438, \t Total Dis Loss : 0.000731717562302947\n",
      "Steps : 86200, \t Total Gen Loss : 26.297000885009766, \t Total Dis Loss : 0.0027352417819201946\n",
      "Steps : 86300, \t Total Gen Loss : 31.975196838378906, \t Total Dis Loss : 0.0005974288797006011\n",
      "Steps : 86400, \t Total Gen Loss : 28.174373626708984, \t Total Dis Loss : 0.0005707016098313034\n",
      "Steps : 86500, \t Total Gen Loss : 30.58591079711914, \t Total Dis Loss : 0.004485621582716703\n",
      "Steps : 86600, \t Total Gen Loss : 27.66205596923828, \t Total Dis Loss : 0.0015863643493503332\n",
      "Steps : 86700, \t Total Gen Loss : 27.08592414855957, \t Total Dis Loss : 0.05049712210893631\n",
      "Steps : 86800, \t Total Gen Loss : 32.04877853393555, \t Total Dis Loss : 0.00295014469884336\n",
      "Steps : 86900, \t Total Gen Loss : 27.895584106445312, \t Total Dis Loss : 0.0004011301789432764\n",
      "Steps : 87000, \t Total Gen Loss : 29.21563720703125, \t Total Dis Loss : 0.001004332210868597\n",
      "Steps : 87100, \t Total Gen Loss : 28.962299346923828, \t Total Dis Loss : 0.00024279662466142327\n",
      "Steps : 87200, \t Total Gen Loss : 30.37954330444336, \t Total Dis Loss : 0.0012502705212682486\n",
      "Steps : 87300, \t Total Gen Loss : 30.217853546142578, \t Total Dis Loss : 0.00013255294470582157\n",
      "Steps : 87400, \t Total Gen Loss : 31.56893539428711, \t Total Dis Loss : 0.0003186054527759552\n",
      "Steps : 87500, \t Total Gen Loss : 29.236557006835938, \t Total Dis Loss : 0.0002807214914355427\n",
      "Steps : 87600, \t Total Gen Loss : 31.67100715637207, \t Total Dis Loss : 0.000144203586387448\n",
      "Steps : 87700, \t Total Gen Loss : 30.61502456665039, \t Total Dis Loss : 9.465368930250406e-05\n",
      "Steps : 87800, \t Total Gen Loss : 25.485004425048828, \t Total Dis Loss : 0.11870667338371277\n",
      "Steps : 87900, \t Total Gen Loss : 26.576248168945312, \t Total Dis Loss : 0.0024990965612232685\n",
      "Steps : 88000, \t Total Gen Loss : 27.416595458984375, \t Total Dis Loss : 0.0007583388942293823\n",
      "Steps : 88100, \t Total Gen Loss : 25.95207405090332, \t Total Dis Loss : 0.0008583877934142947\n",
      "Steps : 88200, \t Total Gen Loss : 25.06760597229004, \t Total Dis Loss : 0.0007776597631163895\n",
      "Steps : 88300, \t Total Gen Loss : 27.656068801879883, \t Total Dis Loss : 0.00037146895192563534\n",
      "Steps : 88400, \t Total Gen Loss : 28.522851943969727, \t Total Dis Loss : 0.00047894727322272956\n",
      "Steps : 88500, \t Total Gen Loss : 26.557954788208008, \t Total Dis Loss : 0.0003878118877764791\n",
      "Steps : 88600, \t Total Gen Loss : 29.251445770263672, \t Total Dis Loss : 0.0006691759917885065\n",
      "Steps : 88700, \t Total Gen Loss : 24.491947174072266, \t Total Dis Loss : 0.0004860609769821167\n",
      "Steps : 88800, \t Total Gen Loss : 29.66566276550293, \t Total Dis Loss : 0.00031485618092119694\n",
      "Steps : 88900, \t Total Gen Loss : 30.156620025634766, \t Total Dis Loss : 0.0009046592167578638\n",
      "Steps : 89000, \t Total Gen Loss : 28.419023513793945, \t Total Dis Loss : 0.0006299839005805552\n",
      "Steps : 89100, \t Total Gen Loss : 32.921695709228516, \t Total Dis Loss : 0.0011193538084626198\n",
      "Steps : 89200, \t Total Gen Loss : 27.7431640625, \t Total Dis Loss : 0.0005051561165601015\n",
      "Steps : 89300, \t Total Gen Loss : 31.553749084472656, \t Total Dis Loss : 0.0008444993291050196\n",
      "Steps : 89400, \t Total Gen Loss : 30.972856521606445, \t Total Dis Loss : 0.0008353484445251524\n",
      "Steps : 89500, \t Total Gen Loss : 30.208234786987305, \t Total Dis Loss : 9.004270395962521e-05\n",
      "Steps : 89600, \t Total Gen Loss : 27.218704223632812, \t Total Dis Loss : 0.00046847929479554296\n",
      "Steps : 89700, \t Total Gen Loss : 24.44492530822754, \t Total Dis Loss : 0.0010586361167952418\n",
      "Steps : 89800, \t Total Gen Loss : 27.806114196777344, \t Total Dis Loss : 0.0007521038642153144\n",
      "Steps : 89900, \t Total Gen Loss : 28.294580459594727, \t Total Dis Loss : 0.0003625383833423257\n",
      "Steps : 90000, \t Total Gen Loss : 27.39749526977539, \t Total Dis Loss : 0.0010953230084851384\n",
      "Time for epoch 16 is 285.3198707103729 sec\n",
      "Steps : 90100, \t Total Gen Loss : 26.220077514648438, \t Total Dis Loss : 0.0032035065814852715\n",
      "Steps : 90200, \t Total Gen Loss : 26.62458038330078, \t Total Dis Loss : 0.00017915439093485475\n",
      "Steps : 90300, \t Total Gen Loss : 29.712398529052734, \t Total Dis Loss : 0.002237435430288315\n",
      "Steps : 90400, \t Total Gen Loss : 25.529319763183594, \t Total Dis Loss : 0.00015481305308640003\n",
      "Steps : 90500, \t Total Gen Loss : 27.175098419189453, \t Total Dis Loss : 0.00046895790728740394\n",
      "Steps : 90600, \t Total Gen Loss : 29.402917861938477, \t Total Dis Loss : 0.0009133113780990243\n",
      "Steps : 90700, \t Total Gen Loss : 31.039722442626953, \t Total Dis Loss : 5.34318205609452e-05\n",
      "Steps : 90800, \t Total Gen Loss : 28.913724899291992, \t Total Dis Loss : 0.019905367866158485\n",
      "Steps : 90900, \t Total Gen Loss : 29.34746742248535, \t Total Dis Loss : 0.00023813254665583372\n",
      "Steps : 91000, \t Total Gen Loss : 24.602380752563477, \t Total Dis Loss : 0.0021695506293326616\n",
      "Steps : 91100, \t Total Gen Loss : 30.30420684814453, \t Total Dis Loss : 0.0009123574709519744\n",
      "Steps : 91200, \t Total Gen Loss : 26.319522857666016, \t Total Dis Loss : 0.0006378630641847849\n",
      "Steps : 91300, \t Total Gen Loss : 27.007755279541016, \t Total Dis Loss : 0.00048230710672214627\n",
      "Steps : 91400, \t Total Gen Loss : 26.2872314453125, \t Total Dis Loss : 0.0005028038867749274\n",
      "Steps : 91500, \t Total Gen Loss : 27.21112632751465, \t Total Dis Loss : 0.0008187418570742011\n",
      "Steps : 91600, \t Total Gen Loss : 24.960494995117188, \t Total Dis Loss : 0.00035649887286126614\n",
      "Steps : 91700, \t Total Gen Loss : 30.748008728027344, \t Total Dis Loss : 0.0002373866445850581\n",
      "Steps : 91800, \t Total Gen Loss : 31.962217330932617, \t Total Dis Loss : 0.0010877503082156181\n",
      "Steps : 91900, \t Total Gen Loss : 30.824474334716797, \t Total Dis Loss : 0.0005877368967048824\n",
      "Steps : 92000, \t Total Gen Loss : 29.41042137145996, \t Total Dis Loss : 0.00030754233011975884\n",
      "Steps : 92100, \t Total Gen Loss : 28.462072372436523, \t Total Dis Loss : 0.0005142493755556643\n",
      "Steps : 92200, \t Total Gen Loss : 28.286598205566406, \t Total Dis Loss : 0.0008370577124878764\n",
      "Steps : 92300, \t Total Gen Loss : 26.62480354309082, \t Total Dis Loss : 0.0026021115481853485\n",
      "Steps : 92400, \t Total Gen Loss : 27.637149810791016, \t Total Dis Loss : 0.0007008016109466553\n",
      "Steps : 92500, \t Total Gen Loss : 31.23394775390625, \t Total Dis Loss : 0.0004824700590688735\n",
      "Steps : 92600, \t Total Gen Loss : 30.387357711791992, \t Total Dis Loss : 0.000682649842929095\n",
      "Steps : 92700, \t Total Gen Loss : 30.309673309326172, \t Total Dis Loss : 0.0014079473912715912\n",
      "Steps : 92800, \t Total Gen Loss : 28.015445709228516, \t Total Dis Loss : 0.0007164770504459739\n",
      "Steps : 92900, \t Total Gen Loss : 28.926362991333008, \t Total Dis Loss : 0.0005130241625010967\n",
      "Steps : 93000, \t Total Gen Loss : 26.799442291259766, \t Total Dis Loss : 0.0006966340588405728\n",
      "Steps : 93100, \t Total Gen Loss : 27.322248458862305, \t Total Dis Loss : 0.00025725329760462046\n",
      "Steps : 93200, \t Total Gen Loss : 28.21953773498535, \t Total Dis Loss : 0.00028410504455678165\n",
      "Steps : 93300, \t Total Gen Loss : 26.347702026367188, \t Total Dis Loss : 0.00039805113920010626\n",
      "Steps : 93400, \t Total Gen Loss : 28.313987731933594, \t Total Dis Loss : 0.0060995290987193584\n",
      "Steps : 93500, \t Total Gen Loss : 29.23553466796875, \t Total Dis Loss : 0.0004679140984080732\n",
      "Steps : 93600, \t Total Gen Loss : 28.994850158691406, \t Total Dis Loss : 0.00025995337637141347\n",
      "Steps : 93700, \t Total Gen Loss : 27.694482803344727, \t Total Dis Loss : 0.0008382977684959769\n",
      "Steps : 93800, \t Total Gen Loss : 29.015121459960938, \t Total Dis Loss : 0.0005703580100089312\n",
      "Steps : 93900, \t Total Gen Loss : 25.50552749633789, \t Total Dis Loss : 0.0011061764089390635\n",
      "Steps : 94000, \t Total Gen Loss : 28.37786102294922, \t Total Dis Loss : 0.0006360916304402053\n",
      "Steps : 94100, \t Total Gen Loss : 26.719890594482422, \t Total Dis Loss : 0.0005443312111310661\n",
      "Steps : 94200, \t Total Gen Loss : 30.04551887512207, \t Total Dis Loss : 0.0002564425521995872\n",
      "Steps : 94300, \t Total Gen Loss : 28.5369815826416, \t Total Dis Loss : 0.00030811576289124787\n",
      "Steps : 94400, \t Total Gen Loss : 26.735591888427734, \t Total Dis Loss : 0.00012778672680724412\n",
      "Steps : 94500, \t Total Gen Loss : 26.876636505126953, \t Total Dis Loss : 0.0003051709500141442\n",
      "Steps : 94600, \t Total Gen Loss : 29.411911010742188, \t Total Dis Loss : 0.00028258422389626503\n",
      "Steps : 94700, \t Total Gen Loss : 29.430072784423828, \t Total Dis Loss : 0.0003164317167829722\n",
      "Steps : 94800, \t Total Gen Loss : 27.50663948059082, \t Total Dis Loss : 0.00034559547202661633\n",
      "Steps : 94900, \t Total Gen Loss : 28.71811866760254, \t Total Dis Loss : 0.00022639278904534876\n",
      "Steps : 95000, \t Total Gen Loss : 28.99599266052246, \t Total Dis Loss : 0.0003126031660940498\n",
      "Steps : 95100, \t Total Gen Loss : 26.027978897094727, \t Total Dis Loss : 0.00038801657501608133\n",
      "Steps : 95200, \t Total Gen Loss : 28.549076080322266, \t Total Dis Loss : 0.0001569152664160356\n",
      "Steps : 95300, \t Total Gen Loss : 27.929790496826172, \t Total Dis Loss : 0.00031386088812723756\n",
      "Steps : 95400, \t Total Gen Loss : 29.959089279174805, \t Total Dis Loss : 3.270895467721857e-05\n",
      "Steps : 95500, \t Total Gen Loss : 28.336769104003906, \t Total Dis Loss : 0.0008243906195275486\n",
      "Steps : 95600, \t Total Gen Loss : 26.908811569213867, \t Total Dis Loss : 0.0003903871984221041\n",
      "Time for epoch 17 is 278.781769990921 sec\n",
      "Steps : 95700, \t Total Gen Loss : 28.057647705078125, \t Total Dis Loss : 0.0005994817474856973\n",
      "Steps : 95800, \t Total Gen Loss : 28.057003021240234, \t Total Dis Loss : 0.00026330313994549215\n",
      "Steps : 95900, \t Total Gen Loss : 27.692119598388672, \t Total Dis Loss : 0.0001629147445783019\n",
      "Steps : 96000, \t Total Gen Loss : 29.129547119140625, \t Total Dis Loss : 0.0005160014843568206\n",
      "Steps : 96100, \t Total Gen Loss : 27.013378143310547, \t Total Dis Loss : 0.0005346082616597414\n",
      "Steps : 96200, \t Total Gen Loss : 28.080551147460938, \t Total Dis Loss : 0.0003526930231601\n",
      "Steps : 96300, \t Total Gen Loss : 28.446691513061523, \t Total Dis Loss : 0.00045882834820076823\n",
      "Steps : 96400, \t Total Gen Loss : 32.10164260864258, \t Total Dis Loss : 7.509526039939374e-05\n",
      "Steps : 96500, \t Total Gen Loss : 28.24504280090332, \t Total Dis Loss : 0.0004440026532392949\n",
      "Steps : 96600, \t Total Gen Loss : 27.346345901489258, \t Total Dis Loss : 0.0005512021016329527\n",
      "Steps : 96700, \t Total Gen Loss : 32.35773468017578, \t Total Dis Loss : 0.0005243862979114056\n",
      "Steps : 96800, \t Total Gen Loss : 25.164669036865234, \t Total Dis Loss : 0.00034325188607908785\n",
      "Steps : 96900, \t Total Gen Loss : 26.636274337768555, \t Total Dis Loss : 0.0004675777454394847\n",
      "Steps : 97000, \t Total Gen Loss : 27.62205696105957, \t Total Dis Loss : 0.0005206711939536035\n",
      "Steps : 97100, \t Total Gen Loss : 30.610469818115234, \t Total Dis Loss : 0.0003062857431359589\n",
      "Steps : 97200, \t Total Gen Loss : 25.48568344116211, \t Total Dis Loss : 0.0003022550663445145\n",
      "Steps : 97300, \t Total Gen Loss : 26.332075119018555, \t Total Dis Loss : 0.00018414932128507644\n",
      "Steps : 97400, \t Total Gen Loss : 25.796648025512695, \t Total Dis Loss : 0.0002222627226728946\n",
      "Steps : 97500, \t Total Gen Loss : 26.4934024810791, \t Total Dis Loss : 0.00016957575280684978\n",
      "Steps : 97600, \t Total Gen Loss : 25.12244415283203, \t Total Dis Loss : 0.0006989824469201267\n",
      "Steps : 97700, \t Total Gen Loss : 28.58271026611328, \t Total Dis Loss : 0.00019792343664448708\n",
      "Steps : 97800, \t Total Gen Loss : 26.558225631713867, \t Total Dis Loss : 0.00011760122288251296\n",
      "Steps : 97900, \t Total Gen Loss : 27.21331214904785, \t Total Dis Loss : 4.739148062071763e-05\n",
      "Steps : 98000, \t Total Gen Loss : 27.74051284790039, \t Total Dis Loss : 9.369905455969274e-05\n",
      "Steps : 98100, \t Total Gen Loss : 26.70549964904785, \t Total Dis Loss : 0.0001517049386166036\n",
      "Steps : 98200, \t Total Gen Loss : 25.567365646362305, \t Total Dis Loss : 0.00011110467312391847\n",
      "Steps : 98300, \t Total Gen Loss : 24.98219871520996, \t Total Dis Loss : 0.00010060799104394391\n",
      "Steps : 98400, \t Total Gen Loss : 29.340696334838867, \t Total Dis Loss : 0.00012734804477076977\n",
      "Steps : 98500, \t Total Gen Loss : 26.78809356689453, \t Total Dis Loss : 7.363974873442203e-05\n",
      "Steps : 98600, \t Total Gen Loss : 28.83303451538086, \t Total Dis Loss : 6.926052446942776e-05\n",
      "Steps : 98700, \t Total Gen Loss : 28.15959930419922, \t Total Dis Loss : 5.536085518542677e-05\n",
      "Steps : 98800, \t Total Gen Loss : 25.764413833618164, \t Total Dis Loss : 0.0001061296061379835\n",
      "Steps : 98900, \t Total Gen Loss : 23.668582916259766, \t Total Dis Loss : 0.0013189383316785097\n",
      "Steps : 99000, \t Total Gen Loss : 25.028181076049805, \t Total Dis Loss : 0.00015985975915100425\n",
      "Steps : 99100, \t Total Gen Loss : 25.148094177246094, \t Total Dis Loss : 0.0007666124729439616\n",
      "Steps : 99200, \t Total Gen Loss : 23.3065128326416, \t Total Dis Loss : 0.0070557924918830395\n",
      "Steps : 99300, \t Total Gen Loss : 28.644744873046875, \t Total Dis Loss : 0.000184981880011037\n",
      "Steps : 99400, \t Total Gen Loss : 25.002038955688477, \t Total Dis Loss : 0.0010854543652385473\n",
      "Steps : 99500, \t Total Gen Loss : 25.261531829833984, \t Total Dis Loss : 0.0001318230788456276\n",
      "Steps : 99600, \t Total Gen Loss : 26.784574508666992, \t Total Dis Loss : 0.00036659848410636187\n",
      "Steps : 99700, \t Total Gen Loss : 27.05242347717285, \t Total Dis Loss : 0.00015187343524303287\n",
      "Steps : 99800, \t Total Gen Loss : 24.325265884399414, \t Total Dis Loss : 0.00032990192994475365\n",
      "Steps : 99900, \t Total Gen Loss : 24.77486801147461, \t Total Dis Loss : 0.002332556527107954\n",
      "Steps : 100000, \t Total Gen Loss : 26.292165756225586, \t Total Dis Loss : 0.0005667164805345237\n",
      "Steps : 100100, \t Total Gen Loss : 23.642257690429688, \t Total Dis Loss : 0.0024662287905812263\n",
      "Steps : 100200, \t Total Gen Loss : 24.11907196044922, \t Total Dis Loss : 0.00026908644940704107\n",
      "Steps : 100300, \t Total Gen Loss : 26.83843994140625, \t Total Dis Loss : 0.00012125463399570435\n",
      "Steps : 100400, \t Total Gen Loss : 24.532703399658203, \t Total Dis Loss : 0.0003122029302176088\n",
      "Steps : 100500, \t Total Gen Loss : 26.110422134399414, \t Total Dis Loss : 0.00063193344976753\n",
      "Steps : 100600, \t Total Gen Loss : 28.41635513305664, \t Total Dis Loss : 0.0003860998258460313\n",
      "Steps : 100700, \t Total Gen Loss : 26.240793228149414, \t Total Dis Loss : 0.0002843406400643289\n",
      "Steps : 100800, \t Total Gen Loss : 22.985794067382812, \t Total Dis Loss : 0.0004886271781288087\n",
      "Steps : 100900, \t Total Gen Loss : 27.84305763244629, \t Total Dis Loss : 0.00016356287233065814\n",
      "Steps : 101000, \t Total Gen Loss : 27.178991317749023, \t Total Dis Loss : 0.00013373023830354214\n",
      "Steps : 101100, \t Total Gen Loss : 27.471054077148438, \t Total Dis Loss : 8.083160355454311e-05\n",
      "Steps : 101200, \t Total Gen Loss : 26.09701156616211, \t Total Dis Loss : 0.00023239516303874552\n",
      "Time for epoch 18 is 278.80686497688293 sec\n",
      "Steps : 101300, \t Total Gen Loss : 27.416954040527344, \t Total Dis Loss : 0.00026134224026463926\n",
      "Steps : 101400, \t Total Gen Loss : 25.902767181396484, \t Total Dis Loss : 8.154713577823713e-05\n",
      "Steps : 101500, \t Total Gen Loss : 24.469009399414062, \t Total Dis Loss : 0.0003852427762467414\n",
      "Steps : 101600, \t Total Gen Loss : 27.85916519165039, \t Total Dis Loss : 9.709160804050043e-05\n",
      "Steps : 101700, \t Total Gen Loss : 25.312822341918945, \t Total Dis Loss : 0.000293114164378494\n",
      "Steps : 101800, \t Total Gen Loss : 26.138919830322266, \t Total Dis Loss : 0.0006048455834388733\n",
      "Steps : 101900, \t Total Gen Loss : 23.15382957458496, \t Total Dis Loss : 0.0008596805273555219\n",
      "Steps : 102000, \t Total Gen Loss : 25.82386016845703, \t Total Dis Loss : 0.0002810046717058867\n",
      "Steps : 102100, \t Total Gen Loss : 25.30927276611328, \t Total Dis Loss : 0.0004419772303663194\n",
      "Steps : 102200, \t Total Gen Loss : 24.069751739501953, \t Total Dis Loss : 0.0002626118657644838\n",
      "Steps : 102300, \t Total Gen Loss : 23.639896392822266, \t Total Dis Loss : 0.0002607509377412498\n",
      "Steps : 102400, \t Total Gen Loss : 27.923688888549805, \t Total Dis Loss : 0.00016190034511964768\n",
      "Steps : 102500, \t Total Gen Loss : 25.804706573486328, \t Total Dis Loss : 0.00010559782094787806\n",
      "Steps : 102600, \t Total Gen Loss : 27.388809204101562, \t Total Dis Loss : 0.00012239439820405096\n",
      "Steps : 102700, \t Total Gen Loss : 24.14557647705078, \t Total Dis Loss : 9.740584209794179e-05\n",
      "Steps : 102800, \t Total Gen Loss : 25.525814056396484, \t Total Dis Loss : 0.0004448021645657718\n",
      "Steps : 102900, \t Total Gen Loss : 25.132850646972656, \t Total Dis Loss : 0.0002940269187092781\n",
      "Steps : 103000, \t Total Gen Loss : 27.246431350708008, \t Total Dis Loss : 0.00012924123439006507\n",
      "Steps : 103100, \t Total Gen Loss : 29.265565872192383, \t Total Dis Loss : 8.739454642636701e-05\n",
      "Steps : 103200, \t Total Gen Loss : 27.604522705078125, \t Total Dis Loss : 0.0006153929280117154\n",
      "Steps : 103300, \t Total Gen Loss : 25.15457534790039, \t Total Dis Loss : 0.00022531303693540394\n",
      "Steps : 103400, \t Total Gen Loss : 23.640125274658203, \t Total Dis Loss : 0.00022454943973571062\n",
      "Steps : 103500, \t Total Gen Loss : 26.41604232788086, \t Total Dis Loss : 0.00012028927449136972\n",
      "Steps : 103600, \t Total Gen Loss : 27.0478515625, \t Total Dis Loss : 7.82374117989093e-05\n",
      "Steps : 103700, \t Total Gen Loss : 26.690998077392578, \t Total Dis Loss : 0.00010329973883926868\n",
      "Steps : 103800, \t Total Gen Loss : 22.6353759765625, \t Total Dis Loss : 0.00024557183496654034\n",
      "Steps : 103900, \t Total Gen Loss : 25.857715606689453, \t Total Dis Loss : 5.617358328890987e-05\n",
      "Steps : 104000, \t Total Gen Loss : 24.307735443115234, \t Total Dis Loss : 0.0020779771730303764\n",
      "Steps : 104100, \t Total Gen Loss : 23.928470611572266, \t Total Dis Loss : 0.00020346176461316645\n",
      "Steps : 104200, \t Total Gen Loss : 23.139589309692383, \t Total Dis Loss : 0.0007416416192427278\n",
      "Steps : 104300, \t Total Gen Loss : 24.030902862548828, \t Total Dis Loss : 0.0014507835730910301\n",
      "Steps : 104400, \t Total Gen Loss : 23.927501678466797, \t Total Dis Loss : 0.0005132363294251263\n",
      "Steps : 104500, \t Total Gen Loss : 26.096471786499023, \t Total Dis Loss : 0.0003468393115326762\n",
      "Steps : 104600, \t Total Gen Loss : 27.224409103393555, \t Total Dis Loss : 0.00040765240555629134\n",
      "Steps : 104700, \t Total Gen Loss : 26.216474533081055, \t Total Dis Loss : 0.000176683344761841\n",
      "Steps : 104800, \t Total Gen Loss : 25.847545623779297, \t Total Dis Loss : 0.00027531053638085723\n",
      "Steps : 104900, \t Total Gen Loss : 25.286270141601562, \t Total Dis Loss : 0.00035824201768264174\n",
      "Steps : 105000, \t Total Gen Loss : 24.958545684814453, \t Total Dis Loss : 0.00018645824457053095\n",
      "Steps : 105100, \t Total Gen Loss : 25.376197814941406, \t Total Dis Loss : 0.00022579412325285375\n",
      "Steps : 105200, \t Total Gen Loss : 23.255521774291992, \t Total Dis Loss : 0.001738720922730863\n",
      "Steps : 105300, \t Total Gen Loss : 24.828649520874023, \t Total Dis Loss : 0.0015681803924962878\n",
      "Steps : 105400, \t Total Gen Loss : 25.69891929626465, \t Total Dis Loss : 0.00026872314629144967\n",
      "Steps : 105500, \t Total Gen Loss : 26.113109588623047, \t Total Dis Loss : 0.000241380330407992\n",
      "Steps : 105600, \t Total Gen Loss : 25.012704849243164, \t Total Dis Loss : 0.0003657325287349522\n",
      "Steps : 105700, \t Total Gen Loss : 23.796411514282227, \t Total Dis Loss : 0.0006601889617741108\n",
      "Steps : 105800, \t Total Gen Loss : 27.322694778442383, \t Total Dis Loss : 0.00029744792846031487\n",
      "Steps : 105900, \t Total Gen Loss : 24.78726577758789, \t Total Dis Loss : 0.0009280522936023772\n",
      "Steps : 106000, \t Total Gen Loss : 24.542871475219727, \t Total Dis Loss : 0.00032841379288583994\n",
      "Steps : 106100, \t Total Gen Loss : 27.410945892333984, \t Total Dis Loss : 0.00021716665651183575\n",
      "Steps : 106200, \t Total Gen Loss : 24.158584594726562, \t Total Dis Loss : 9.216819307766855e-05\n",
      "Steps : 106300, \t Total Gen Loss : 25.427818298339844, \t Total Dis Loss : 0.00017390119319316\n",
      "Steps : 106400, \t Total Gen Loss : 26.37131690979004, \t Total Dis Loss : 0.00045126647455617785\n",
      "Steps : 106500, \t Total Gen Loss : 24.98480224609375, \t Total Dis Loss : 0.0004319106228649616\n",
      "Steps : 106600, \t Total Gen Loss : 27.034530639648438, \t Total Dis Loss : 0.00041889454587362707\n",
      "Steps : 106700, \t Total Gen Loss : 25.94781494140625, \t Total Dis Loss : 0.001392766134813428\n",
      "Steps : 106800, \t Total Gen Loss : 25.35555648803711, \t Total Dis Loss : 0.0005985823227092624\n",
      "Time for epoch 19 is 278.3432569503784 sec\n",
      "Steps : 106900, \t Total Gen Loss : 26.847402572631836, \t Total Dis Loss : 0.001477277372032404\n",
      "Steps : 107000, \t Total Gen Loss : 23.82419204711914, \t Total Dis Loss : 0.008448287844657898\n",
      "Steps : 107100, \t Total Gen Loss : 26.164447784423828, \t Total Dis Loss : 0.0002449557068757713\n",
      "Steps : 107200, \t Total Gen Loss : 24.404281616210938, \t Total Dis Loss : 0.0007684569573029876\n",
      "Steps : 107300, \t Total Gen Loss : 24.086498260498047, \t Total Dis Loss : 0.00045703042997047305\n",
      "Steps : 107400, \t Total Gen Loss : 25.49603271484375, \t Total Dis Loss : 0.0002058531390503049\n",
      "Steps : 107500, \t Total Gen Loss : 26.601913452148438, \t Total Dis Loss : 0.0003352166968397796\n",
      "Steps : 107600, \t Total Gen Loss : 25.340484619140625, \t Total Dis Loss : 0.0001983064430532977\n",
      "Steps : 107700, \t Total Gen Loss : 25.80777359008789, \t Total Dis Loss : 7.267812179634348e-05\n",
      "Steps : 107800, \t Total Gen Loss : 23.189775466918945, \t Total Dis Loss : 0.000765577016863972\n",
      "Steps : 107900, \t Total Gen Loss : 23.88322639465332, \t Total Dis Loss : 0.0008146789623424411\n",
      "Steps : 108000, \t Total Gen Loss : 22.93638038635254, \t Total Dis Loss : 0.000186858611414209\n",
      "Steps : 108100, \t Total Gen Loss : 29.077754974365234, \t Total Dis Loss : 8.386153058381751e-05\n",
      "Steps : 108200, \t Total Gen Loss : 25.404342651367188, \t Total Dis Loss : 0.0006354937213473022\n",
      "Steps : 108300, \t Total Gen Loss : 28.829984664916992, \t Total Dis Loss : 0.0003505986533127725\n",
      "Steps : 108400, \t Total Gen Loss : 29.28591537475586, \t Total Dis Loss : 0.00013177607615943998\n",
      "Steps : 108500, \t Total Gen Loss : 25.51862335205078, \t Total Dis Loss : 0.0004557637148536742\n",
      "Steps : 108600, \t Total Gen Loss : 26.765331268310547, \t Total Dis Loss : 0.004733275156468153\n",
      "Steps : 108700, \t Total Gen Loss : 22.232650756835938, \t Total Dis Loss : 0.0005808798596262932\n",
      "Steps : 108800, \t Total Gen Loss : 27.212080001831055, \t Total Dis Loss : 0.0007395721622742712\n",
      "Steps : 108900, \t Total Gen Loss : 26.012950897216797, \t Total Dis Loss : 8.50335563882254e-05\n",
      "Steps : 109000, \t Total Gen Loss : 25.10527801513672, \t Total Dis Loss : 0.00024742312962189317\n",
      "Steps : 109100, \t Total Gen Loss : 23.429019927978516, \t Total Dis Loss : 0.0002016890939557925\n",
      "Steps : 109200, \t Total Gen Loss : 24.709287643432617, \t Total Dis Loss : 0.0004670925554819405\n",
      "Steps : 109300, \t Total Gen Loss : 27.89109230041504, \t Total Dis Loss : 0.00034544896334409714\n",
      "Steps : 109400, \t Total Gen Loss : 25.71512794494629, \t Total Dis Loss : 0.0003346493176650256\n",
      "Steps : 109500, \t Total Gen Loss : 26.17906951904297, \t Total Dis Loss : 0.00023310539836529642\n",
      "Steps : 109600, \t Total Gen Loss : 29.030506134033203, \t Total Dis Loss : 0.0001225413870997727\n",
      "Steps : 109700, \t Total Gen Loss : 24.888917922973633, \t Total Dis Loss : 0.00013772556849289685\n",
      "Steps : 109800, \t Total Gen Loss : 27.621299743652344, \t Total Dis Loss : 9.469159704167396e-05\n",
      "Steps : 109900, \t Total Gen Loss : 25.449459075927734, \t Total Dis Loss : 0.0007432170677930117\n",
      "Steps : 110000, \t Total Gen Loss : 23.682466506958008, \t Total Dis Loss : 0.0010015818988904357\n",
      "Steps : 110100, \t Total Gen Loss : 25.464065551757812, \t Total Dis Loss : 0.0002728362160269171\n",
      "Steps : 110200, \t Total Gen Loss : 26.45752716064453, \t Total Dis Loss : 0.00035145561560057104\n",
      "Steps : 110300, \t Total Gen Loss : 26.56045913696289, \t Total Dis Loss : 0.00010494064190424979\n",
      "Steps : 110400, \t Total Gen Loss : 26.033123016357422, \t Total Dis Loss : 8.48206109367311e-05\n",
      "Steps : 110500, \t Total Gen Loss : 25.67400360107422, \t Total Dis Loss : 0.00017157317779492587\n",
      "Steps : 110600, \t Total Gen Loss : 26.109270095825195, \t Total Dis Loss : 8.335492748301476e-05\n",
      "Steps : 110700, \t Total Gen Loss : 25.886112213134766, \t Total Dis Loss : 0.00011424210970290005\n",
      "Steps : 110800, \t Total Gen Loss : 27.14301872253418, \t Total Dis Loss : 0.0002493001229595393\n",
      "Steps : 110900, \t Total Gen Loss : 24.540271759033203, \t Total Dis Loss : 5.9172707551624626e-05\n",
      "Steps : 111000, \t Total Gen Loss : 24.51770782470703, \t Total Dis Loss : 0.0004672096692956984\n",
      "Steps : 111100, \t Total Gen Loss : 23.33746337890625, \t Total Dis Loss : 0.00032233487581834197\n",
      "Steps : 111200, \t Total Gen Loss : 25.18933868408203, \t Total Dis Loss : 8.900694956537336e-05\n",
      "Steps : 111300, \t Total Gen Loss : 24.90167236328125, \t Total Dis Loss : 0.0002449009334668517\n",
      "Steps : 111400, \t Total Gen Loss : 24.00116729736328, \t Total Dis Loss : 7.197444210760295e-05\n",
      "Steps : 111500, \t Total Gen Loss : 24.438549041748047, \t Total Dis Loss : 0.00024199938343372196\n",
      "Steps : 111600, \t Total Gen Loss : 23.99411392211914, \t Total Dis Loss : 0.00031036121072247624\n",
      "Steps : 111700, \t Total Gen Loss : 28.05975341796875, \t Total Dis Loss : 8.808381244307384e-05\n",
      "Steps : 111800, \t Total Gen Loss : 26.730913162231445, \t Total Dis Loss : 0.00010651801858330145\n",
      "Steps : 111900, \t Total Gen Loss : 29.711532592773438, \t Total Dis Loss : 4.255609746905975e-05\n",
      "Steps : 112000, \t Total Gen Loss : 25.944501876831055, \t Total Dis Loss : 8.740814519114792e-05\n",
      "Steps : 112100, \t Total Gen Loss : 24.2139949798584, \t Total Dis Loss : 0.00032295327400788665\n",
      "Steps : 112200, \t Total Gen Loss : 26.06065559387207, \t Total Dis Loss : 0.00011875329073518515\n",
      "Steps : 112300, \t Total Gen Loss : 24.815767288208008, \t Total Dis Loss : 8.177717973012477e-05\n",
      "Steps : 112400, \t Total Gen Loss : 27.1827449798584, \t Total Dis Loss : 4.004117363365367e-05\n",
      "Steps : 112500, \t Total Gen Loss : 27.116111755371094, \t Total Dis Loss : 0.00025522190844640136\n",
      "Time for epoch 20 is 283.40610694885254 sec\n",
      "Steps : 112600, \t Total Gen Loss : 25.906707763671875, \t Total Dis Loss : 0.0003533083654474467\n",
      "Steps : 112700, \t Total Gen Loss : 26.14979362487793, \t Total Dis Loss : 0.00010146127169718966\n",
      "Steps : 112800, \t Total Gen Loss : 29.33339500427246, \t Total Dis Loss : 0.00013341769226826727\n",
      "Steps : 112900, \t Total Gen Loss : 23.47747039794922, \t Total Dis Loss : 0.0001820962643250823\n",
      "Steps : 113000, \t Total Gen Loss : 26.01566505432129, \t Total Dis Loss : 0.00020878840587101877\n",
      "Steps : 113100, \t Total Gen Loss : 26.000097274780273, \t Total Dis Loss : 0.0001901680661831051\n",
      "Steps : 113200, \t Total Gen Loss : 23.891828536987305, \t Total Dis Loss : 0.00012930948287248611\n",
      "Steps : 113300, \t Total Gen Loss : 24.743322372436523, \t Total Dis Loss : 0.0002973264781758189\n",
      "Steps : 113400, \t Total Gen Loss : 24.404747009277344, \t Total Dis Loss : 0.0003185085952281952\n",
      "Steps : 113500, \t Total Gen Loss : 23.26044273376465, \t Total Dis Loss : 0.0002669024106580764\n",
      "Steps : 113600, \t Total Gen Loss : 23.92989158630371, \t Total Dis Loss : 0.0017386757535859942\n",
      "Steps : 113700, \t Total Gen Loss : 23.60108757019043, \t Total Dis Loss : 0.0014187172055244446\n",
      "Steps : 113800, \t Total Gen Loss : 27.105661392211914, \t Total Dis Loss : 0.00010963305976474658\n",
      "Steps : 113900, \t Total Gen Loss : 26.12596893310547, \t Total Dis Loss : 8.528726175427437e-05\n",
      "Steps : 114000, \t Total Gen Loss : 25.87779998779297, \t Total Dis Loss : 0.00020528298045974225\n",
      "Steps : 114100, \t Total Gen Loss : 23.859769821166992, \t Total Dis Loss : 0.0010071095312014222\n",
      "Steps : 114200, \t Total Gen Loss : 24.14923667907715, \t Total Dis Loss : 0.0006543007330037653\n",
      "Steps : 114300, \t Total Gen Loss : 27.489582061767578, \t Total Dis Loss : 0.0006568614626303315\n",
      "Steps : 114400, \t Total Gen Loss : 23.29572868347168, \t Total Dis Loss : 0.00032206534524448216\n",
      "Steps : 114500, \t Total Gen Loss : 24.9141902923584, \t Total Dis Loss : 0.0003987110103480518\n",
      "Steps : 114600, \t Total Gen Loss : 26.40876579284668, \t Total Dis Loss : 0.0001953651662915945\n",
      "Steps : 114700, \t Total Gen Loss : 26.5379638671875, \t Total Dis Loss : 8.95449411473237e-05\n",
      "Steps : 114800, \t Total Gen Loss : 26.419349670410156, \t Total Dis Loss : 0.0009604887454770505\n",
      "Steps : 114900, \t Total Gen Loss : 27.517059326171875, \t Total Dis Loss : 5.64128058613278e-05\n",
      "Steps : 115000, \t Total Gen Loss : 26.40578842163086, \t Total Dis Loss : 6.577733438462019e-05\n",
      "Steps : 115100, \t Total Gen Loss : 26.950969696044922, \t Total Dis Loss : 7.677980465814471e-05\n",
      "Steps : 115200, \t Total Gen Loss : 25.557010650634766, \t Total Dis Loss : 5.500496627064422e-05\n",
      "Steps : 115300, \t Total Gen Loss : 26.101165771484375, \t Total Dis Loss : 0.0003173996810801327\n",
      "Steps : 115400, \t Total Gen Loss : 24.784564971923828, \t Total Dis Loss : 2.7951795345870778e-05\n",
      "Steps : 115500, \t Total Gen Loss : 24.877201080322266, \t Total Dis Loss : 0.0002867684233933687\n",
      "Steps : 115600, \t Total Gen Loss : 25.63441276550293, \t Total Dis Loss : 0.0009191350545734167\n",
      "Steps : 115700, \t Total Gen Loss : 23.643634796142578, \t Total Dis Loss : 0.041168246418237686\n",
      "Steps : 115800, \t Total Gen Loss : 26.18355369567871, \t Total Dis Loss : 5.034040805185214e-05\n",
      "Steps : 115900, \t Total Gen Loss : 25.556726455688477, \t Total Dis Loss : 0.0001748747017700225\n",
      "Steps : 116000, \t Total Gen Loss : 26.141712188720703, \t Total Dis Loss : 0.00043315417133271694\n",
      "Steps : 116100, \t Total Gen Loss : 23.607070922851562, \t Total Dis Loss : 0.00037052147672511637\n",
      "Steps : 116200, \t Total Gen Loss : 24.443180084228516, \t Total Dis Loss : 0.0005135361570864916\n",
      "Steps : 116300, \t Total Gen Loss : 25.948566436767578, \t Total Dis Loss : 0.00022596966300625354\n",
      "Steps : 116400, \t Total Gen Loss : 28.59361457824707, \t Total Dis Loss : 0.00023038349172566086\n",
      "Steps : 116500, \t Total Gen Loss : 25.466196060180664, \t Total Dis Loss : 0.0002203913318226114\n",
      "Steps : 116600, \t Total Gen Loss : 25.711427688598633, \t Total Dis Loss : 0.0007576211355626583\n",
      "Steps : 116700, \t Total Gen Loss : 22.685272216796875, \t Total Dis Loss : 0.00034601116203702986\n",
      "Steps : 116800, \t Total Gen Loss : 28.09693717956543, \t Total Dis Loss : 0.0005818884819746017\n",
      "Steps : 116900, \t Total Gen Loss : 28.306961059570312, \t Total Dis Loss : 0.0001046098186634481\n",
      "Steps : 117000, \t Total Gen Loss : 27.510704040527344, \t Total Dis Loss : 0.00010337845742469653\n",
      "Steps : 117100, \t Total Gen Loss : 25.719833374023438, \t Total Dis Loss : 0.0009514017147012055\n",
      "Steps : 117200, \t Total Gen Loss : 25.128398895263672, \t Total Dis Loss : 0.000484863092424348\n",
      "Steps : 117300, \t Total Gen Loss : 26.8007755279541, \t Total Dis Loss : 0.00073453807272017\n",
      "Steps : 117400, \t Total Gen Loss : 25.87386131286621, \t Total Dis Loss : 0.00047833152348175645\n",
      "Steps : 117500, \t Total Gen Loss : 23.430822372436523, \t Total Dis Loss : 0.0004621326515916735\n",
      "Steps : 117600, \t Total Gen Loss : 26.144710540771484, \t Total Dis Loss : 0.0007558133802376688\n",
      "Steps : 117700, \t Total Gen Loss : 25.280454635620117, \t Total Dis Loss : 9.468385542277247e-05\n",
      "Steps : 117800, \t Total Gen Loss : 27.46521759033203, \t Total Dis Loss : 0.00014350212586577982\n",
      "Steps : 117900, \t Total Gen Loss : 28.151020050048828, \t Total Dis Loss : 0.00011887640721397474\n",
      "Steps : 118000, \t Total Gen Loss : 27.93299102783203, \t Total Dis Loss : 9.710187441669405e-05\n",
      "Steps : 118100, \t Total Gen Loss : 27.14569854736328, \t Total Dis Loss : 0.00021028437186032534\n",
      "Time for epoch 21 is 257.5763165950775 sec\n",
      "Steps : 118200, \t Total Gen Loss : 30.6755428314209, \t Total Dis Loss : 1.0639540050760843e-05\n",
      "Steps : 118300, \t Total Gen Loss : 26.5993709564209, \t Total Dis Loss : 0.00045764897367917\n",
      "Steps : 118400, \t Total Gen Loss : 27.003881454467773, \t Total Dis Loss : 0.0001753207907313481\n",
      "Steps : 118500, \t Total Gen Loss : 27.972129821777344, \t Total Dis Loss : 9.871153451967984e-05\n",
      "Steps : 118600, \t Total Gen Loss : 26.348968505859375, \t Total Dis Loss : 0.0001765374036040157\n",
      "Steps : 118700, \t Total Gen Loss : 27.078330993652344, \t Total Dis Loss : 0.00012946453352924436\n",
      "Steps : 118800, \t Total Gen Loss : 26.941036224365234, \t Total Dis Loss : 8.845508273225278e-05\n",
      "Steps : 118900, \t Total Gen Loss : 27.7702693939209, \t Total Dis Loss : 4.7395777073688805e-05\n",
      "Steps : 119000, \t Total Gen Loss : 27.7873592376709, \t Total Dis Loss : 9.936979768099263e-05\n",
      "Steps : 119100, \t Total Gen Loss : 31.771398544311523, \t Total Dis Loss : 8.73694516485557e-05\n",
      "Steps : 119200, \t Total Gen Loss : 28.219404220581055, \t Total Dis Loss : 3.104035204160027e-05\n",
      "Steps : 119300, \t Total Gen Loss : 25.78815460205078, \t Total Dis Loss : 0.00027241333737038076\n",
      "Steps : 119400, \t Total Gen Loss : 30.979331970214844, \t Total Dis Loss : 0.00019285613961983472\n",
      "Steps : 119500, \t Total Gen Loss : 25.293846130371094, \t Total Dis Loss : 0.0004927775007672608\n",
      "Steps : 119600, \t Total Gen Loss : 29.09147834777832, \t Total Dis Loss : 0.00016027706442400813\n",
      "Steps : 119700, \t Total Gen Loss : 26.307186126708984, \t Total Dis Loss : 0.0005122453439980745\n",
      "Steps : 119800, \t Total Gen Loss : 25.017292022705078, \t Total Dis Loss : 0.00024919709539972246\n",
      "Steps : 119900, \t Total Gen Loss : 28.39551544189453, \t Total Dis Loss : 9.79287433438003e-05\n",
      "Steps : 120000, \t Total Gen Loss : 28.87677764892578, \t Total Dis Loss : 9.498967847321182e-05\n",
      "Steps : 120100, \t Total Gen Loss : 24.67657470703125, \t Total Dis Loss : 0.00012468849308788776\n",
      "Steps : 120200, \t Total Gen Loss : 31.80664825439453, \t Total Dis Loss : 0.0001326782366959378\n",
      "Steps : 120300, \t Total Gen Loss : 28.255985260009766, \t Total Dis Loss : 6.687116547254845e-05\n",
      "Steps : 120400, \t Total Gen Loss : 27.719409942626953, \t Total Dis Loss : 5.802719169878401e-05\n",
      "Steps : 120500, \t Total Gen Loss : 26.780338287353516, \t Total Dis Loss : 6.276960630202666e-05\n",
      "Steps : 120600, \t Total Gen Loss : 27.3975830078125, \t Total Dis Loss : 7.717601693002507e-05\n",
      "Steps : 120700, \t Total Gen Loss : 31.385160446166992, \t Total Dis Loss : 0.0009253554162569344\n",
      "Steps : 120800, \t Total Gen Loss : 31.81439208984375, \t Total Dis Loss : 0.00015414385416079313\n",
      "Steps : 120900, \t Total Gen Loss : 26.5792179107666, \t Total Dis Loss : 0.0001155103454948403\n",
      "Steps : 121000, \t Total Gen Loss : 31.369064331054688, \t Total Dis Loss : 0.0036821679677814245\n",
      "Steps : 121100, \t Total Gen Loss : 26.292688369750977, \t Total Dis Loss : 0.05083918571472168\n",
      "Steps : 121200, \t Total Gen Loss : 27.08027458190918, \t Total Dis Loss : 0.0002506178861949593\n",
      "Steps : 121300, \t Total Gen Loss : 26.14982795715332, \t Total Dis Loss : 0.0037713521160185337\n",
      "Steps : 121400, \t Total Gen Loss : 27.366260528564453, \t Total Dis Loss : 0.00023065420100465417\n",
      "Steps : 121500, \t Total Gen Loss : 23.671039581298828, \t Total Dis Loss : 0.00018299029034096748\n",
      "Steps : 121600, \t Total Gen Loss : 26.379384994506836, \t Total Dis Loss : 0.00027879682602360845\n",
      "Steps : 121700, \t Total Gen Loss : 26.889680862426758, \t Total Dis Loss : 0.000128976593259722\n",
      "Steps : 121800, \t Total Gen Loss : 26.324403762817383, \t Total Dis Loss : 0.00013583243708126247\n",
      "Steps : 121900, \t Total Gen Loss : 27.6260986328125, \t Total Dis Loss : 0.0001403929345542565\n",
      "Steps : 122000, \t Total Gen Loss : 26.040142059326172, \t Total Dis Loss : 0.024162186309695244\n",
      "Steps : 122100, \t Total Gen Loss : 23.89398765563965, \t Total Dis Loss : 0.00016528696869499981\n",
      "Steps : 122200, \t Total Gen Loss : 26.57388687133789, \t Total Dis Loss : 0.00018214441661257297\n",
      "Steps : 122300, \t Total Gen Loss : 24.485538482666016, \t Total Dis Loss : 0.000122893150546588\n",
      "Steps : 122400, \t Total Gen Loss : 26.124614715576172, \t Total Dis Loss : 0.00011846031702589244\n",
      "Steps : 122500, \t Total Gen Loss : 25.031530380249023, \t Total Dis Loss : 8.65046531544067e-05\n",
      "Steps : 122600, \t Total Gen Loss : 25.539125442504883, \t Total Dis Loss : 9.220496576745063e-05\n",
      "Steps : 122700, \t Total Gen Loss : 26.084447860717773, \t Total Dis Loss : 8.361382060684264e-05\n",
      "Steps : 122800, \t Total Gen Loss : 26.69667625427246, \t Total Dis Loss : 4.946692570229061e-05\n",
      "Steps : 122900, \t Total Gen Loss : 26.47711181640625, \t Total Dis Loss : 5.0331142119830474e-05\n",
      "Steps : 123000, \t Total Gen Loss : 25.156524658203125, \t Total Dis Loss : 0.00017011119052767754\n",
      "Steps : 123100, \t Total Gen Loss : 28.57701873779297, \t Total Dis Loss : 8.464651182293892e-05\n",
      "Steps : 123200, \t Total Gen Loss : 29.312654495239258, \t Total Dis Loss : 0.00014260994794312865\n",
      "Steps : 123300, \t Total Gen Loss : 28.64623260498047, \t Total Dis Loss : 0.0001068307101377286\n",
      "Steps : 123400, \t Total Gen Loss : 25.39250946044922, \t Total Dis Loss : 0.0001089517681975849\n",
      "Steps : 123500, \t Total Gen Loss : 26.841629028320312, \t Total Dis Loss : 6.635718455072492e-05\n",
      "Steps : 123600, \t Total Gen Loss : 26.67032814025879, \t Total Dis Loss : 5.493986827787012e-05\n",
      "Steps : 123700, \t Total Gen Loss : 28.677860260009766, \t Total Dis Loss : 6.268610741244629e-05\n",
      "Time for epoch 22 is 260.3283517360687 sec\n",
      "Steps : 123800, \t Total Gen Loss : 28.613971710205078, \t Total Dis Loss : 3.905739140463993e-05\n",
      "Steps : 123900, \t Total Gen Loss : 26.285146713256836, \t Total Dis Loss : 4.1520321246935055e-05\n",
      "Steps : 124000, \t Total Gen Loss : 28.15208625793457, \t Total Dis Loss : 5.961102215223946e-05\n",
      "Steps : 124100, \t Total Gen Loss : 27.28941535949707, \t Total Dis Loss : 6.331979966489598e-05\n",
      "Steps : 124200, \t Total Gen Loss : 26.66828155517578, \t Total Dis Loss : 4.975718184141442e-05\n",
      "Steps : 124300, \t Total Gen Loss : 29.308788299560547, \t Total Dis Loss : 3.712360194185749e-05\n",
      "Steps : 124400, \t Total Gen Loss : 27.607656478881836, \t Total Dis Loss : 4.5523378503276035e-05\n",
      "Steps : 124500, \t Total Gen Loss : 28.60869026184082, \t Total Dis Loss : 0.0006264706025831401\n",
      "Steps : 124600, \t Total Gen Loss : 25.736417770385742, \t Total Dis Loss : 2.6487627110327594e-05\n",
      "Steps : 124700, \t Total Gen Loss : 30.895511627197266, \t Total Dis Loss : 2.662149927346036e-05\n",
      "Steps : 124800, \t Total Gen Loss : 23.687061309814453, \t Total Dis Loss : 0.00019133251043967903\n",
      "Steps : 124900, \t Total Gen Loss : 26.833768844604492, \t Total Dis Loss : 0.00011116667155874893\n",
      "Steps : 125000, \t Total Gen Loss : 27.59267807006836, \t Total Dis Loss : 0.00017560413107275963\n",
      "Steps : 125100, \t Total Gen Loss : 26.53657341003418, \t Total Dis Loss : 0.0012959465384483337\n",
      "Steps : 125200, \t Total Gen Loss : 23.78626251220703, \t Total Dis Loss : 0.00010340261360397562\n",
      "Steps : 125300, \t Total Gen Loss : 26.481931686401367, \t Total Dis Loss : 0.00011306480155326426\n",
      "Steps : 125400, \t Total Gen Loss : 25.85059928894043, \t Total Dis Loss : 0.00015245485701598227\n",
      "Steps : 125500, \t Total Gen Loss : 25.995168685913086, \t Total Dis Loss : 9.205243259202689e-05\n",
      "Steps : 125600, \t Total Gen Loss : 25.76384735107422, \t Total Dis Loss : 0.00040031911339610815\n",
      "Steps : 125700, \t Total Gen Loss : 28.950145721435547, \t Total Dis Loss : 5.8786354202311486e-05\n",
      "Steps : 125800, \t Total Gen Loss : 23.97399139404297, \t Total Dis Loss : 0.0001900363276945427\n",
      "Steps : 125900, \t Total Gen Loss : 25.580766677856445, \t Total Dis Loss : 0.0003088288358412683\n",
      "Steps : 126000, \t Total Gen Loss : 23.922199249267578, \t Total Dis Loss : 0.0008565492462366819\n",
      "Steps : 126100, \t Total Gen Loss : 23.521570205688477, \t Total Dis Loss : 0.0005690507823601365\n",
      "Steps : 126200, \t Total Gen Loss : 24.134693145751953, \t Total Dis Loss : 0.00022487201204057783\n",
      "Steps : 126300, \t Total Gen Loss : 28.195344924926758, \t Total Dis Loss : 7.548491703346372e-05\n",
      "Steps : 126400, \t Total Gen Loss : 25.061416625976562, \t Total Dis Loss : 7.610854663653299e-05\n",
      "Steps : 126500, \t Total Gen Loss : 27.436166763305664, \t Total Dis Loss : 5.175971455173567e-05\n",
      "Steps : 126600, \t Total Gen Loss : 27.982784271240234, \t Total Dis Loss : 9.46129803196527e-05\n",
      "Steps : 126700, \t Total Gen Loss : 25.91314697265625, \t Total Dis Loss : 0.00011537462705746293\n",
      "Steps : 126800, \t Total Gen Loss : 26.112491607666016, \t Total Dis Loss : 0.0001271163346245885\n",
      "Steps : 126900, \t Total Gen Loss : 25.789981842041016, \t Total Dis Loss : 0.00040180893847718835\n",
      "Steps : 127000, \t Total Gen Loss : 23.757511138916016, \t Total Dis Loss : 0.0003168424591422081\n",
      "Steps : 127100, \t Total Gen Loss : 26.233905792236328, \t Total Dis Loss : 0.00012684197281487286\n",
      "Steps : 127200, \t Total Gen Loss : 26.457733154296875, \t Total Dis Loss : 0.00011076183000113815\n",
      "Steps : 127300, \t Total Gen Loss : 26.213205337524414, \t Total Dis Loss : 7.116523192962632e-05\n",
      "Steps : 127400, \t Total Gen Loss : 26.16112518310547, \t Total Dis Loss : 0.00052473577670753\n",
      "Steps : 127500, \t Total Gen Loss : 26.144624710083008, \t Total Dis Loss : 0.0003100195899605751\n",
      "Steps : 127600, \t Total Gen Loss : 26.24420738220215, \t Total Dis Loss : 9.365165169583634e-05\n",
      "Steps : 127700, \t Total Gen Loss : 26.941194534301758, \t Total Dis Loss : 6.66314153932035e-05\n",
      "Steps : 127800, \t Total Gen Loss : 28.55718421936035, \t Total Dis Loss : 8.577050175517797e-05\n",
      "Steps : 127900, \t Total Gen Loss : 25.458005905151367, \t Total Dis Loss : 6.26478431513533e-05\n",
      "Steps : 128000, \t Total Gen Loss : 25.529638290405273, \t Total Dis Loss : 4.97550172440242e-05\n",
      "Steps : 128100, \t Total Gen Loss : 27.232574462890625, \t Total Dis Loss : 0.0002167557249777019\n",
      "Steps : 128200, \t Total Gen Loss : 24.423477172851562, \t Total Dis Loss : 9.312949259765446e-05\n",
      "Steps : 128300, \t Total Gen Loss : 26.78417205810547, \t Total Dis Loss : 2.9074144549667835e-05\n",
      "Steps : 128400, \t Total Gen Loss : 26.262210845947266, \t Total Dis Loss : 3.6241883208276704e-05\n",
      "Steps : 128500, \t Total Gen Loss : 27.287111282348633, \t Total Dis Loss : 0.00021979572193231434\n",
      "Steps : 128600, \t Total Gen Loss : 23.556411743164062, \t Total Dis Loss : 0.0002300182095495984\n",
      "Steps : 128700, \t Total Gen Loss : 27.234603881835938, \t Total Dis Loss : 7.992249447852373e-05\n",
      "Steps : 128800, \t Total Gen Loss : 28.529468536376953, \t Total Dis Loss : 5.68023206142243e-05\n",
      "Steps : 128900, \t Total Gen Loss : 25.21680450439453, \t Total Dis Loss : 0.00013358343858271837\n",
      "Steps : 129000, \t Total Gen Loss : 25.895763397216797, \t Total Dis Loss : 0.000227436437853612\n",
      "Steps : 129100, \t Total Gen Loss : 26.718719482421875, \t Total Dis Loss : 5.522079300135374e-05\n",
      "Steps : 129200, \t Total Gen Loss : 27.96588134765625, \t Total Dis Loss : 5.472121847560629e-05\n",
      "Steps : 129300, \t Total Gen Loss : 26.080408096313477, \t Total Dis Loss : 0.00023645056353416294\n",
      "Time for epoch 23 is 260.17283368110657 sec\n",
      "Steps : 129400, \t Total Gen Loss : 26.382526397705078, \t Total Dis Loss : 5.408611104940064e-05\n",
      "Steps : 129500, \t Total Gen Loss : 27.193645477294922, \t Total Dis Loss : 4.1121646063402295e-05\n",
      "Steps : 129600, \t Total Gen Loss : 29.069721221923828, \t Total Dis Loss : 4.004456786788069e-05\n",
      "Steps : 129700, \t Total Gen Loss : 26.824277877807617, \t Total Dis Loss : 2.643180232553277e-05\n",
      "Steps : 129800, \t Total Gen Loss : 27.727632522583008, \t Total Dis Loss : 7.313912647077814e-05\n",
      "Steps : 129900, \t Total Gen Loss : 26.3392391204834, \t Total Dis Loss : 0.00010790394298965111\n",
      "Steps : 130000, \t Total Gen Loss : 30.414806365966797, \t Total Dis Loss : 1.3843446140526794e-05\n",
      "Steps : 130100, \t Total Gen Loss : 27.12107276916504, \t Total Dis Loss : 0.00038260300061665475\n",
      "Steps : 130200, \t Total Gen Loss : 28.78873062133789, \t Total Dis Loss : 0.00029397665639407933\n",
      "Steps : 130300, \t Total Gen Loss : 26.84290313720703, \t Total Dis Loss : 7.378355076070875e-05\n",
      "Steps : 130400, \t Total Gen Loss : 30.5233211517334, \t Total Dis Loss : 2.971483627334237e-05\n",
      "Steps : 130500, \t Total Gen Loss : 28.000755310058594, \t Total Dis Loss : 0.0003043175966013223\n",
      "Steps : 130600, \t Total Gen Loss : 29.71729278564453, \t Total Dis Loss : 9.203129593515769e-05\n",
      "Steps : 130700, \t Total Gen Loss : 28.81606101989746, \t Total Dis Loss : 0.00011812346201622859\n",
      "Steps : 130800, \t Total Gen Loss : 26.919979095458984, \t Total Dis Loss : 9.178335312753916e-05\n",
      "Steps : 130900, \t Total Gen Loss : 26.074846267700195, \t Total Dis Loss : 0.00011958458344452083\n",
      "Steps : 131000, \t Total Gen Loss : 27.809396743774414, \t Total Dis Loss : 0.00046122423373162746\n",
      "Steps : 131100, \t Total Gen Loss : 28.514087677001953, \t Total Dis Loss : 5.759830673923716e-05\n",
      "Steps : 131200, \t Total Gen Loss : 31.63098907470703, \t Total Dis Loss : 0.00022986366820987314\n",
      "Steps : 131300, \t Total Gen Loss : 28.634414672851562, \t Total Dis Loss : 2.326516550965607e-05\n",
      "Steps : 131400, \t Total Gen Loss : 28.4387264251709, \t Total Dis Loss : 7.310856017284095e-05\n",
      "Steps : 131500, \t Total Gen Loss : 31.904783248901367, \t Total Dis Loss : 0.0014556350652128458\n",
      "Steps : 131600, \t Total Gen Loss : 28.30202293395996, \t Total Dis Loss : 0.0001804744388209656\n",
      "Steps : 131700, \t Total Gen Loss : 30.079919815063477, \t Total Dis Loss : 0.0001892587897600606\n",
      "Steps : 131800, \t Total Gen Loss : 30.761133193969727, \t Total Dis Loss : 0.00017113395733758807\n",
      "Steps : 131900, \t Total Gen Loss : 28.999507904052734, \t Total Dis Loss : 0.00045358543866313994\n",
      "Steps : 132000, \t Total Gen Loss : 32.342369079589844, \t Total Dis Loss : 0.0001248954504262656\n",
      "Steps : 132100, \t Total Gen Loss : 28.62061882019043, \t Total Dis Loss : 4.413534770719707e-05\n",
      "Steps : 132200, \t Total Gen Loss : 31.540611267089844, \t Total Dis Loss : 7.803317566867918e-05\n",
      "Steps : 132300, \t Total Gen Loss : 28.273244857788086, \t Total Dis Loss : 2.0088307792320848e-05\n",
      "Steps : 132400, \t Total Gen Loss : 26.65045928955078, \t Total Dis Loss : 0.00031919372850097716\n",
      "Steps : 132500, \t Total Gen Loss : 26.038267135620117, \t Total Dis Loss : 7.95876476331614e-05\n",
      "Steps : 132600, \t Total Gen Loss : 31.165264129638672, \t Total Dis Loss : 0.00021620825282298028\n",
      "Steps : 132700, \t Total Gen Loss : 26.909339904785156, \t Total Dis Loss : 0.00020189027418382466\n",
      "Steps : 132800, \t Total Gen Loss : 25.537935256958008, \t Total Dis Loss : 7.493753219023347e-05\n",
      "Steps : 132900, \t Total Gen Loss : 27.3803653717041, \t Total Dis Loss : 9.094702545553446e-05\n",
      "Steps : 133000, \t Total Gen Loss : 26.12320327758789, \t Total Dis Loss : 0.00010015066072810441\n",
      "Steps : 133100, \t Total Gen Loss : 28.103471755981445, \t Total Dis Loss : 0.00012907249038107693\n",
      "Steps : 133200, \t Total Gen Loss : 31.25369644165039, \t Total Dis Loss : 4.7703084419481456e-05\n",
      "Steps : 133300, \t Total Gen Loss : 27.279497146606445, \t Total Dis Loss : 0.0001034654924296774\n",
      "Steps : 133400, \t Total Gen Loss : 27.338802337646484, \t Total Dis Loss : 8.810253348201513e-05\n",
      "Steps : 133500, \t Total Gen Loss : 27.712181091308594, \t Total Dis Loss : 5.500510087586008e-05\n",
      "Steps : 133600, \t Total Gen Loss : 27.70062255859375, \t Total Dis Loss : 5.104611409478821e-05\n",
      "Steps : 133700, \t Total Gen Loss : 26.576881408691406, \t Total Dis Loss : 5.391105150920339e-05\n",
      "Steps : 133800, \t Total Gen Loss : 26.030357360839844, \t Total Dis Loss : 0.00016704968584235758\n",
      "Steps : 133900, \t Total Gen Loss : 24.247007369995117, \t Total Dis Loss : 0.00018710472795646638\n",
      "Steps : 134000, \t Total Gen Loss : 24.93903350830078, \t Total Dis Loss : 0.00017431685409974307\n",
      "Steps : 134100, \t Total Gen Loss : 25.1907958984375, \t Total Dis Loss : 0.00011184874892933294\n",
      "Steps : 134200, \t Total Gen Loss : 27.632516860961914, \t Total Dis Loss : 0.0001381584006594494\n",
      "Steps : 134300, \t Total Gen Loss : 30.77609634399414, \t Total Dis Loss : 9.576081356499344e-05\n",
      "Steps : 134400, \t Total Gen Loss : 26.59296417236328, \t Total Dis Loss : 5.184429028304294e-05\n",
      "Steps : 134500, \t Total Gen Loss : 25.35310173034668, \t Total Dis Loss : 0.00037029440863989294\n",
      "Steps : 134600, \t Total Gen Loss : 26.561973571777344, \t Total Dis Loss : 0.00027465252787806094\n",
      "Steps : 134700, \t Total Gen Loss : 24.17951202392578, \t Total Dis Loss : 0.00018493355310056359\n",
      "Steps : 134800, \t Total Gen Loss : 25.494245529174805, \t Total Dis Loss : 0.00023705998319201171\n",
      "Steps : 134900, \t Total Gen Loss : 27.057680130004883, \t Total Dis Loss : 0.00017945017316378653\n",
      "Steps : 135000, \t Total Gen Loss : 27.94913101196289, \t Total Dis Loss : 9.53213166212663e-05\n",
      "Time for epoch 24 is 258.2757194042206 sec\n",
      "Steps : 135100, \t Total Gen Loss : 26.712692260742188, \t Total Dis Loss : 9.234446042682976e-05\n",
      "Steps : 135200, \t Total Gen Loss : 27.995115280151367, \t Total Dis Loss : 0.00022419707966037095\n",
      "Steps : 135300, \t Total Gen Loss : 27.069461822509766, \t Total Dis Loss : 0.00010635294165695086\n",
      "Steps : 135400, \t Total Gen Loss : 31.323577880859375, \t Total Dis Loss : 3.1595674954587594e-05\n",
      "Steps : 135500, \t Total Gen Loss : 27.658222198486328, \t Total Dis Loss : 0.00010934563033515587\n",
      "Steps : 135600, \t Total Gen Loss : 30.697690963745117, \t Total Dis Loss : 1.2867327313870192e-05\n",
      "Steps : 135700, \t Total Gen Loss : 30.27133560180664, \t Total Dis Loss : 6.881231092847884e-05\n",
      "Steps : 135800, \t Total Gen Loss : 26.432966232299805, \t Total Dis Loss : 0.00016278791008517146\n",
      "Steps : 135900, \t Total Gen Loss : 24.543237686157227, \t Total Dis Loss : 0.00017988201580010355\n",
      "Steps : 136000, \t Total Gen Loss : 27.104347229003906, \t Total Dis Loss : 0.00010935249156318605\n",
      "Steps : 136100, \t Total Gen Loss : 29.1241455078125, \t Total Dis Loss : 0.0001410336553817615\n",
      "Steps : 136200, \t Total Gen Loss : 27.309600830078125, \t Total Dis Loss : 8.67225244292058e-05\n",
      "Steps : 136300, \t Total Gen Loss : 24.881237030029297, \t Total Dis Loss : 0.00017338560428470373\n",
      "Steps : 136400, \t Total Gen Loss : 25.915325164794922, \t Total Dis Loss : 8.455491479253396e-05\n",
      "Steps : 136500, \t Total Gen Loss : 29.75890350341797, \t Total Dis Loss : 6.34047610219568e-05\n",
      "Steps : 136600, \t Total Gen Loss : 26.803316116333008, \t Total Dis Loss : 0.00017632248636800796\n",
      "Steps : 136700, \t Total Gen Loss : 26.97646713256836, \t Total Dis Loss : 4.633478238247335e-05\n",
      "Steps : 136800, \t Total Gen Loss : 27.117719650268555, \t Total Dis Loss : 4.467544567887671e-05\n",
      "Steps : 136900, \t Total Gen Loss : 26.183143615722656, \t Total Dis Loss : 7.817777805030346e-05\n",
      "Steps : 137000, \t Total Gen Loss : 25.933155059814453, \t Total Dis Loss : 5.5395088566001505e-05\n",
      "Steps : 137100, \t Total Gen Loss : 26.60822868347168, \t Total Dis Loss : 3.617836409830488e-05\n",
      "Steps : 137200, \t Total Gen Loss : 28.087295532226562, \t Total Dis Loss : 2.912182389991358e-05\n",
      "Steps : 137300, \t Total Gen Loss : 25.49984359741211, \t Total Dis Loss : 7.97563698142767e-05\n",
      "Steps : 137400, \t Total Gen Loss : 30.15416717529297, \t Total Dis Loss : 7.928507693577558e-05\n",
      "Steps : 137500, \t Total Gen Loss : 25.371170043945312, \t Total Dis Loss : 0.0002176004200009629\n",
      "Steps : 137600, \t Total Gen Loss : 28.21085548400879, \t Total Dis Loss : 2.290027441631537e-05\n",
      "Steps : 137700, \t Total Gen Loss : 28.239988327026367, \t Total Dis Loss : 2.3649334252695553e-05\n",
      "Steps : 137800, \t Total Gen Loss : 29.049055099487305, \t Total Dis Loss : 3.555199145921506e-05\n",
      "Steps : 137900, \t Total Gen Loss : 25.579683303833008, \t Total Dis Loss : 3.679694782476872e-05\n",
      "Steps : 138000, \t Total Gen Loss : 29.26565170288086, \t Total Dis Loss : 4.391951370052993e-05\n",
      "Steps : 138100, \t Total Gen Loss : 27.867616653442383, \t Total Dis Loss : 8.19853667053394e-05\n",
      "Steps : 138200, \t Total Gen Loss : 27.42339515686035, \t Total Dis Loss : 4.890469790552743e-05\n",
      "Steps : 138300, \t Total Gen Loss : 28.658172607421875, \t Total Dis Loss : 4.2298604967072606e-05\n",
      "Steps : 138400, \t Total Gen Loss : 29.881519317626953, \t Total Dis Loss : 0.00018001724674832076\n",
      "Steps : 138500, \t Total Gen Loss : 25.562318801879883, \t Total Dis Loss : 0.00043153867591172457\n",
      "Steps : 138600, \t Total Gen Loss : 26.726648330688477, \t Total Dis Loss : 0.00026612504734657705\n",
      "Steps : 138700, \t Total Gen Loss : 28.40052604675293, \t Total Dis Loss : 0.0001483648520661518\n",
      "Steps : 138800, \t Total Gen Loss : 27.95787811279297, \t Total Dis Loss : 8.286946103908122e-05\n",
      "Steps : 138900, \t Total Gen Loss : 27.062885284423828, \t Total Dis Loss : 0.0003122268244624138\n",
      "Steps : 139000, \t Total Gen Loss : 23.840057373046875, \t Total Dis Loss : 0.0012138363672420382\n",
      "Steps : 139100, \t Total Gen Loss : 25.459924697875977, \t Total Dis Loss : 0.00020938983652740717\n",
      "Steps : 139200, \t Total Gen Loss : 27.172897338867188, \t Total Dis Loss : 0.00013220778782851994\n",
      "Steps : 139300, \t Total Gen Loss : 26.760103225708008, \t Total Dis Loss : 0.00018066515622194856\n",
      "Steps : 139400, \t Total Gen Loss : 30.7685489654541, \t Total Dis Loss : 4.120205630897544e-06\n",
      "Steps : 139500, \t Total Gen Loss : 27.762226104736328, \t Total Dis Loss : 8.070216426858678e-05\n",
      "Steps : 139600, \t Total Gen Loss : 26.50733184814453, \t Total Dis Loss : 0.0007236840901896358\n",
      "Steps : 139700, \t Total Gen Loss : 30.87201690673828, \t Total Dis Loss : 5.3388728701975197e-05\n",
      "Steps : 139800, \t Total Gen Loss : 25.88779640197754, \t Total Dis Loss : 3.1150000722846016e-05\n",
      "Steps : 139900, \t Total Gen Loss : 28.988637924194336, \t Total Dis Loss : 5.574020906351507e-05\n",
      "Steps : 140000, \t Total Gen Loss : 28.378032684326172, \t Total Dis Loss : 7.866146916057914e-05\n",
      "Steps : 140100, \t Total Gen Loss : 28.538448333740234, \t Total Dis Loss : 3.115181243629195e-05\n",
      "Steps : 140200, \t Total Gen Loss : 28.59833526611328, \t Total Dis Loss : 3.218969141016714e-05\n",
      "Steps : 140300, \t Total Gen Loss : 26.933290481567383, \t Total Dis Loss : 7.778514373057988e-06\n",
      "Steps : 140400, \t Total Gen Loss : 26.325143814086914, \t Total Dis Loss : 0.00010888245014939457\n",
      "Steps : 140500, \t Total Gen Loss : 26.608922958374023, \t Total Dis Loss : 9.001300350064412e-05\n",
      "Steps : 140600, \t Total Gen Loss : 30.212329864501953, \t Total Dis Loss : 2.4498376660631038e-05\n",
      "Time for epoch 25 is 261.4428246021271 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f85acd2c310>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35020947 0.379769\n",
      "0.11644505 0.15310171\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
