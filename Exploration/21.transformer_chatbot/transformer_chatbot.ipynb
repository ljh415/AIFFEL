{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 준비하기\n",
    "---\n",
    "- 사용할 데이터 : [songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData%20.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-28 00:20:47--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 889842 (869K) [text/plain]\n",
      "Saving to: ‘ChatbotData .csv’\n",
      "\n",
      "ChatbotData .csv    100%[===================>] 868.99K  2.45MB/s    in 0.3s    \n",
      "\n",
      "2020-10-28 00:20:49 (2.45 MB/s) - ‘ChatbotData .csv’ saved [889842/889842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'transformer_chatbot.ipynb',\n",
       " 'e21_transformer_chatbot.ipynb',\n",
       " 'ChatbotData .csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>그냥 무작정 그녀 집에 갑니다</td>\n",
       "      <td>행동파시군요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>화장 지워야 하는데 졸려</td>\n",
       "      <td>피부 생각하셔야죠!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7885</th>\n",
       "      <td>잘참아왔는데 눈물 흘렸네.</td>\n",
       "      <td>자신의 감정에 귀 기울이세요. 우는 건 죄가 아니랍니다.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>왜 맨날 속는 건 나인거지?</td>\n",
       "      <td>즐겁게 속아주세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>카톡 대화를 왜 삭제하지</td>\n",
       "      <td>복원해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819</th>\n",
       "      <td>빼빼로 만들어주는 거 좋아할까?</td>\n",
       "      <td>정성이 담긴 것이니 좋아할 거예요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>식욕폭발해서 폭풍 먹방 찍었어</td>\n",
       "      <td>맛있게 먹으면 영칼로리!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5254</th>\n",
       "      <td>휴가 고고</td>\n",
       "      <td>푹 쉬고 오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>그녀를 떠나 보냈습니다.</td>\n",
       "      <td>맘이 편치 않겠어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>바다가 가고 싶어요</td>\n",
       "      <td>탁 트인 바다 좋죠!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Q                                A  label\n",
       "5607   그냥 무작정 그녀 집에 갑니다                          행동파시군요.      1\n",
       "5185      화장 지워야 하는데 졸려                       피부 생각하셔야죠!      0\n",
       "7885     잘참아왔는데 눈물 흘렸네.  자신의 감정에 귀 기울이세요. 우는 건 죄가 아니랍니다.      1\n",
       "3434    왜 맨날 속는 건 나인거지?                       즐겁게 속아주세요.      0\n",
       "4700      카톡 대화를 왜 삭제하지                          복원해보세요.      0\n",
       "9819  빼빼로 만들어주는 거 좋아할까?              정성이 담긴 것이니 좋아할 거예요.      2\n",
       "2765   식욕폭발해서 폭풍 먹방 찍었어                    맛있게 먹으면 영칼로리!      0\n",
       "5254              휴가 고고                        푹 쉬고 오세요.      0\n",
       "5634      그녀를 떠나 보냈습니다.                      맘이 편치 않겠어요.      1\n",
       "1896         바다가 가고 싶어요                      탁 트인 바다 좋죠!      0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ChatbotData .csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 데이터 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅣ가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'왜 맨날 속는 건 나인거지 ?'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = '왜 맨날 속는 건 나인거지?'\n",
    "preprocess_sentence(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', ..., '흑기사 해주는 짝남.',\n",
       "        '힘든 연애 좋은 연애라는게 무슨 차이일까?', '힘들어서 결혼할까봐'], dtype=object),\n",
       " 11823)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = df.Q.values\n",
    "questions, len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', ..., '설렜겠어요.',\n",
       "        '잘 헤어질 수 있는 사이 여부인 거 같아요.', '도피성 결혼은 하지 않길 바라요.'], dtype=object),\n",
       " 11823)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = df.A.values\n",
    "answers, len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(map(preprocess_sentence, questions))\n",
    "answers = list(map(preprocess_sentence, answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'ppl 심하네'],\n",
       " ['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .'])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:5], answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Token화 하기\n",
    "- ```TensorFlow Datasets```의 ```SubwordTextEncoder```를 활용해서 토큰화\n",
    "- ```tokenize_and_filter```함수\n",
    "    - 문장을 정수로 인코딩 수행\n",
    "    - 설정한 최대 길이를 넘어갈 경우 샘플을 제거\n",
    "    - 최대 길이보다 짧을 경우, 패딩 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8170]\n",
      "END_TOKEN의 번호 : [8171]\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8172\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이.\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 40이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8172\n",
      "필터링 후의 샘플 개수: 11823\n",
      "필터링 후의 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. dataset만들기\n",
    "---\n",
    "- ```teacher forcing```사용\n",
    "    - ```answer[:,:-1]```를 디코더의 입력값, ```answer[:,1:]```를 디코더의 레이블로 사용한다\n",
    "- ```tf.data.Dataset``` API활용, 파이프라인 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1) PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        \n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2) scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \n",
    "    \"\"\"어텐션 가중치를 계산. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-3) MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다.\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷-프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다.\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-4) Padding Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-5) Look-ahead Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-6) Encoder 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "          })\n",
    "\n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 두번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-7) Decoder 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 두번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "\n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 세번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-8) transformer 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더에서 패딩을 위한 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크하기위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 두번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "      # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "      )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, None, 512)    13651968    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, None, 512)    19961856    dec_inputs[0][0]                 \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8172)   4192236     decoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 37,806,060\n",
      "Trainable params: 37,806,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 6 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2) 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3) 커스텀 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 38s 205ms/step - loss: 1.3443 - accuracy: 0.0227\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 41s 220ms/step - loss: 1.0780 - accuracy: 0.0482\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.9811 - accuracy: 0.0507\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.9383 - accuracy: 0.0529\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.9014 - accuracy: 0.0554\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.8614 - accuracy: 0.0575\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.8158 - accuracy: 0.0601\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.7627 - accuracy: 0.0638\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.7044 - accuracy: 0.0689\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.6441 - accuracy: 0.0747\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.5812 - accuracy: 0.0818\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.5206 - accuracy: 0.0888\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.4651 - accuracy: 0.0957\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.4142 - accuracy: 0.1026\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.3706 - accuracy: 0.1082\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.3370 - accuracy: 0.1126\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.3096 - accuracy: 0.1159\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.2864 - accuracy: 0.1188\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 41s 220ms/step - loss: 0.2723 - accuracy: 0.1207\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.2588 - accuracy: 0.1226\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.2496 - accuracy: 0.1238\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 41s 220ms/step - loss: 0.2406 - accuracy: 0.1253\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.2262 - accuracy: 0.1272\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.2128 - accuracy: 0.1294\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1997 - accuracy: 0.1316\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1864 - accuracy: 0.1337\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1763 - accuracy: 0.1353\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1669 - accuracy: 0.1370\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1586 - accuracy: 0.1383\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1490 - accuracy: 0.1401\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1418 - accuracy: 0.1413\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1345 - accuracy: 0.1427\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1271 - accuracy: 0.1440\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1220 - accuracy: 0.1452\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1150 - accuracy: 0.1464\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1104 - accuracy: 0.1471\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1061 - accuracy: 0.1480\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.1005 - accuracy: 0.1493\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0969 - accuracy: 0.1499\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0924 - accuracy: 0.1510\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0877 - accuracy: 0.1520\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0847 - accuracy: 0.1526\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0799 - accuracy: 0.1536\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0787 - accuracy: 0.1538\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0739 - accuracy: 0.1551\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0713 - accuracy: 0.1557\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0691 - accuracy: 0.1561\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 41s 222ms/step - loss: 0.0662 - accuracy: 0.1568\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0631 - accuracy: 0.1576\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 41s 221ms/step - loss: 0.0615 - accuracy: 0.1581\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-1) 학습결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(121)\n",
    "plt.title('accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.legend(['accuracy'], loc='best')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) 챗봇 테스트하기\n",
    "---\n",
    "예측(inference) 단계는 기본적으로 다음과 같은 과정을 거칩니다.\n",
    "\n",
    "1. 새로운 입력 문장에 대해서는 훈련때와 동일한 전처리를 거친다.\n",
    "2. 입력 문장을 토크나이징하고, START_TOKEN과 END_TOKEN을 추가한다.\n",
    "3. 패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n",
    "4. 디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n",
    "5. 디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n",
    "6. ```END_TOKEN```이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다.\n",
    "  \n",
    "다음의 과정이 담긴 decoder_inference()함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞 뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없으므로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 아 졸려\n",
      "출력 : 오늘 일찍 주무세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'오늘 일찍 주무세요 .'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('아 졸려')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 추워\n",
      "출력 : 따듯한 차 한잔 어때요 ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'따듯한 차 한잔 어때요 ?'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"추워\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정리\n",
    "---\n",
    "1. 한국어 전처리\n",
    "    - 기존의 영어와 구두점에 한글과 숫자도 추가했다.\n",
    "    ```python\n",
    "    # 전처리 함수\n",
    "    def preprocess_sentence(sentence):\n",
    "        sentence = sentence.lower().strip()\n",
    "\n",
    "        # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "        # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "        # student와 온점 사이에 거리를 만듭니다.\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "        # 한글, 영어, 숫자, 4개의 구두점(? ! , .)을 제외한 모든 문자를 공백인 ' '로 대체\n",
    "        sentence = re.sub(r\"[^ㄱ-ㅣ가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "        sentence = sentence.strip()\n",
    "        return sentence\n",
    "    ```\n",
    "<br>  \n",
    "2. 트랜스포머 모델의 하이퍼파라미터 변경\n",
    "    - 원래 논문과 같이 ```NUM_LAYERS=2```, ```D_MODEL=512```로 사용\n",
    "    - ```EPOCHS```도 기존에 20에서 50으로 늘려서 학습을 시켰다.\n",
    "    - loss도 안정적으로 줄었고 accuracy도 꾸준히 증가하였다.  \n",
    "<br>  \n",
    "3. 한국어 입력문장에 그럴듯한 답변을 리턴하였다.\n",
    "    ![image](https://user-images.githubusercontent.com/48716219/97334479-221c4e80-18c0-11eb-8b94-3016a58b5bfe.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
