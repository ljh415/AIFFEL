# 정리
---
- 뉴스 데이터를 두가지 방법으로 요악을 시도하였다.
    1. 추상적 요약
        - Attenttion 매터니즘 활용
        - LSTM을 사용, Encoder -> Decoder, 앞의 결과와 Attention층을 연결하여 결과가 나온다.
    2. 추출적 요약
        - ```Summa```활용  
        <br>  
    
- 데이터 전처리
    - 입력데이터의 길이는 다음과 같다.
        - [ 1, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 91]
    - 인코더의 입력 데이터 중 길이가 44 이하인 경우는 불용어 제거를 하지 않고 그대로 전처리만 적용 
        <br>  
    
- 다음과 같이 안정적인 학습 그래프가 나왔다.
    ![image](https://user-images.githubusercontent.com/48716219/96459690-5a3ae600-125d-11eb-919c-bc3c59ef89c6.png) 

<br>  

- 학습 결과 loss값은 다음과 같다.
    - ```loss: 2.6574```, ```val_loss: 3.5886```
    - train데이터에 대한 loss값은 많이 줄었지만 validation에 대한 loss값은 많이 줄지 않았다.
    - 개선시킬 방법은?
        1. 학습률 변경
        2. 데이터 전처리 방법 변경  
        <br>  
- ```Summa```를 활용한 요약문의 경우 ```ratio```가 0.4보다 작아질 경우 결과가 제대로 나오지 않았다.
- 추상적요약은 loss결과를 봐서 예상이 가능하듯이 생각보다 정답 데이터에 비해서 결과가 많이 비슷하게 나오지는 않았다. 하지만 실제 요약문에서 사용되는 단어는 어느정도 포함이 되어 있는 것으로 봐서 학습이 진행한 것은 알 수 있었다.